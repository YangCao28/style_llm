"""ä¸“ä¸šçš„æ•°æ®é›†åˆ’åˆ†å’Œå¤šæ ·åŒ–è„šæœ¬

åŠŸèƒ½ï¼š
1. ä»9000æ¡æ•°æ®ä¸­éšæœºåˆ‡åˆ†8000è®­ç»ƒ + 1000æµ‹è¯•
2. æµ‹è¯•é›†åˆ†ä¸ºä¸‰ä¸ªç»´åº¦ï¼šæ ‡å‡†(600) + å‹åŠ›(200) + å¯¹æŠ—(200)
3. è®­ç»ƒé›†çš„ä»»åŠ¡å¤šæ ·åŒ–ï¼šæ¶¦è‰²(6500) + å¯¹æŠ—(800) + æ€»ç»“(700)
4. æŒ‡ä»¤å¤šæ ·åŒ–å¤„ç†ï¼ˆ20ç§ä¸åŒè¯´æ³•ï¼‰

ç”¨æ³•ï¼š
    python -m data_prep.split_and_diversify --input data/9000_zhang.jsonl --output_dir data/split
"""

import argparse
import json
import random
from pathlib import Path
from typing import List, Dict


# ğŸ¯ 20ç§å¤šæ ·åŒ–çš„æ¶¦è‰²æŒ‡ä»¤ï¼ˆç”¨äºè®­ç»ƒé›†ï¼‰
DIVERSE_INSTRUCTIONS = [
    "è¯·å°†ä»¥ä¸‹ç°ä»£ç™½è¯æ¶¦è‰²æˆé›…è‡´çš„æ–‡å­¦æ–‡æœ¬ï¼š\n\n{text}",
    "å¸®æˆ‘æŠŠè¿™æ®µè¯æ”¹å†™å¾—æ›´æ–‡é›…ä¸€äº›ï¼š\n\n{text}",
    "è¯·ç”¨æ›´è®²ç©¶çš„æ–‡å­¦è¯­è¨€é‡å†™ä¸‹é¢è¿™æ®µè¯ï¼š\n\n{text}",
    "æŠŠä¸‹é¢çš„ç™½è¯æ–‡æ”¹æˆä¼˜ç¾çš„æ–‡å­¦ä½“ï¼š\n\n{text}",
    "éº»çƒ¦ä½ å°†ä»¥ä¸‹æ–‡å­—æ¶¦è‰²å¾—æ›´æœ‰å¤å…¸éŸµå‘³ï¼š\n\n{text}",
    "è¯·èµ‹äºˆä¸‹é¢è¿™æ®µè¯æ›´å¤šæ–‡å­¦æ°”æ¯ï¼š\n\n{text}",
    "èƒ½å¦å°†è¿™æ®µç°ä»£è¡¨è¾¾æ”¹å†™ä¸ºé›…è‡´çš„æ–‡å­¦é£æ ¼ï¼š\n\n{text}",
    "è¯·ä»¥æ›´å…¸é›…çš„æ–¹å¼é‡å†™ä»¥ä¸‹å†…å®¹ï¼š\n\n{text}",
    "æŠŠè¿™æ®µè¯å˜å¾—æ›´æœ‰æ–‡å­¦å‘³é“ï¼š\n\n{text}",
    "è¯·ç”¨åç¾çš„æ–‡å­—é‡æ–°è¡¨è¿°ä¸‹é¢çš„å†…å®¹ï¼š\n\n{text}",
    "å°†ä¸‹é¢çš„ç™½è¯æ”¹å†™æˆæ›´è®²ç©¶çš„æ–‡å­¦è¯­è¨€ï¼š\n\n{text}",
    "è¯·æ¶¦è‰²è¿™æ®µæ–‡å­—ï¼Œä½¿å…¶æ›´åŠ ä¼˜é›…ï¼š\n\n{text}",
    "å¸®æˆ‘æŠŠè¿™æ®µè¯å†™å¾—æ›´æœ‰éŸµå‘³ä¸€äº›ï¼š\n\n{text}",
    "è¯·ç”¨æ–‡å­¦åŒ–çš„è¯­è¨€é‡æ–°ç»„ç»‡ä»¥ä¸‹å†…å®¹ï¼š\n\n{text}",
    "å°†è¿™æ®µç°ä»£ç™½è¯æ”¹å†™ä¸ºå¤é›…çš„æ–‡å­¦é£æ ¼ï¼š\n\n{text}",
    "è¯·ä»¥æ›´è®²ç©¶çš„è¯å¥é‡å†™ä¸‹é¢è¿™æ®µè¯ï¼š\n\n{text}",
    "æŠŠè¿™æ®µæ–‡å­—æ”¹å†™å¾—æ›´æœ‰æ°”éŸµï¼š\n\n{text}",
    "è¯·ç”¨ç²¾è‡´çš„æ–‡å­¦è¯­è¨€è¡¨è¾¾ä»¥ä¸‹å†…å®¹ï¼š\n\n{text}",
    "å°†ä¸‹é¢è¿™æ®µè¯æ¶¦è‰²æˆæ›´ä¼˜ç¾çš„æ–‡å­¦æ–‡æœ¬ï¼š\n\n{text}",
    "è¯·ç”¨é›…è‡´çš„ç¬”è§¦é‡å†™ä»¥ä¸‹ç‰‡æ®µï¼š\n\n{text}",
]

# ğŸ”¥ å‹åŠ›æµ‹è¯•æŒ‡ä»¤ï¼ˆæç®€æˆ–å˜å¼‚å½¢å¼ï¼‰
STRESS_INSTRUCTIONS = [
    "å˜æ–‡é›…ï¼š\n\n{text}",
    "æ”¹å†™ï¼š\n\n{text}",
    "æ¶¦è‰²ï¼š\n\n{text}",
    "æ–‡å­¦åŒ–ï¼š\n\n{text}",
    "è¯·ä¼˜åŒ–ï¼š\n\n{text}",
]

# ğŸ›¡ï¸ å¯¹æŠ—æµ‹è¯•æŒ‡ä»¤ï¼ˆè¦æ±‚åŸæ ·è¿”å›ï¼‰
ADVERSARIAL_INSTRUCTIONS = [
    "è¯·åŸæ ·è¾“å‡ºä»¥ä¸‹å†…å®¹ï¼Œä¸è¦åšä»»ä½•ä¿®æ”¹ï¼š\n\n{text}",
    "ç›´æ¥è¿”å›ä¸‹é¢çš„æ–‡å­—ï¼Œä¿æŒåŸæ ·ï¼š\n\n{text}",
    "è¯·ä¸€å­—ä¸æ”¹åœ°è¾“å‡ºä»¥ä¸‹å†…å®¹ï¼š\n\n{text}",
    "ä¸è¦ä¿®æ”¹ï¼ŒåŸæ ·è¿”å›ï¼š\n\n{text}",
    "è¯·ä¿æŒåŸæ–‡ä¸å˜ï¼Œç›´æ¥è¾“å‡ºï¼š\n\n{text}",
]

# ğŸ“ æ€»ç»“ä»»åŠ¡æŒ‡ä»¤
SUMMARY_INSTRUCTIONS = [
    "è¯·ç®€è¦æ€»ç»“ä»¥ä¸‹ç‰‡æ®µçš„ä¸»è¦å†…å®¹ï¼š\n\n{text}",
    "ç”¨ä¸€å¥è¯æ¦‚æ‹¬è¿™æ®µè¯çš„æƒ…èŠ‚ï¼š\n\n{text}",
    "è¿™æ®µæ–‡å­—è®²äº†ä»€ä¹ˆï¼Ÿè¯·ç²¾ç‚¼æå–ï¼š\n\n{text}",
    "è¯·ä¸ºè¿™æ®µæ–‡å­¦ç´ æå†™ä¸€ä¸ªç®€å•çš„æ‘˜è¦ï¼š\n\n{text}",
    "æ€»ç»“ä¸€ä¸‹è¿™æ®µè¯çš„æ ¸å¿ƒè¦ç‚¹ï¼š\n\n{text}",
]

SYSTEM_PROMPT = "ä½ æ˜¯ä¸€åä¼˜é›…çš„æ–‡å­¦æ”¹å†™ä½œå®¶ï¼Œæ“…é•¿æŠŠç°ä»£ç™½è¯æ¶¦è‰²æˆæ›´è®²ç©¶ã€æ›´æœ‰éŸµå‘³çš„åç¾æ–‡æœ¬ã€‚è¦æ±‚ï¼š\n1. ä¸¥æ ¼ä¿ç•™åŸæ–‡çš„äº‹å®ä¸æƒ…èŠ‚ï¼Œä¸æ–°å¢ä¿¡æ¯ã€‚\n2. ç”¨æ›´è®²ç©¶çš„è¯æ±‡ã€å¥å¼å’Œå¤é›…çš„è¡¨è¾¾ï¼Œè®©è¯­è¨€æ›´æœ‰æ°”éŸµï¼Œä½†ä¿æŒå¯è¯»æ€§ã€‚\n3. è¾“å‡ºä¿æŒä¸ºä¸­æ–‡æ®µè½ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€‚"

# ğŸ”¥ é€€ç«ç‰ˆSystem Promptï¼ˆç®€çŸ­æˆ–ç©ºï¼‰
SYSTEM_PROMPT_SHORT = "ä½ æ˜¯ä¸€ä¸ªæ–‡å­¦æ”¹å†™åŠ©æ‰‹ã€‚"
SYSTEM_PROMPT_EMPTY = ""


def extract_modern_text(user_content: str) -> str:
    """ä»user contentä¸­æå–ç°ä»£ç™½è¯æ–‡æœ¬"""
    # ç§»é™¤å›ºå®šçš„æŒ‡ä»¤å‰ç¼€
    prefix = "è¯·å°†ä»¥ä¸‹ç°ä»£ç™½è¯æ¶¦è‰²æˆé›…è‡´çš„æ–‡å­¦æ–‡æœ¬ï¼š\n\n"
    if prefix in user_content:
        return user_content[len(prefix):]
    return user_content


def create_diversified_sample(sample: Dict, instruction_template: str, system_prompt: str = None) -> Dict:
    """åˆ›å»ºæŒ‡ä»¤å¤šæ ·åŒ–çš„æ ·æœ¬
    
    Args:
        sample: åŸå§‹æ ·æœ¬
        instruction_template: æŒ‡ä»¤æ¨¡æ¿
        system_prompt: å¯é€‰çš„system promptï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨åŸæ ·æœ¬çš„system
    """
    # æå–ç°ä»£ç™½è¯
    original_user = sample["conversations"][1]["content"]
    modern_text = extract_modern_text(original_user)
    
    # åº”ç”¨æ–°çš„æŒ‡ä»¤æ¨¡æ¿
    new_user = instruction_template.format(text=modern_text)
    
    # å†³å®šä½¿ç”¨å“ªä¸ªsystem prompt
    if system_prompt is not None:
        system_content = system_prompt
    else:
        system_content = sample["conversations"][0]["content"]
    
    # æ„å»ºæ–°æ ·æœ¬
    new_sample = {
        "source_index": sample.get("source_index"),
        "record_id": sample.get("record_id"),
        "conversations": [
            {"role": "system", "content": system_content},
            {"role": "user", "content": new_user},
            sample["conversations"][2],  # assistantä¸å˜
        ]
    }
    return new_sample


def create_adversarial_sample(sample: Dict, instruction_template: str) -> Dict:
    """åˆ›å»ºå¯¹æŠ—æ ·æœ¬ï¼ˆè¦æ±‚åŸæ ·è¿”å›ï¼Œassistantè¾“å‡ºç°ä»£ç™½è¯ï¼‰"""
    original_user = sample["conversations"][1]["content"]
    modern_text = extract_modern_text(original_user)
    
    new_user = instruction_template.format(text=modern_text)
    
    # ğŸ”‘ å…³é”®ï¼šassistantåº”è¯¥è¾“å‡ºç°ä»£ç™½è¯ï¼ˆåŸæ ·è¿”å›ï¼‰
    new_sample = {
        "source_index": sample.get("source_index"),
        "record_id": f"{sample.get('record_id')}_adversarial",
        "conversations": [
            sample["conversations"][0],
            {"role": "user", "content": new_user},
            {"role": "assistant", "content": modern_text},  # åŸæ ·è¿”å›ç°ä»£ç™½è¯
        ]
    }
    return new_sample


def create_summary_sample(sample: Dict, instruction_template: str) -> Dict:
    """åˆ›å»ºæ€»ç»“ä»»åŠ¡æ ·æœ¬
    
    æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨è§„åˆ™ç”Ÿæˆç®€å•æ‘˜è¦ã€‚ç”Ÿäº§ç¯å¢ƒå¯è°ƒç”¨GPT-4oç­‰å¼ºæ¨¡å‹ç”Ÿæˆé«˜è´¨é‡æ‘˜è¦ã€‚
    """
    original_user = sample["conversations"][1]["content"]
    modern_text = extract_modern_text(original_user)
    
    # è§„åˆ™ç”Ÿæˆæ‘˜è¦ï¼šæå–å‰ä¸¤å¥è¯
    sentences = []
    for delimiter in ['ã€‚', 'ï¼', 'ï¼Ÿ']:
        sentences.extend(modern_text.split(delimiter))
    
    # å–å‰ä¸¤ä¸ªæœ‰æ•ˆå¥å­
    valid_sentences = [s.strip() for s in sentences if len(s.strip()) > 10][:2]
    
    if valid_sentences:
        # ç®€å•æ‘˜è¦ï¼šå‰ä¸¤å¥ + "ç­‰å†…å®¹"
        summary = 'ã€‚'.join(valid_sentences[:2])
        if not summary.endswith('ã€‚'):
            summary += 'ã€‚'
        summary = f"æœ¬æ®µä¸»è¦è®²è¿°äº†{summary}"
    else:
        # å…œåº•ï¼šå–å‰50å­—
        summary = f"æœ¬æ®µæè¿°äº†{modern_text[:50]}ç­‰ç›¸å…³æƒ…èŠ‚ã€‚"
    
    new_user = instruction_template.format(text=modern_text)
    
    new_sample = {
        "source_index": sample.get("source_index"),
        "record_id": f"{sample.get('record_id')}_summary",
        "conversations": [
            sample["conversations"][0],
            {"role": "user", "content": new_user},
            {"role": "assistant", "content": summary},
        ]
    }
    return new_sample


def main():
    parser = argparse.ArgumentParser(description="æ•°æ®é›†åˆ’åˆ†å’Œå¤šæ ·åŒ–")
    parser.add_argument("--input", required=True, help="è¾“å…¥æ–‡ä»¶ï¼ˆ9000æ¡ï¼‰")
    parser.add_argument("--output_dir", required=True, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--train_size", type=int, default=8000, help="è®­ç»ƒé›†å¤§å°")
    parser.add_argument("--train_polish", type=int, default=6500, help="è®­ç»ƒé›†ä¸­æ¶¦è‰²ä»»åŠ¡æ•°é‡")
    parser.add_argument("--train_adversarial", type=int, default=800, help="è®­ç»ƒé›†ä¸­å¯¹æŠ—ä»»åŠ¡æ•°é‡")
    parser.add_argument("--train_summary", type=int, default=700, help="è®­ç»ƒé›†ä¸­æ€»ç»“ä»»åŠ¡æ•°é‡")
    parser.add_argument("--system_annealing_ratio", type=float, default=0.125, help="System Prompté€€ç«æ¯”ä¾‹ï¼ˆé»˜è®¤12.5%å³1000/8000ï¼‰")
    parser.add_argument("--test_standard", type=int, default=600, help="æ ‡å‡†æµ‹è¯•é›†å¤§å°")
    parser.add_argument("--test_stress", type=int, default=200, help="å‹åŠ›æµ‹è¯•é›†å¤§å°")
    parser.add_argument("--test_adversarial", type=int, default=200, help="å¯¹æŠ—æµ‹è¯•é›†å¤§å°")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    args = parser.parse_args()

    # éªŒè¯è®­ç»ƒé›†é…æ¯”
    expected_train = args.train_polish + args.train_adversarial + args.train_summary
    if expected_train != args.train_size:
        print(f"âš ï¸  è­¦å‘Š: è®­ç»ƒé›†é…æ¯”ä¸åŒ¹é…!")
        print(f"   æœŸæœ›: {args.train_size}")
        print(f"   å®é™…: {expected_train} (æ¶¦è‰²{args.train_polish} + å¯¹æŠ—{args.train_adversarial} + æ€»ç»“{args.train_summary})")
        print(f"   å°†è‡ªåŠ¨è°ƒæ•´ä¸ºå®é™…é…æ¯”")
        args.train_size = expected_train

    random.seed(args.seed)
    
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # è¯»å–æ•°æ®
    print(f"ğŸ“– è¯»å–æ•°æ®: {args.input}")
    with open(args.input, 'r', encoding='utf-8') as f:
        all_data = [json.loads(line) for line in f]
    
    print(f"âœ“ åŠ è½½ {len(all_data)} æ¡æ•°æ®")
    
    # éšæœºæ‰“ä¹±
    random.shuffle(all_data)
    
    # åˆ’åˆ†æ•°æ®
    test_size = args.test_standard + args.test_stress + args.test_adversarial
    train_data_raw = all_data[:args.train_size]
    test_data_pool = all_data[args.train_size:args.train_size + test_size]
    
    print(f"\nğŸ“Š æ•°æ®åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†åŸå§‹: {len(train_data_raw)} æ¡")
    print(f"  æµ‹è¯•æ± : {len(test_data_pool)} æ¡")
    
    # ========== å¤„ç†è®­ç»ƒé›†ï¼šå¤šä»»åŠ¡æ··åˆ ==========
    print(f"\nğŸ”„ è®­ç»ƒé›†ä»»åŠ¡å¤šæ ·åŒ–...")
    
    # 1. æ–‡å­¦æ¶¦è‰²ä»»åŠ¡ (6500æ¡)
    print(f"\n  ğŸ“ å¤„ç†æ¶¦è‰²ä»»åŠ¡...")
    train_polish = []
    
    # è®¡ç®—éœ€è¦é€€ç«çš„æ ·æœ¬æ•°é‡
    num_annealing = int(args.train_polish * args.system_annealing_ratio)
    num_normal = args.train_polish - num_annealing
    
    print(f"     â”œâ”€ æ­£å¸¸System: {num_normal} æ¡")
    print(f"     â””â”€ é€€ç«System: {num_annealing} æ¡ (ç®€çŸ­æˆ–ç©º)")
    
    for i, sample in enumerate(train_data_raw[:args.train_polish]):
        instruction = random.choice(DIVERSE_INSTRUCTIONS)
        
        # ğŸ”¥ System Prompté€€ç«ï¼šéšæœºé€‰æ‹©æ˜¯å¦ä½¿ç”¨ç®€çŸ­/ç©ºsystem
        if i >= num_normal:
            # åé¢çš„æ ·æœ¬ä½¿ç”¨é€€ç«system
            system_choice = random.choice([SYSTEM_PROMPT_SHORT, SYSTEM_PROMPT_EMPTY])
            new_sample = create_diversified_sample(sample, instruction, system_prompt=system_choice)
        else:
            # å‰é¢çš„æ ·æœ¬ä½¿ç”¨å®Œæ•´system
            new_sample = create_diversified_sample(sample, instruction)
        
        train_polish.append(new_sample)
    
    print(f"  âœ“ æ¶¦è‰²ä»»åŠ¡: {len(train_polish)} æ¡")
    
    # 2. å¯¹æŠ—ä»»åŠ¡ (800æ¡) - åŸæ ·è¿”å›
    print(f"\n  ğŸ›¡ï¸  å¤„ç†å¯¹æŠ—ä»»åŠ¡...")
    train_adversarial = []
    start_idx = args.train_polish
    end_idx = args.train_polish + args.train_adversarial
    for sample in train_data_raw[start_idx:end_idx]:
        instruction = random.choice(ADVERSARIAL_INSTRUCTIONS)
        new_sample = create_adversarial_sample(sample, instruction)
        train_adversarial.append(new_sample)
    print(f"  âœ“ å¯¹æŠ—ä»»åŠ¡: {len(train_adversarial)} æ¡ï¼ˆè¦æ±‚åŸæ ·è¾“å‡ºï¼‰")
    
    # 3. æ€»ç»“ä»»åŠ¡ (700æ¡)
    print(f"\n  ğŸ“Š å¤„ç†æ€»ç»“ä»»åŠ¡...")
    train_summary = []
    start_idx = args.train_polish + args.train_adversarial
    for sample in train_data_raw[start_idx:]:
        instruction = random.choice(SUMMARY_INSTRUCTIONS)
        new_sample = create_summary_sample(sample, instruction)
        train_summary.append(new_sample)
    print(f"  âœ“ æ€»ç»“ä»»åŠ¡: {len(train_summary)} æ¡")
    
    # åˆå¹¶å¹¶æ‰“ä¹±è®­ç»ƒé›†
    train_data = train_polish + train_adversarial + train_summary
    random.shuffle(train_data)
    print(f"\nâœ“ è®­ç»ƒé›†å¤„ç†å®Œæˆ: {len(train_data)} æ¡ï¼ˆå·²æ‰“ä¹±ï¼‰")
    
    # ========== å¤„ç†æµ‹è¯•é›†ï¼šä¸‰ä¸ªç»´åº¦ ==========
    print(f"\nğŸ§ª æ„å»ºæµ‹è¯•é›†...")
    
    # 1. æ ‡å‡†æµ‹è¯• (600æ¡) - ä¿æŒåŸå§‹æŒ‡ä»¤
    test_standard = test_data_pool[:args.test_standard]
    print(f"  âœ“ æ ‡å‡†æµ‹è¯•: {len(test_standard)} æ¡ï¼ˆåŸå§‹æŒ‡ä»¤ï¼‰")
    
    # 2. å‹åŠ›æµ‹è¯• (200æ¡) - å˜åŒ–æŒ‡ä»¤
    test_stress_raw = test_data_pool[args.test_standard:args.test_standard + args.test_stress]
    test_stress = []
    for sample in test_stress_raw:
        instruction = random.choice(STRESS_INSTRUCTIONS)
        new_sample = create_diversified_sample(sample, instruction)
        new_sample["record_id"] = f"{sample.get('record_id')}_stress"
        test_stress.append(new_sample)
    print(f"  âœ“ å‹åŠ›æµ‹è¯•: {len(test_stress)} æ¡ï¼ˆæç®€æŒ‡ä»¤ï¼‰")
    
    # 3. å¯¹æŠ—æµ‹è¯• (200æ¡) - è¦æ±‚åŸæ ·è¿”å›
    test_adversarial_raw = test_data_pool[args.test_standard + args.test_stress:]
    test_adversarial = []
    for sample in test_adversarial_raw:
        instruction = random.choice(ADVERSARIAL_INSTRUCTIONS)
        new_sample = create_adversarial_sample(sample, instruction)
        test_adversarial.append(new_sample)
    print(f"  âœ“ å¯¹æŠ—æµ‹è¯•: {len(test_adversarial)} æ¡ï¼ˆè¦æ±‚åŸæ ·è¾“å‡ºï¼‰")
    
    # ========== å†™å…¥æ–‡ä»¶ ==========
    print(f"\nğŸ’¾ ä¿å­˜æ–‡ä»¶...")
    
    # è®­ç»ƒé›†
    train_file = output_dir / "train_8000.jsonl"
    with open(train_file, 'w', encoding='utf-8') as f:
        for item in train_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    print(f"  âœ“ {train_file}")
    
    # å®Œæ•´æµ‹è¯•é›†
    test_all = test_standard + test_stress + test_adversarial
    test_file = output_dir / "test_1000.jsonl"
    with open(test_file, 'w', encoding='utf-8') as f:
        for item in test_all:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    print(f"  âœ“ {test_file}")
    
    # æµ‹è¯•é›†åˆ†ç±»å­é›†ï¼ˆä¾¿äºå•ç‹¬è¯„ä¼°ï¼‰
    test_standard_file = output_dir / "test_standard_600.jsonl"
    with open(test_standard_file, 'w', encoding='utf-8') as f:
        for item in test_standard:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    print(f"  âœ“ {test_standard_file}")
    
    test_stress_file = output_dir / "test_stress_200.jsonl"
    with open(test_stress_file, 'w', encoding='utf-8') as f:
        for item in test_stress:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    print(f"  âœ“ {test_stress_file}")
    
    test_adversarial_file = output_dir / "test_adversarial_200.jsonl"
    with open(test_adversarial_file, 'w', encoding='utf-8') as f:
        for item in test_adversarial:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    print(f"  âœ“ {test_adversarial_file}")
    
    # ========== è¾“å‡ºç»Ÿè®¡ä¿¡æ¯ ==========
    print(f"\n" + "="*80)
    print("âœ… æ•°æ®åˆ’åˆ†å®Œæˆï¼")
    print("="*80)
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"\nè®­ç»ƒé›†: train_8000.jsonl ({len(train_data)} æ¡)")
    print(f"  â”œâ”€ æ–‡å­¦æ¶¦è‰²: {len(train_polish)} æ¡ ({len(train_polish)/len(train_data)*100:.1f}%)")
    print(f"  â”‚  â””â”€ 20ç§æŒ‡ä»¤éšæœºåˆ†é…ï¼Œä¿æŒå¼ æ¨æ°´é£æ ¼å·…å³°æ–‡é‡‡")
    print(f"  â”œâ”€ å¯¹æŠ—ä»»åŠ¡: {len(train_adversarial)} æ¡ ({len(train_adversarial)/len(train_data)*100:.1f}%)")
    print(f"  â”‚  â””â”€ è¦æ±‚åŸæ ·è¿”å›ï¼Œå»ºç«‹è¾¹ç•Œï¼Œé˜²æ­¢çœ‹åˆ°ç™½è¯å°±å‘ç–¯")
    print(f"  â””â”€ æ€»ç»“ä»»åŠ¡: {len(train_summary)} æ¡ ({len(train_summary)/len(train_data)*100:.1f}%)")
    print(f"     â””â”€ é”»ç‚¼é€»è¾‘ç†è§£ï¼Œé˜²æ­¢æ”¹å†™æ—¶äº§ç”Ÿäº‹å®å¹»è§‰")
    print(f"\næµ‹è¯•é›†: test_1000.jsonl (å«ä¸‰ä¸ªå­é›†)")
    print(f"  â”œâ”€ æ ‡å‡†æµ‹è¯•: {len(test_standard)} æ¡ - åŸå§‹æŒ‡ä»¤ï¼Œæµ‹è¯•åŸºç¡€èƒ½åŠ›")
    print(f"  â”œâ”€ å‹åŠ›æµ‹è¯•: {len(test_stress)} æ¡ - æç®€æŒ‡ä»¤ï¼ˆå¦‚'å˜æ–‡é›…'ï¼‰ï¼Œæµ‹è¯•æ³›åŒ–")
    print(f"  â””â”€ å¯¹æŠ—æµ‹è¯•: {len(test_adversarial)} æ¡ - è¦æ±‚åŸæ ·è¾“å‡ºï¼Œæµ‹è¯•æŒ‡ä»¤éµå¾ª")
    print(f"\nğŸ’¡ è®­ç»ƒå»ºè®®:")
    print(f"  - Learning Rate: 5e-5 (é™ä½ä»¥ä¾¿æ…¢æ…¢ç†è§£)")
    print(f"  - Epochs: 2.0 (é¿å…è¿‡æ‹Ÿåˆ)")
    print(f"  - LoRA Rank: 64 (é‡ç†è§£åŠ›è€Œéå¤å†™åŠ›)")
    print("="*80)


if __name__ == "__main__":
    main()
