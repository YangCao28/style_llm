"""æ·»åŠ è´Ÿæ ·æœ¬ï¼ˆæ€»ç»“ä»»åŠ¡ï¼‰åˆ°è®­ç»ƒæ•°æ®ä¸­

ç”¨æ³•:
    python -m data_prep.add_negative_samples --input data/modern_pairs_5000.jsonl --output data/mixed_train.jsonl --negative_count 200
"""

import argparse
import json
import random
from pathlib import Path


SUMMARY_SYSTEM = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬æ‘˜è¦åŠ©æ‰‹ï¼Œæ“…é•¿æå–æ ¸å¿ƒè¦ç‚¹ã€‚"

SUMMARY_USER_TEMPLATE = "è¯·ç”¨ä¸€å¥è¯æ€»ç»“ä»¥ä¸‹å†…å®¹çš„æ ¸å¿ƒè¦ç‚¹ï¼š\n\n{text}"


def create_summary(original_text: str) -> str:
    """æ ¹æ®åŸæ–‡åˆ›å»ºç®€çŸ­æ‘˜è¦ï¼ˆè§„åˆ™ç”Ÿæˆï¼Œé¿å…è°ƒç”¨APIï¼‰"""
    # ç®€å•ç­–ç•¥ï¼šæå–å‰50å­— + "ç­‰å†…å®¹" 
    # å®é™…ä½¿ç”¨æ—¶å¯ä»¥è°ƒç”¨LLMç”Ÿæˆæ›´å¥½çš„æ‘˜è¦
    text = original_text.strip()
    
    # å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¥å­
    for delimiter in ['ã€‚', 'ï¼', 'ï¼Ÿ']:
        if delimiter in text[:100]:
            first_sentence = text[:text.find(delimiter) + 1]
            if len(first_sentence) > 20:
                return f"æœ¬æ®µä¸»è¦è®²è¿°äº†{first_sentence[:50]}çš„ç›¸å…³å†…å®¹ã€‚"
    
    # å¦‚æœæ²¡æ‰¾åˆ°å¥å·ï¼Œå°±å–å‰30å­—
    return f"æœ¬æ®µä¸»è¦è®²è¿°äº†{text[:30]}ç­‰å†…å®¹ã€‚"


def main():
    parser = argparse.ArgumentParser(description="æ·»åŠ è´Ÿæ ·æœ¬åˆ°è®­ç»ƒæ•°æ®")
    parser.add_argument("--input", required=True, help="åŸå§‹è®­ç»ƒæ•°æ®")
    parser.add_argument("--output", required=True, help="æ··åˆåçš„è¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--negative_count", type=int, default=200, help="è´Ÿæ ·æœ¬æ•°é‡")
    args = parser.parse_args()

    input_file = Path(args.input)
    output_file = Path(args.output)

    print(f"ğŸ“– è¯»å–åŸå§‹æ•°æ®: {input_file}")
    with open(input_file, 'r', encoding='utf-8') as f:
        positive_samples = [json.loads(line) for line in f]

    print(f"âœ“ åŠ è½½ {len(positive_samples)} æ¡æ­£æ ·æœ¬")

    # éšæœºé€‰æ‹©Næ¡æ ·æœ¬åˆ›å»ºè´Ÿæ ·æœ¬
    selected = random.sample(positive_samples, min(args.negative_count, len(positive_samples)))
    
    negative_samples = []
    for item in selected:
        # ä»åŸå§‹æ ·æœ¬ä¸­æå–æ–‡å­¦åŸæ–‡
        original_text = item["conversations"][2]["content"]
        
        # åˆ›å»ºæ€»ç»“ä»»åŠ¡
        summary = create_summary(original_text)
        
        negative_sample = {
            "source_index": item["source_index"],
            "record_id": f"{item['record_id']}_summary",
            "conversations": [
                {"role": "system", "content": SUMMARY_SYSTEM},
                {"role": "user", "content": SUMMARY_USER_TEMPLATE.format(text=original_text)},
                {"role": "assistant", "content": summary}
            ]
        }
        negative_samples.append(negative_sample)

    print(f"âœ“ ç”Ÿæˆ {len(negative_samples)} æ¡è´Ÿæ ·æœ¬ï¼ˆæ€»ç»“ä»»åŠ¡ï¼‰")

    # åˆå¹¶å¹¶æ‰“ä¹±
    all_samples = positive_samples + negative_samples
    random.shuffle(all_samples)

    print(f"âœ“ æ··åˆåæ€»è®¡: {len(all_samples)} æ¡")

    # å†™å…¥è¾“å‡º
    output_file.parent.mkdir(parents=True, exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in all_samples:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    print(f"\nâœ… å®Œæˆï¼")
    print(f"   æ­£æ ·æœ¬: {len(positive_samples)}")
    print(f"   è´Ÿæ ·æœ¬: {len(negative_samples)}")
    print(f"   è¾“å‡º: {output_file}")


if __name__ == "__main__":
    main()
