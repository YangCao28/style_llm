"""è¯„ä¼°æ¨¡å‹åœ¨å¯¹æŠ—æµ‹è¯•é›†ä¸Šçš„è¡¨ç°

é‡ç‚¹æŒ‡æ ‡ï¼š
- ROUGE-L: è¡¡é‡åŸæ ·è¿”å›çš„å‡†ç¡®åº¦ï¼ˆæœŸæœ›æ¥è¿‘1.0ï¼‰
- æ”¹å†™ç‡: å¤šå°‘æ ·æœ¬è¢«é”™è¯¯æ”¹å†™äº†ï¼ˆæœŸæœ›æ¥è¿‘0%ï¼‰

ç”¨æ³•ï¼š
    python -m lora.evaluate_adversarial --base_model Qwen3-8B-Base --lora_model single_stage_v2 --test_file data/split/test_adversarial_200.jsonl
"""

import argparse
import json
from pathlib import Path
from typing import List, Dict

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from rouge import Rouge


def extract_modern_text(user_content: str) -> str:
    """ä»user contentä¸­æå–ç°ä»£ç™½è¯æ–‡æœ¬"""
    # ç§»é™¤å„ç§å¯èƒ½çš„æŒ‡ä»¤å‰ç¼€
    prefixes = [
        "è¯·åŸæ ·è¾“å‡ºä»¥ä¸‹å†…å®¹ï¼Œä¸è¦åšä»»ä½•ä¿®æ”¹ï¼š\n\n",
        "ç›´æ¥è¿”å›ä¸‹é¢çš„æ–‡å­—ï¼Œä¿æŒåŸæ ·ï¼š\n\n",
        "è¯·ä¸€å­—ä¸æ”¹åœ°è¾“å‡ºä»¥ä¸‹å†…å®¹ï¼š\n\n",
        "ä¸è¦ä¿®æ”¹ï¼ŒåŸæ ·è¿”å›ï¼š\n\n",
        "è¯·ä¿æŒåŸæ–‡ä¸å˜ï¼Œç›´æ¥è¾“å‡ºï¼š\n\n",
    ]
    
    for prefix in prefixes:
        if prefix in user_content:
            return user_content[len(prefix):]
    
    return user_content


def build_prompt(system: str, user: str) -> str:
    """æ„å»ºQwen Chatæ ¼å¼çš„prompt"""
    return f"<|im_start|>system\n{system}<|im_end|>\n<|im_start|>user\n{user}<|im_end|>\n<|im_start|>assistant\n"


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--base_model", default="Qwen3-8B-Base", help="åŸºåº§æ¨¡å‹è·¯å¾„")
    parser.add_argument("--lora_model", required=True, help="LoRAæ¨¡å‹è·¯å¾„")
    parser.add_argument("--test_file", required=True, help="å¯¹æŠ—æµ‹è¯•é›†æ–‡ä»¶")
    parser.add_argument("--max_new_tokens", type=int, default=512, help="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    parser.add_argument("--output", type=str, default=None, help="ä¿å­˜è¯¦ç»†ç»“æœçš„æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰")
    args = parser.parse_args()

    print(f"ğŸ”§ é…ç½®:")
    print(f"  Base Model: {args.base_model}")
    print(f"  LoRA Model: {args.lora_model}")
    print(f"  Test File: {args.test_file}")
    
    # åŠ è½½æ¨¡å‹
    print(f"\nğŸ“¦ Loading model...")
    tokenizer = AutoTokenizer.from_pretrained(
        args.base_model,
        trust_remote_code=True,
        local_files_only=True
    )
    
    base_model = AutoModelForCausalLM.from_pretrained(
        args.base_model,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        local_files_only=True
    )
    
    model = PeftModel.from_pretrained(base_model, args.lora_model, torch_dtype=torch.bfloat16)
    model.eval()
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    print(f"\nğŸ“Š Loading test data...")
    with open(args.test_file, 'r', encoding='utf-8') as f:
        test_data = [json.loads(line) for line in f]
    
    print(f"âœ“ Loaded {len(test_data)} samples")
    
    # è¯„ä¼°
    print(f"\nğŸ§ª Evaluating...")
    rouge = Rouge()
    results = []
    rouge_scores = []
    rewrite_count = 0
    
    for i, sample in enumerate(test_data):
        system = sample["conversations"][0]["content"]
        user = sample["conversations"][1]["content"]
        expected = sample["conversations"][2]["content"]  # åº”è¯¥æ˜¯åŸæ ·è¿”å›çš„ç°ä»£ç™½è¯
        
        # æ„å»ºprompt
        prompt = build_prompt(system, user)
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        # ç”Ÿæˆ
        with torch.no_grad():
            output_ids = model.generate(
                **inputs,
                max_new_tokens=args.max_new_tokens,
                do_sample=False,  # ä½¿ç”¨è´ªå¿ƒè§£ç ï¼Œæ›´ç¨³å®š
                pad_token_id=tokenizer.pad_token_id,
            )
        
        # è§£ç 
        generated = tokenizer.decode(output_ids[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
        
        # æ¸…ç†
        if "<|im_end|>" in generated:
            generated = generated[:generated.find("<|im_end|>")]
        generated = generated.strip()
        
        # è®¡ç®—ROUGE-L
        try:
            score = rouge.get_scores(generated, expected)[0]["rouge-l"]["f"]
            rouge_scores.append(score)
        except:
            score = 0.0
            rouge_scores.append(0.0)
        
        # åˆ¤æ–­æ˜¯å¦è¢«é”™è¯¯æ”¹å†™ï¼ˆROUGE-L < 0.8 è®¤ä¸ºæ˜¯æ”¹å†™äº†ï¼‰
        if score < 0.8:
            rewrite_count += 1
        
        results.append({
            "index": i,
            "record_id": sample.get("record_id"),
            "expected": expected[:100],
            "generated": generated[:100],
            "rouge_l": score,
            "is_rewritten": score < 0.8
        })
        
        if (i + 1) % 50 == 0:
            print(f"  è¿›åº¦: {i+1}/{len(test_data)}")
    
    # ç»Ÿè®¡
    avg_rouge = sum(rouge_scores) / len(rouge_scores)
    rewrite_rate = rewrite_count / len(test_data) * 100
    
    print(f"\n" + "="*80)
    print("ğŸ“Š è¯„ä¼°ç»“æœ")
    print("="*80)
    print(f"\næ€»æ ·æœ¬æ•°: {len(test_data)}")
    print(f"å¹³å‡ ROUGE-L: {avg_rouge:.4f} (æœŸæœ›æ¥è¿‘1.0)")
    print(f"é”™è¯¯æ”¹å†™æ•°: {rewrite_count} / {len(test_data)}")
    print(f"é”™è¯¯æ”¹å†™ç‡: {rewrite_rate:.2f}% (æœŸæœ›æ¥è¿‘0%)")
    
    print(f"\nâœ… è¯„ä¼°æ ‡å‡†:")
    if avg_rouge >= 0.95:
        print(f"  ROUGE-L â‰¥ 0.95: ğŸ‰ ä¼˜ç§€ï¼æ¨¡å‹å®Œç¾ç†è§£äº†å¯¹æŠ—æŒ‡ä»¤")
    elif avg_rouge >= 0.85:
        print(f"  ROUGE-L â‰¥ 0.85: âœ… è‰¯å¥½ï¼Œæ¨¡å‹åŸºæœ¬æŒæ¡äº†æŒ‡ä»¤éµå¾ª")
    elif avg_rouge >= 0.70:
        print(f"  ROUGE-L â‰¥ 0.70: âš ï¸  ä¸€èˆ¬ï¼Œæ¨¡å‹éƒ¨åˆ†ç†è§£æŒ‡ä»¤")
    else:
        print(f"  ROUGE-L < 0.70: âŒ å¤±è´¥ï¼Œæ¨¡å‹ä»åœ¨å¿½ç•¥æŒ‡ä»¤")
    
    if rewrite_rate <= 5:
        print(f"  æ”¹å†™ç‡ â‰¤ 5%: ğŸ‰ ä¼˜ç§€ï¼å‡ ä¹ä¸è¯¯æ”¹")
    elif rewrite_rate <= 15:
        print(f"  æ”¹å†™ç‡ â‰¤ 15%: âœ… å¯æ¥å—")
    else:
        print(f"  æ”¹å†™ç‡ > 15%: âš ï¸  éœ€è¦æ”¹è¿›")
    
    # æ˜¾ç¤ºå‡ ä¸ªå…¸å‹æ¡ˆä¾‹
    print(f"\nğŸ“ å…¸å‹æ¡ˆä¾‹:")
    print("="*80)
    
    # æ‰¾å‡ºæœ€å¥½å’Œæœ€å·®çš„3ä¸ª
    sorted_results = sorted(results, key=lambda x: x["rouge_l"], reverse=True)
    
    print(f"\nâœ… æœ€å¥½çš„3ä¸ªæ¡ˆä¾‹:")
    for i, r in enumerate(sorted_results[:3]):
        print(f"\næ¡ˆä¾‹ {i+1} (ROUGE-L: {r['rouge_l']:.4f})")
        print(f"  æœŸæœ›: {r['expected']}")
        print(f"  å®é™…: {r['generated']}")
    
    print(f"\nâŒ æœ€å·®çš„3ä¸ªæ¡ˆä¾‹:")
    for i, r in enumerate(sorted_results[-3:]):
        print(f"\næ¡ˆä¾‹ {i+1} (ROUGE-L: {r['rouge_l']:.4f})")
        print(f"  æœŸæœ›: {r['expected']}")
        print(f"  å®é™…: {r['generated']}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    if args.output:
        output_file = Path(args.output)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "summary": {
                    "total_samples": len(test_data),
                    "avg_rouge_l": avg_rouge,
                    "rewrite_count": rewrite_count,
                    "rewrite_rate": rewrite_rate
                },
                "details": results
            }, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    print("="*80)


if __name__ == "__main__":
    main()
