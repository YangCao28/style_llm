"""LoRA stage-1 (style injection) training script for 45k x 1024-token corpus.

è®­ç»ƒæµç¨‹è¯´æ˜ï¼š
1. åŠ è½½åŸå§‹JSONLæ•°æ®ï¼ˆæ¯æ¡è®°å½•åŒ…å«"text"å­—æ®µï¼‰
2. å¯¹æ¯æ¡æ–‡æœ¬è¿›è¡Œæ ¼å¼åŒ–ï¼šæ·»åŠ æç¤ºæ¨¡æ¿ï¼ˆ"### æ–‡é£è¯­æ–™\nã€packed_style_sampleã€‘\n### æ­£æ–‡\n"ï¼‰+ æ­£æ–‡å†…å®¹
3. Tokenizeæ‰€æœ‰æ–‡æœ¬ï¼Œä¸padding
4. åœ¨DataLoaderé˜¶æ®µä½¿ç”¨PackingDataCollatorï¼š
   - ä»batchä¸­çš„å¤šä¸ªæ ·æœ¬æ‹¼æ¥token ids
   - æ¯ä¸ªæ ·æœ¬ä¹‹é—´æ’å…¥EOS tokenä½œä¸ºåˆ†éš”
   - æ‰“åŒ…æˆ4096 tokençš„å®Œæ•´åºåˆ—
   - åŠ¨æ€ç”Ÿæˆattention_maskå’Œlabels
5. ä½¿ç”¨LoRAè®­ç»ƒï¼ˆr=128, alpha=256ï¼‰ï¼Œåªè®­ç»ƒq/k/v/o/gate/up/downæŠ•å½±å±‚
6. BF16æ··åˆç²¾åº¦ + gradient checkpointingèŠ‚çœæ˜¾å­˜

æ•ˆç‡ä¼˜åŠ¿ï¼š
- Packingé¿å…paddingæµªè´¹ï¼ŒGPUåˆ©ç”¨ç‡æ¥è¿‘100%
- LoRAåªè®­ç»ƒ~2%å‚æ•°ï¼Œæ˜¾å­˜å ç”¨ä½
- å•å¡A100/A800å¯ä»¥è·‘batch_size=8 * grad_accum=16 = æœ‰æ•ˆbatch 128

Usage:
    python -m lora.stage1_style_injection --config lora/stage1_style_config.example.json
"""

from __future__ import annotations

import argparse
import json
import re
from pathlib import Path
from typing import Dict, List

import matplotlib.pyplot as plt
import torch
from datasets import load_dataset
from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, TrainerCallback

DEFAULT_MODEL = "Qwen/Qwen3-8B-Base"
RESPONSE_TEMPLATE = "### æ­£æ–‡\n"
PROMPT_TEMPLATE = "### æ–‡é£è¯­æ–™\n"
MERGED_TOKEN_STATEMENT = "ã€packed_style_sampleã€‘\n"

# æ¸…ç†æ–‡æœ¬çš„æ­£åˆ™è¡¨è¾¾å¼
META_PATTERNS = [
    re.compile(r"ä½œè€…[:ï¼š].*?$", re.MULTILINE),
    re.compile(r"^\s*ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+[ç« å›èŠ‚].*?$", re.MULTILINE),
    re.compile(r"https?://\S+", re.IGNORECASE),
]
WHITESPACE_RE = re.compile(r"[\s\u3000]+")


def parse_args() -> argparse.Namespace:
    config_parser = argparse.ArgumentParser(add_help=False)
    config_parser.add_argument("--config", type=Path, help="Path to JSON config file.")
    config_args, remaining_argv = config_parser.parse_known_args()

    parser = argparse.ArgumentParser(
        description="Stage-1 LoRA style injection training",
        parents=[config_parser],
    )
    parser.add_argument("--model_name_or_path", default=DEFAULT_MODEL)
    parser.add_argument(
        "--dataset_path",
        type=Path,
        default=Path("data/dataset/combined_dataset.jsonl"),
    )
    parser.add_argument("--output_dir", type=Path, default=Path("./stage1_style_injection"))
    parser.add_argument("--logging_steps", type=int, default=10)
    parser.add_argument("--save_steps", type=int, default=100)
    parser.add_argument("--streaming", action="store_true")
    parser.add_argument("--per_device_train_batch_size", type=int, default=16)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=16)
    parser.add_argument("--learning_rate", type=float, default=1e-4)
    parser.add_argument("--warmup_ratio", type=float, default=0.03)
    parser.add_argument("--warmup_steps", type=int, default=0)
    parser.add_argument("--num_train_epochs", type=float, default=3.0)
    parser.add_argument("--max_seq_length", type=int, default=4096)
    parser.add_argument("--attn_impl", type=str, default="flash_attention_2")
    # LoRA ç›¸å…³è¶…å‚ï¼Œä¾¿äºåç»­åš Alpha å¢å¼ºè®­ç»ƒ
    parser.add_argument("--lora_r", type=int, default=128)
    parser.add_argument("--lora_alpha", type=int, default=256)
    # ç»­è·‘ç”¨çš„ checkpoint è·¯å¾„ï¼ˆå¯åœ¨ JSON config é‡ŒæŒ‡å®šï¼‰
    parser.add_argument("--resume_from_checkpoint", type=str, default=None)

    if config_args.config:
        if not config_args.config.exists():
            raise FileNotFoundError(config_args.config)
        with config_args.config.open("r", encoding="utf-8") as f:
            config_data = json.load(f)
        normalized_defaults: Dict[str, object] = {}
        for key, raw_value in config_data.items():
            matched_action = next((a for a in parser._actions if a.dest == key), None)
            if matched_action is None:
                raise ValueError(f"Unknown config key '{key}'")
            converter = matched_action.type
            if converter and raw_value is not None and not isinstance(raw_value, converter):
                normalized_defaults[key] = converter(raw_value)
            else:
                normalized_defaults[key] = raw_value
        parser.set_defaults(**normalized_defaults)

    args = parser.parse_args(remaining_argv)
    return args


def normalize_text(record: Dict[str, str]) -> str:
    """æ¸…ç†æ–‡æœ¬ï¼šå»é™¤ç« èŠ‚æ ‡é¢˜ã€ä½œè€…ä¿¡æ¯ã€URLç­‰å…ƒæ•°æ®"""
    text = record.get("text", "")
    for pattern in META_PATTERNS:
        text = pattern.sub("", text)
    text = text.replace("\r", "\n")
    text = WHITESPACE_RE.sub(" ", text)
    text = text.strip()
    if not text:
        return ""
    paragraphs = [line.strip() for line in text.split("\n") if line.strip()]
    return "\n\n".join(paragraphs)


def formatting_func(record: Dict[str, str]) -> str:
    """æ ¼å¼åŒ–ä¸ºè®­ç»ƒæ¨¡æ¿ï¼šæç¤ºè¯ + æ­£æ–‡"""
    body = normalize_text(record)
    if not body:
        return ""
    return f"{PROMPT_TEMPLATE}{MERGED_TOKEN_STATEMENT}{RESPONSE_TEMPLATE}{body}"


class PackingDataCollator:
    """
    åŠ¨æ€æ‰“åŒ…æ•°æ®collatorï¼š
    - å°†batchä¸­çš„å¤šä¸ªæ ·æœ¬çš„token idsæ‹¼æ¥èµ·æ¥
    - æ¯ä¸ªæ ·æœ¬ä¹‹é—´æ’å…¥EOS token
    - æ‰“åŒ…æˆmax_lengthé•¿åº¦çš„åºåˆ—
    - è‡ªåŠ¨ç”Ÿæˆattention_maskå’Œlabelsï¼ˆpaddingä½ç½®label=-100ï¼‰
    """
    
    def __init__(self, tokenizer, max_length: int = 4096):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.pad_token_id = tokenizer.pad_token_id
        self.eos_token_id = tokenizer.eos_token_id
    
    def __call__(self, examples):
        # æ‹¼æ¥batchä¸­æ‰€æœ‰æ ·æœ¬çš„token ids
        all_input_ids = []
        for example in examples:
            ids = example["input_ids"]
            if isinstance(ids, torch.Tensor):
                ids = ids.tolist()
            all_input_ids.extend(ids)
            all_input_ids.append(self.eos_token_id)  # æ ·æœ¬é—´æ’å…¥EOS
        
        # åˆ‡åˆ†æˆmax_lengthçš„chunk
        packed_sequences = []
        for i in range(0, len(all_input_ids), self.max_length):
            chunk = all_input_ids[i : i + self.max_length]
            if len(chunk) == self.max_length:
                packed_sequences.append(chunk)
            elif chunk:  # æœ€åä¸€ä¸ªä¸å®Œæ•´çš„chunk
                chunk = chunk + [self.pad_token_id] * (self.max_length - len(chunk))
                packed_sequences.append(chunk)
        
        if not packed_sequences:
            # Fallbackï¼šè‡³å°‘è¿”å›ä¸€ä¸ªåºåˆ—
            packed_sequences = [[self.pad_token_id] * self.max_length]
        
        # è½¬tensor
        input_ids = torch.tensor(packed_sequences, dtype=torch.long)
        attention_mask = (input_ids != self.pad_token_id).long()
        labels = input_ids.clone()
        labels[labels == self.pad_token_id] = -100  # paddingä¸å‚ä¸loss
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels,
        }

class LossRecorderCallback(TrainerCallback):
    """è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„ loss å€¼"""
    
    def __init__(self):
        self.training_losses: List[float] = []
        self.steps: List[int] = []
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs and "loss" in logs:
            self.training_losses.append(logs["loss"])
            self.steps.append(state.global_step)


def plot_loss_curve(losses: List[float], steps: List[int], output_dir: Path) -> None:
    """ç»˜åˆ¶å¹¶ä¿å­˜ loss æ›²çº¿å›¾"""
    if not losses:
        print("âš  æ²¡æœ‰ loss æ•°æ®ï¼Œè·³è¿‡ç»˜å›¾")
        return
    
    plt.figure(figsize=(12, 6))
    plt.plot(steps, losses, 'b-', linewidth=1.5, alpha=0.8)
    plt.xlabel('Training Steps', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.title('Training Loss Curve', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    loss_plot_path = output_dir / "loss_curve.png"
    plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ Loss æ›²çº¿å›¾å·²ä¿å­˜åˆ°: {loss_plot_path}")
    
    # ä¿å­˜ loss æ•°æ®åˆ° JSON
    loss_data = {
        "steps": steps,
        "losses": losses,
        "min_loss": min(losses),
        "final_loss": losses[-1],
        "total_steps": len(steps)
    }
    loss_json_path = output_dir / "loss_history.json"
    with open(loss_json_path, 'w', encoding='utf-8') as f:
        json.dump(loss_data, f, indent=2)
    print(f"âœ“ Loss æ•°æ®å·²ä¿å­˜åˆ°: {loss_json_path}")
    
    plt.close()



def main() -> None:
    args = parse_args()
    
    # æ¸…ç†GPUç¼“å­˜ï¼Œé‡Šæ”¾æ®‹ç•™æ˜¾å­˜
    import torch
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print(f"âœ“ GPUç¼“å­˜å·²æ¸…ç†")
    
    # 1. åŠ è½½æ•°æ®é›†
    print(f"Loading dataset from {args.dataset_path}")
    dataset = load_dataset(
        "json",
        data_files=str(args.dataset_path),
        split="train",
        streaming=args.streaming,
    )
    
    # ç»Ÿè®¡æ•°æ®é›†å¤§å°
    if not args.streaming:
        dataset_size = len(dataset)
        print(f"âœ“ æ•°æ®é›†åŠ è½½å®Œæˆï¼šå…± {dataset_size:,} æ¡è®­ç»ƒæ ·æœ¬")
    else:
        print(f"âœ“ æ•°æ®é›†åŠ è½½å®Œæˆï¼šæµå¼æ¨¡å¼ï¼ˆæ— æ³•æå‰ç»Ÿè®¡æ€»æ•°ï¼‰")
    
    # 2. åŠ è½½tokenizer
    print(f"\nLoading tokenizer from {args.model_name_or_path}")
    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)
    tokenizer.padding_side = "right"
    tokenizer.model_max_length = args.max_seq_length
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    print(f"âœ“ TokenizeråŠ è½½å®Œæˆ")
    
    # 3. åŠ è½½æ¨¡å‹
    print(f"\nLoading model from {args.model_name_or_path}")
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name_or_path,
        torch_dtype="auto",
        device_map="auto",
        attn_implementation=args.attn_impl,
    )
    print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # 4. åº”ç”¨LoRA
    print("\nApplying LoRA configuration")
    lora_config = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # 5. Tokenizeæ•°æ®é›†ï¼ˆä¸paddingï¼Œä¿æŒåŸå§‹é•¿åº¦ï¼‰
    print("\nTokenizing dataset...")
    def tokenize_function(examples):
        texts = [formatting_func({"text": text}) for text in examples["text"]]
        return tokenizer(
            texts,
            Loss è®°å½•å™¨å’Œ Trainer
    loss_recorder = LossRecorderCallback()
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
        callbacks=[loss_recorder]
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset.column_names,
    )
    
    # æ‰“å°tokenizeåçš„ç»Ÿè®¡ä¿¡æ¯
    if not args.streaming:
        print(f"âœ“ Tokenizationå®Œæˆï¼š{len(tokenized_dataset):,} æ¡æ ·æœ¬å·²è½¬æ¢ä¸ºtoken ids")
        estimated_steps = len(tokenized_dataset) // (args.per_device_train_batch_size * args.gradient_accumulation_steps)
        print(f"  é¢„è®¡è®­ç»ƒæ­¥æ•°ï¼š{estimated_steps} steps")
    else:
        print(f"âœ“ Tokenizationå®Œæˆï¼ˆæµå¼æ¨¡å¼ï¼‰")
    
    # 6. åˆ›å»ºpacking collator
    data_collator = PackingDataCollator(
        tokenizer=tokenizer,
        max_length=args.max_seq_length,
    )
    
    # 7. è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=str(args.output_dir),
    
    # 11. ç»˜åˆ¶å¹¶ä¿å­˜ loss æ›²çº¿
    print("\nç”Ÿæˆ Loss æ›²çº¿å›¾...")
    plot_loss_curve(loss_recorder.training_losses, loss_recorder.steps, args.output_dir)
    print(f"\nğŸ“Š è®­ç»ƒæ‘˜è¦:")
    if loss_recorder.training_losses:
        print(f"  æœ€å° Loss: {min(loss_recorder.training_losses):.4f}")
        print(f"  æœ€ç»ˆ Loss: {loss_recorder.training_losses[-1]:.4f}")
        print(f"  æ€»è®­ç»ƒæ­¥æ•°: {len(loss_recorder.steps)}")
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        lr_scheduler_type="cosine",
        warmup_steps=args.warmup_steps,
        num_train_epochs=args.num_train_epochs,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        save_total_limit=5,
        bf16=True,
        tf32=True,
        optim="adamw_torch_fused",
        max_grad_norm=1.0,
        gradient_checkpointing=True,
        report_to=[],
        dataloader_drop_last=True,
        ddp_find_unused_parameters=False,
    )
    
    # 8. åˆ›å»ºTrainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
    )
    
    # 9. å¼€å§‹è®­ç»ƒï¼ˆå¦‚æä¾› resume_from_checkpointï¼Œåˆ™ä»å¯¹åº” checkpoint ç»­è·‘ï¼‰
    if not args.streaming:
        effective_batch = args.per_device_train_batch_size * args.gradient_accumulation_steps
        print("\n" + "="*70)
        print(f"å¼€å§‹è®­ç»ƒ Stage-1 é£æ ¼æ³¨å…¥")
        print(f"  è®­ç»ƒæ•°æ®ï¼š{len(tokenized_dataset):,} æ¡æ ·æœ¬")
        print(f"  Batch sizeï¼š{args.per_device_train_batch_size} Ã— {args.gradient_accumulation_steps} = {effective_batch}")
        print(f"  å­¦ä¹ ç‡ï¼š{args.learning_rate}")
        print(f"  è®­ç»ƒè½®æ•°ï¼š{args.num_train_epochs}")
        print(f"  é¢„è®¡æ­¥æ•°ï¼š~{len(tokenized_dataset) // effective_batch} steps")
        print(f"  ä¿å­˜é—´éš”ï¼šæ¯ {args.save_steps} steps")
        print("="*70 + "\n")
    else:
        print("\n" + "="*70)
        print(f"å¼€å§‹è®­ç»ƒ Stage-1 é£æ ¼æ³¨å…¥ï¼ˆæµå¼æ¨¡å¼ï¼‰")
        print("="*70 + "\n")
    
    if args.resume_from_checkpoint:
        print(f"ä» checkpoint ç»­è·‘: {args.resume_from_checkpoint}")
        trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)
    else:
        trainer.train()
    
    # 10. ä¿å­˜æ¨¡å‹å’Œtokenizer
    print(f"\nä¿å­˜æ¨¡å‹åˆ° {args.output_dir}")
    trainer.save_model()
    tokenizer.save_pretrained(args.output_dir)
    print("âœ“ è®­ç»ƒå®Œæˆ!")


if __name__ == "__main__":
    main()
