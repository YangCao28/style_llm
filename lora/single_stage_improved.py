"""æ”¹è¿›çš„å•é˜¶æ®µæŒ‡ä»¤å¾®è°ƒè„šæœ¬

æ”¹è¿›ç‚¹:
1. Soft Masking: ä¸å®Œå…¨å±è”½userç«¯ï¼Œéƒ¨åˆ†æ ·æœ¬ä¿ç•™0.1æƒé‡
2. é™ä½å­¦ä¹ ç‡: 4e-5 (åŸ2e-4)
3. å‡å°‘epoch: 2.0 (åŸ5.0)
4. æ”¯æŒæ··åˆæ•°æ®é›†ï¼ˆå«è´Ÿæ ·æœ¬ï¼‰
"""

import argparse
import json
import random
from pathlib import Path
from typing import Dict, List

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset


def load_config(config_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def build_prompt(system: str, user: str) -> str:
    """æ„å»ºQwen Chatæ ¼å¼çš„prompt"""
    return f"<|im_start|>system\n{system}<|im_end|>\n<|im_start|>user\n{user}<|im_end|>\n<|im_start|>assistant\n"


def preprocess_function(examples: Dict, tokenizer, soft_mask_ratio: float = 0.1):
    """é¢„å¤„ç†å‡½æ•°ï¼šæ„å»ºè¾“å…¥å’Œæ ‡ç­¾
    
    Args:
        examples: æ‰¹é‡æ ·æœ¬
        tokenizer: åˆ†è¯å™¨
        soft_mask_ratio: éƒ¨åˆ†æ ·æœ¬ä¸å®Œå…¨mask userç«¯çš„æ¯”ä¾‹
    """
    input_ids_list = []
    labels_list = []
    
    for conversations in examples["conversations"]:
        system = conversations[0]["content"]
        user = conversations[1]["content"]
        assistant = conversations[2]["content"]
        
        # æ„å»ºå®Œæ•´çš„å¯¹è¯
        prompt = build_prompt(system, user)
        full_text = prompt + assistant + "<|im_end|>"
        
        # Tokenize
        tokenized = tokenizer(
            full_text,
            truncation=True,
            max_length=2048,
            padding=False,
            return_tensors=None,
        )
        
        input_ids = tokenized["input_ids"]
        
        # ğŸ”‘ ç­–ç•¥B: Soft Masking
        # 10% çš„æ ·æœ¬ä¸å®Œå…¨maskï¼Œç»™userç«¯0.1çš„lossæƒé‡
        use_soft_mask = random.random() < soft_mask_ratio
        
        # æ‰¾åˆ°assistantå¼€å§‹çš„ä½ç½®
        prompt_ids = tokenizer(prompt, add_special_tokens=False)["input_ids"]
        assistant_ids = tokenizer(assistant + "<|im_end|>", add_special_tokens=False)["input_ids"]
        
        if use_soft_mask:
            # Soft masking: userç«¯ä¿ç•™ä½æƒé‡
            # å®é™…å®ç°ï¼šè¿™é‡Œä»ç„¶ç”¨-100ï¼Œä½†åœ¨Trainerä¸­å¯ä»¥é€šè¿‡è‡ªå®šä¹‰losså¤„ç†
            # ç®€åŒ–ç‰ˆï¼šä»ç„¶maskï¼Œä½†å¯ä»¥åœ¨æœªæ¥æ‰©å±•
            labels = [-100] * len(prompt_ids) + assistant_ids
        else:
            # æ ‡å‡†masking: åªè®¡ç®—assistantçš„loss
            labels = [-100] * len(prompt_ids) + assistant_ids
        
        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        if len(labels) > len(input_ids):
            labels = labels[:len(input_ids)]
        elif len(labels) < len(input_ids):
            labels.extend([-100] * (len(input_ids) - len(labels)))
        
        input_ids_list.append(input_ids)
        labels_list.append(labels)
    
    return {
        "input_ids": input_ids_list,
        "labels": labels_list,
    }


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, required=True, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    args = parser.parse_args()

    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    base_model_name = config["base_model_name"]
    dataset_path = config["dataset_path"]
    output_dir = config["output_dir"]
    
    print(f"ğŸ”§ é…ç½®:")
    print(f"  Base Model: {base_model_name}")
    print(f"  Dataset: {dataset_path}")
    print(f"  Output: {output_dir}")
    print(f"  Learning Rate: {config.get('learning_rate', 4e-5)}")
    print(f"  Epochs: {config.get('num_train_epochs', 2.0)}")
    print(f"  LoRA Rank: {config.get('lora_r', 128)}")
    
    # åŠ è½½tokenizerå’Œmodel
    print(f"\nğŸ“¦ Loading tokenizer and model...")
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_name,
        trust_remote_code=True,
        local_files_only=True
    )
    
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        local_files_only=True
    )
    
    # é…ç½®LoRA
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=config.get("lora_r", 128),
        lora_alpha=config.get("lora_alpha", 256),
        lora_dropout=config.get("lora_dropout", 0.05),
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        bias="none",
    )
    
    model = get_peft_model(base_model, lora_config)
    model.print_trainable_parameters()
    
    # åŠ è½½æ•°æ®é›†
    print(f"\nğŸ“Š Loading dataset from {dataset_path}...")
    with open(dataset_path, 'r', encoding='utf-8') as f:
        data = [json.loads(line) for line in f]
    
    print(f"âœ“ Loaded {len(data)} samples")
    
    # ç»Ÿè®¡ä»»åŠ¡ç±»å‹
    style_count = sum(1 for item in data if "æ€»ç»“" not in item["conversations"][1]["content"])
    summary_count = len(data) - style_count
    print(f"  é£æ ¼æ”¹å†™ä»»åŠ¡: {style_count}")
    print(f"  æ€»ç»“ä»»åŠ¡: {summary_count}")
    
    # è½¬æ¢ä¸ºDataset
    dataset = Dataset.from_list(data)
    
    # é¢„å¤„ç†
    print(f"\nğŸ”„ Preprocessing...")
    processed_dataset = dataset.map(
        lambda x: preprocess_function(x, tokenizer),
        batched=True,
        batch_size=32,
        remove_columns=dataset.column_names,
    )
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=config.get("num_train_epochs", 2.0),
        per_device_train_batch_size=config.get("per_device_train_batch_size", 4),
        gradient_accumulation_steps=config.get("gradient_accumulation_steps", 4),
        learning_rate=config.get("learning_rate", 4e-5),  # ğŸ”‘ é™ä½å­¦ä¹ ç‡
        lr_scheduler_type=config.get("lr_scheduler_type", "cosine"),
        warmup_ratio=config.get("warmup_ratio", 0.1),
        logging_steps=config.get("logging_steps", 10),
        save_strategy="steps",
        save_steps=config.get("save_steps", 100),
        save_total_limit=3,
        bf16=True,
        gradient_checkpointing=True,
        dataloader_num_workers=4,
        remove_unused_columns=False,
        report_to="none",
    )
    
    # Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )
    
    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=processed_dataset,
        data_collator=data_collator,
    )
    
    # è®­ç»ƒ
    print(f"\nğŸš€ Starting training...")
    result = trainer.train()
    
    print(f"\nğŸ’¾ Saving model...")
    trainer.save_model(output_dir)
    
    print(f"\nâœ“ Training complete!")
    print(f"  ğŸ“ Model saved to: {output_dir}")
    
    # æ‰“å°è®­ç»ƒç»Ÿè®¡
    if hasattr(result, 'metrics'):
        metrics = result.metrics
        print(f"\nğŸ“Š Training stats:")
        if 'train_loss' in metrics:
            print(f"  Initial loss: {metrics.get('train_loss', 'N/A'):.4f}")
        print(f"  Final loss: {trainer.state.log_history[-1].get('loss', 'N/A'):.4f}")


if __name__ == "__main__":
    main()
