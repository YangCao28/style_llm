"""æµ‹è¯•æ¨¡å‹åœ¨æ²¡æœ‰æŒ‡ä»¤çš„æƒ…å†µä¸‹æ˜¯å¦è¿˜èƒ½æ‰§è¡Œä»»åŠ¡

å¦‚æœæ¨¡å‹èƒ½åœ¨æ— æŒ‡ä»¤ä¸‹å®Œæˆé£æ ¼è½¬æ¢ï¼Œè¯´æ˜å®ƒåªæ˜¯åœ¨åšæ¨¡å¼åŒ¹é…ï¼Œè€ŒéçœŸæ­£çš„æŒ‡ä»¤éµå¾ªã€‚
"""

import argparse
import json
from pathlib import Path

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--base_model", default="Qwen3-8B-Base", help="åŸºåº§æ¨¡å‹è·¯å¾„")
    parser.add_argument("--lora_model", required=True, help="LoRAæ¨¡å‹è·¯å¾„")
    parser.add_argument("--test_data", default="data/modern_pairs_5000.jsonl", help="æµ‹è¯•æ•°æ®è·¯å¾„")
    parser.add_argument("--sample_line", type=int, default=10, help="æµ‹è¯•æ ·æœ¬çš„è¡Œå·ï¼ˆä»0å¼€å§‹ï¼‰")
    parser.add_argument("--max_new_tokens", type=int, default=512, help="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    args = parser.parse_args()

    print(f"ğŸ”§ é…ç½®:")
    print(f"  Base Model: {args.base_model}")
    print(f"  LoRA Model: {args.lora_model}")
    print(f"  Sample Line: {args.sample_line}")
    
    # åŠ è½½æ¨¡å‹
    print(f"\nğŸ“¦ Loading model...")
    tokenizer = AutoTokenizer.from_pretrained(
        args.base_model,
        trust_remote_code=True,
        local_files_only=True
    )
    
    base_model = AutoModelForCausalLM.from_pretrained(
        args.base_model,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        local_files_only=True
    )
    
    model = PeftModel.from_pretrained(base_model, args.lora_model, torch_dtype=torch.bfloat16)
    model.eval()
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    print(f"\nğŸ“Š Loading test data...")
    with open(args.test_data, 'r', encoding='utf-8') as f:
        samples = [json.loads(line) for line in f]
    
    # è¯»å–æŒ‡å®šè¡Œå·çš„æ ·æœ¬
    if args.sample_line >= len(samples):
        print(f"âŒ Line {args.sample_line} out of range! Total lines: {len(samples)}")
        return
    
    target_sample = samples[args.sample_line]
    
    print(f"âœ“ Loaded sample at line {args.sample_line}")
    print(f"  source_index: {target_sample.get('source_index', 'N/A')}")
    print(f"  record_id: {target_sample.get('record_id', 'N/A')}")
    user_content = target_sample["conversations"][1]["content"]
    modern_text_start = user_content.find("è¯·å°†ä»¥ä¸‹ç°ä»£ç™½è¯æ¶¦è‰²æˆé›…è‡´çš„æ–‡å­¦æ–‡æœ¬ï¼š\n\n")
    if modern_text_start != -1:
        modern_text = user_content[modern_text_start + len("è¯·å°†ä»¥ä¸‹ç°ä»£ç™½è¯æ¶¦è‰²æˆé›…è‡´çš„æ–‡å­¦æ–‡æœ¬ï¼š\n\n"):]
    else:
        modern_text = user_content
    
    expected_output = target_sample["conversations"][2]["content"]
    
    print(f"âœ“ Found sample {args.sample_id}")
    print(f"\n" + "="*80)
    print("å®éªŒè®¾è®¡:")
    print("="*80)
    print("å¦‚æœæ¨¡å‹åœ¨ã€æ— æŒ‡ä»¤ã€‘æƒ…å†µä¸‹ä»èƒ½è¾“å‡ºæ–‡è¨€é£æ ¼ï¼Œ")
    print("è¯´æ˜å®ƒåªå­¦åˆ°äº†'çœ‹åˆ°ç™½è¯â†’è¾“å‡ºæ–‡è¨€'çš„æ¨¡å¼åŒ¹é…ï¼Œ")
    print("è€ŒéçœŸæ­£ç†è§£æŒ‡ä»¤ã€‚")
    print("="*80)
    
    # æµ‹è¯•1: å®Œå…¨æ— æŒ‡ä»¤ï¼Œç›´æ¥è¾“å…¥ç°ä»£ç™½è¯
    print(f"\n" + "="*80)
    print("æµ‹è¯•1: å®Œå…¨æ— æŒ‡ä»¤ (è£¸æ–‡æœ¬è¾“å…¥)")
    print("="*80)
    prompt1 = modern_text
    print(f"\nè¾“å…¥ (å‰200å­—):")
    print(f"{prompt1[:200]}...")
    
    inputs1 = tokenizer(prompt1, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        output_ids1 = model.generate(
            **inputs1,
            max_new_tokens=args.max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.pad_token_id,
        )
    
    output1 = tokenizer.decode(output_ids1[0][len(inputs1.input_ids[0]):], skip_special_tokens=True)
    print(f"\nè¾“å‡º (å‰300å­—):")
    print(output1[:300])
    
    # æµ‹è¯•2: åªæœ‰systemï¼Œæ— user instruction
    print(f"\n" + "="*80)
    print("æµ‹è¯•2: åªæœ‰systemæ¶ˆæ¯ (æ— useræŒ‡ä»¤)")
    print("="*80)
    system_msg = target_sample["conversations"][0]["content"]
    prompt2 = f"<|im_start|>system\n{system_msg}<|im_end|>\n<|im_start|>user\n{modern_text}<|im_end|>\n<|im_start|>assistant\n"
    
    print(f"\nè¾“å…¥æ ¼å¼:")
    print(f"<system>{system_msg[:100]}...</system>")
    print(f"<user>{modern_text[:100]}... (æ— 'è¯·æ¶¦è‰²'ç­‰æŒ‡ä»¤)</user>")
    
    inputs2 = tokenizer(prompt2, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        output_ids2 = model.generate(
            **inputs2,
            max_new_tokens=args.max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.pad_token_id,
        )
    
    output2 = tokenizer.decode(output_ids2[0][len(inputs2.input_ids[0]):], skip_special_tokens=True)
    print(f"\nè¾“å‡º (å‰300å­—):")
    print(output2[:300])
    
    # æµ‹è¯•3: æ ‡å‡†æŒ‡ä»¤ï¼ˆä½œä¸ºå¯¹ç…§ç»„ï¼‰
    print(f"\n" + "="*80)
    print("æµ‹è¯•3: å®Œæ•´æŒ‡ä»¤ (æ ‡å‡†è®­ç»ƒæ ¼å¼)")
    print("="*80)
    prompt3 = f"<|im_start|>system\n{system_msg}<|im_end|>\n<|im_start|>user\n{user_content}<|im_end|>\n<|im_start|>assistant\n"
    
    print(f"\nè¾“å…¥æ ¼å¼:")
    print(f"<system>{system_msg[:100]}...</system>")
    print(f"<user>è¯·å°†ä»¥ä¸‹ç°ä»£ç™½è¯æ¶¦è‰²æˆé›…è‡´çš„æ–‡å­¦æ–‡æœ¬ï¼š\\n\\n{modern_text[:100]}...</user>")
    
    inputs3 = tokenizer(prompt3, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        output_ids3 = model.generate(
            **inputs3,
            max_new_tokens=args.max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.pad_token_id,
        )
    
    output3 = tokenizer.decode(output_ids3[0][len(inputs3.input_ids[0]):], skip_special_tokens=True)
    print(f"\nè¾“å‡º (å‰300å­—):")
    print(output3[:300])
    
    # æ˜¾ç¤ºæœŸæœ›è¾“å‡º
    print(f"\n" + "="*80)
    print("æœŸæœ›è¾“å‡º (è®­ç»ƒæ•°æ®ä¸­çš„åŸæ–‡)")
    print("="*80)
    print(expected_output[:300])
    
    # åˆ†æ
    print(f"\n" + "="*80)
    print("ç»“è®ºåˆ†æ")
    print("="*80)
    print("å¦‚æœæµ‹è¯•1å’Œæµ‹è¯•2çš„è¾“å‡ºéƒ½æ˜¯æ–‡è¨€é£æ ¼ä¸”ç±»ä¼¼æµ‹è¯•3ï¼Œ")
    print("è¯´æ˜æ¨¡å‹ã€æ²¡æœ‰ã€‘çœŸæ­£å­¦ä¼šæŒ‡ä»¤éµå¾ªï¼Œåªæ˜¯åœ¨åš:")
    print("  è¾“å…¥æ¨¡å¼: ç°ä»£ç™½è¯æ–‡æœ¬")
    print("  è¾“å‡ºæ¨¡å¼: æ–‡è¨€é£æ ¼æ–‡æœ¬")
    print("\nå¦‚æœæµ‹è¯•1å’Œæµ‹è¯•2è¾“å‡ºæ··ä¹±æˆ–ä¸åƒæ–‡è¨€ï¼Œè€Œæµ‹è¯•3æ­£å¸¸ï¼Œ")
    print("è¯´æ˜æ¨¡å‹ã€ç¡®å®ã€‘åœ¨ä¾èµ–æŒ‡ä»¤æ¥å†³å®šä»»åŠ¡ç±»å‹ã€‚")
    print("="*80)


if __name__ == "__main__":
    main()
