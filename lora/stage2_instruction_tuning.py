"""LoRA stage-2 (instruction tuning) training script with FIXED label segmentation.

ğŸ”§ å…³é”®ä¿®å¤ï¼š
  - Labels åªåŒ…å« assistant çš„å›å¤æ–‡æœ¬ï¼Œä¸åŒ…å« system/user/assistant æ ‡è®°
  - è¿™æ ·æ¨¡å‹å°±ä¸ä¼šå­¦åˆ°"ç»§ç»­å¯¹è¯"çš„è¡Œä¸º

ä» Stage 1 checkpoint ç»§ç»­è®­ç»ƒï¼Œä½¿ç”¨å¯¹è¯æ ¼å¼çš„æ•°æ®è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒã€‚

Usage:
    python -m lora.stage2_instruction_tuning_fixed --config lora/stage2_instruction_config.json
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Dict, List

import torch
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, PeftModel
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    TrainerCallback,
)

DEFAULT_SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä¸ªæ–‡å­¦åˆ›ä½œåŠ©æ‰‹ï¼Œæ“…é•¿ç”¨å„ç§é£æ ¼æ”¹å†™æ–‡æœ¬ã€‚"


def formatting_func_stage2_fixed(example, tokenizer, max_seq_length=2048):
    """æ ¼å¼åŒ– stage2 å¯¹è¯æ•°æ® - å…³é”®ä¿®å¤ï¼šlabels åªåŒ…å« assistant å›å¤
    
    âœ… æ­£ç¡®åšæ³•ï¼š
      - input_ids: åŒ…å«å®Œæ•´å¯¹è¯ï¼ˆsystem + user + assistant å¼€å¤´ï¼‰
      - labels: åªæ ‡æ³¨ assistant çš„å›å¤æ–‡æœ¬ + EOSï¼Œå…¶ä»–éƒ¨åˆ†è®¾ä¸º -100ï¼ˆå¿½ç•¥ï¼‰
      - å¼ºåˆ¶åœ¨ assistant å›å¤ç»“å°¾æ·»åŠ  <|im_end|> ä½œä¸ºæ˜ç¡®çš„åœæ­¢ä¿¡å·
    
    âŒ é”™è¯¯åšæ³•ï¼ˆæ—§ç‰ˆï¼‰ï¼š
      - æ•´ä¸ªæ–‡æœ¬éƒ½ä½œä¸º labelï¼Œå¯¼è‡´æ¨¡å‹å­¦åˆ° "ç»§ç»­å¯¹è¯" çš„è¡Œä¸º
      - æ²¡æœ‰å¼ºåˆ¶ EOS tokenï¼Œå¯¼è‡´æ¨¡å‹ä¸çŸ¥é“ä½•æ—¶åœæ­¢
    """
    conversations = example.get("conversations", [])
    if not conversations:
        return {"input_ids": [], "attention_mask": [], "labels": []}
    
    # æ„å»ºå®Œæ•´çš„ prompt å’Œæ‰¾åˆ° assistant çš„å›å¤
    messages = []
    assistant_response = None
    
    for msg in conversations:
        # å…¼å®¹ä¸¤ç§å­—æ®µå‘½å
        role = msg.get("role") or msg.get("from") or "user"
        content = msg.get("content") or msg.get("value") or ""
        
        # å½’ä¸€åŒ–è§’è‰²åç§°
        if role in ("system", "sys"):
            norm_role = "system"
        elif role in ("assistant", "gpt", "bot"):
            norm_role = "assistant"
            assistant_response = content  # ä¿å­˜ assistant çš„å›å¤
        else:
            norm_role = "user"
        
        messages.append({"role": norm_role, "content": content})
    
    if assistant_response is None:
        return {"input_ids": [], "attention_mask": [], "labels": []}
    
    # ç¡®ä¿ assistant å›å¤ä¸åŒ…å«é¢å¤–çš„åç¼€ï¼ˆå¦‚"æ”¹å†™å®Œæˆ"ã€"è¯·å‚è€ƒ"ç­‰ï¼‰
    # æ¸…ç†å¯èƒ½çš„å°¾å·´
    assistant_response = assistant_response.strip()
    
    # ä½¿ç”¨ apply_chat_template æ„å»ºå®Œæ•´å¯¹è¯
    # æ³¨æ„ï¼šæˆ‘ä»¬éœ€è¦å…ˆæ„å»ºä¸åŒ…å« assistant å›å¤çš„ promptï¼Œç„¶åå†åŠ ä¸Š assistant çš„éƒ¨åˆ†
    prompt_messages = [m for m in messages if m["role"] != "assistant"]
    
    # æ‰‹åŠ¨æ„å»ºï¼ˆå› ä¸º Qwen çš„ tokenizer å¯èƒ½ä¸æ”¯æŒ apply_chat_templateï¼‰
    prompt_parts = []
    for msg in prompt_messages:
        prompt_parts.append(f"<|im_start|>{msg['role']}\n{msg['content']}<|im_end|>")
    prompt_parts.append("<|im_start|>assistant\n")
    prompt_text = "\n".join(prompt_parts)
    
    # å®Œæ•´æ–‡æœ¬ï¼ˆåŒ…å« assistant å›å¤ + å¼ºåˆ¶çš„ç»“æŸæ ‡è®°ï¼‰
    # ğŸ”‘ å…³é”®ï¼šç¡®ä¿ <|im_end|> è¢«åŒ…å«åœ¨è®­ç»ƒä¸­ï¼Œè®©æ¨¡å‹å­¦ä¼š"è¯´å®Œå°±åœ"
    full_text = prompt_text + assistant_response + "<|im_end|>"
    
    # Tokenize
    prompt_ids = tokenizer(prompt_text, add_special_tokens=False)["input_ids"]
    full_ids = tokenizer(full_text, truncation=True, max_length=max_seq_length, add_special_tokens=False)["input_ids"]
    
    # æ„å»º labelsï¼šåªæœ‰ assistant å›å¤éƒ¨åˆ†ï¼ˆåŒ…æ‹¬ <|im_end|>ï¼‰æ˜¯æœ‰æ•ˆçš„ï¼Œå…¶ä»–éƒ¨åˆ†è®¾ä¸º -100
    # è¿™æ ·æ¨¡å‹ä¼šå­¦åˆ°ï¼šç”Ÿæˆå›å¤å†…å®¹ â†’ è¾“å‡º <|im_end|> â†’ åœæ­¢
    labels = [-100] * len(prompt_ids) + full_ids[len(prompt_ids):]
    
    # Padding to max_length
    input_ids = full_ids + [tokenizer.pad_token_id] * (max_seq_length - len(full_ids))
    attention_mask = [1] * len(full_ids) + [0] * (max_seq_length - len(full_ids))
    labels = labels + [-100] * (max_seq_length - len(labels))
    
    return {
        "input_ids": input_ids[:max_seq_length],
        "attention_mask": attention_mask[:max_seq_length],
        "labels": labels[:max_seq_length],
    }


class LossRecorderCallback(TrainerCallback):
    """è®°å½•è®­ç»ƒ loss"""
    def __init__(self):
        self.training_losses = []
        self.steps = []
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs and "loss" in logs:
            self.training_losses.append(logs["loss"])
            self.steps.append(state.global_step)
            # æ‰“å°å½“å‰ step çš„ lossï¼Œé¢‘ç‡ç”± TrainingArguments.logging_steps æ§åˆ¶
            if getattr(state, "is_world_process_zero", True):
                print(f"[step {state.global_step}] loss = {logs['loss']:.4f}")


def main():
    # å¼ºåˆ¶æ¸…ç† CUDA ç¼“å­˜å’Œé‡ç½®è®¾å¤‡
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        # å°è¯•åˆå§‹åŒ– CUDA
        try:
            _ = torch.zeros(1).cuda()
            torch.cuda.synchronize()
            torch.cuda.empty_cache()
            print(f"âœ“ CUDA initialized: {torch.cuda.get_device_name(0)}")
            print(f"  Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        except RuntimeError as e:
            print(f"âš ï¸  CUDA initialization failed: {e}")
            print("  Try: pkill -9 python; nvidia-smi --gpu-reset")
            raise
    
    # 1. è§£æå‚æ•°
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=Path, help="Path to JSON config file")
    parser.add_argument("--model_name_or_path", type=str)
    parser.add_argument("--dataset_path", type=str)
    parser.add_argument("--output_dir", type=str)
    parser.add_argument("--per_device_train_batch_size", type=int, default=8)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=8)
    parser.add_argument("--learning_rate", type=float, default=5e-5)
    parser.add_argument("--warmup_steps", type=int, default=50)
    parser.add_argument("--num_train_epochs", type=float, default=2.0)
    parser.add_argument("--max_seq_length", type=int, default=2048)
    parser.add_argument("--logging_steps", type=int, default=10)
    parser.add_argument("--save_steps", type=int, default=50)
    parser.add_argument("--attn_impl", type=str, default="sdpa")
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®æ–‡ä»¶
    if args.config:
        if not args.config.exists():
            raise FileNotFoundError(f"Config file not found: {args.config}")
        with args.config.open("r", encoding="utf-8") as f:
            config_data = json.load(f)
        # ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼è¦†ç›–é»˜è®¤å€¼ï¼ˆå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆï¼‰
        for key, value in config_data.items():
            if not hasattr(args, key) or getattr(args, key) is None or getattr(args, key) == parser.get_default(key):
                setattr(args, key, value)
    
    # æ£€æŸ¥å¿…éœ€å‚æ•°
    if not args.model_name_or_path or not args.dataset_path or not args.output_dir:
        parser.error("Required arguments: --model_name_or_path, --dataset_path, --output_dir (or provide via --config)")
        
    args.dataset_path = Path(args.dataset_path)
    args.output_dir = Path(args.output_dir)
    args.output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 80)
    print("Stage 2: Instruction Tuning (FIXED - Proper Label Segmentation)")
    print("=" * 80)
    print(f"Model checkpoint: {args.model_name_or_path}")
    print(f"Dataset: {args.dataset_path}")
    print(f"Output: {args.output_dir}")
    print(f"Batch size: {args.per_device_train_batch_size} Ã— {args.gradient_accumulation_steps}")
    print(f"Learning rate: {args.learning_rate}")
    print(f"Epochs: {args.num_train_epochs}")
    print("\nğŸ”§ Key Fix: Labels only contain assistant response (no role markers)")
    
    # 2. åŠ è½½æ•°æ®é›†
    print(f"\nLoading dataset from {args.dataset_path}")
    dataset = load_dataset("json", data_files=str(args.dataset_path), split="train")
    print(f"âœ“ Loaded {len(dataset):,} samples")
    
    # 3. åŠ è½½ tokenizer
    print(f"\nLoading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)
    tokenizer.padding_side = "right"
    tokenizer.model_max_length = args.max_seq_length
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    print(f"âœ“ Tokenizer loaded")
    
    # 4. åŠ è½½æ¨¡å‹ï¼ˆè¿™æ˜¯ stage1 checkpointï¼Œå·²ç»åŒ…å« LoRAï¼‰
    print(f"\nLoading model from {args.model_name_or_path}")
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name_or_path,
        torch_dtype=torch.bfloat16,
        attn_implementation=args.attn_impl,
    )
    print(f"âœ“ Model loaded (ç»§ç»­è®­ç»ƒå·²æœ‰çš„ LoRA æƒé‡)")
    
    # 5. Tokenize æ•°æ®é›† - ä½¿ç”¨ä¿®å¤åçš„æ ¼å¼åŒ–å‡½æ•°
    print("\nTokenizing dataset with proper label segmentation...")
    def tokenize_function(examples):
        # å½“ batched=True æ—¶ï¼Œexamples æ˜¯å­—å…¸ï¼Œæ¯ä¸ªé”®å¯¹åº”ä¸€ä¸ªåˆ—è¡¨
        results = {
            "input_ids": [],
            "attention_mask": [],
            "labels": [],
        }
        
        num_samples = len(examples["conversations"])
        for i in range(num_samples):
            example = {key: examples[key][i] for key in examples}
            formatted = formatting_func_stage2_fixed(example, tokenizer, args.max_seq_length)
            
            if formatted["input_ids"]:  # åªæ·»åŠ æœ‰æ•ˆæ ·æœ¬
                results["input_ids"].append(formatted["input_ids"])
                results["attention_mask"].append(formatted["attention_mask"])
                results["labels"].append(formatted["labels"])
        
        return results
    
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        batch_size=100,
        remove_columns=dataset.column_names,
    )
    print(f"âœ“ Tokenization complete: {len(tokenized_dataset):,} samples")
    
    # éªŒè¯ï¼šæ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ labelsï¼Œç¡®ä¿æ²¡æœ‰ role markers
    print("\nğŸ” éªŒè¯ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ labels (åº”è¯¥åªåŒ…å« assistant å›å¤):")
    first_labels = tokenized_dataset[0]["labels"]
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªé -100 çš„ä½ç½®
    valid_label_ids = [lid for lid in first_labels if lid != -100]
    if valid_label_ids:
        decoded_labels = tokenizer.decode(valid_label_ids, skip_special_tokens=False)
        print(f"Labels preview (å‰200å­—ç¬¦): {decoded_labels[:200]}")
        if any(marker in decoded_labels.lower() for marker in ["<|im_start|>", "system", "user", "assistant"]):
            print("âš ï¸  WARNING: Labels åŒ…å« role markersï¼è¿™ä¼šå¯¼è‡´æ¨¡å‹ç»§ç»­å¯¹è¯ã€‚")
        else:
            print("âœ… Labels çœ‹èµ·æ¥æ­£ç¡®ï¼ˆåªæœ‰å›å¤å†…å®¹ï¼‰")
    
    # æ¸…ç†ä¸€æ¬¡æ˜¾å­˜ï¼ˆä»…åœ¨æœ‰ CUDA æ—¶ï¼‰
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # 6. è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=str(args.output_dir),
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        lr_scheduler_type="cosine",
        warmup_steps=args.warmup_steps,
        num_train_epochs=args.num_train_epochs,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        save_total_limit=3,
        bf16=True,
        tf32=True,
        optim="adamw_torch_fused",
        max_grad_norm=1.0,
        gradient_checkpointing=True,
        report_to=[],
        dataloader_drop_last=False,
    )
    
    # 7. åˆ›å»º Trainerï¼ˆä¸éœ€è¦ data_collatorï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åšå¥½äº† paddingï¼‰
    loss_recorder = LossRecorderCallback()
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        callbacks=[loss_recorder],
    )
    
    # 8. å¼€å§‹è®­ç»ƒ
    print("\n" + "=" * 80)
    print("Starting training...")
    print("=" * 80 + "\n")
    
    trainer.train()
    
    # 9. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("\nSaving final model...")
    trainer.save_model()
    tokenizer.save_pretrained(args.output_dir)
    
    print(f"\nâœ“ Training complete!")
    print(f"  Model saved to: {args.output_dir}")
    if loss_recorder.training_losses:
        print(f"  Initial loss: {loss_recorder.training_losses[0]:.4f}")
        print(f"  Final loss: {loss_recorder.training_losses[-1]:.4f}")
        print(f"  Total steps: {len(loss_recorder.steps)}")

    # è®­ç»ƒç»“æŸåå†æ¸…ä¸€æ¬¡æ˜¾å­˜ï¼Œæ–¹ä¾¿åŒä¸€è¿›ç¨‹åç»­ç»§ç»­ä½¿ç”¨ GPU
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


if __name__ == "__main__":
    main()
