"""Quick smoke test for Stage-2 instruction-tuned checkpoints.

Three testing modes (determined by provided arguments):

1. Base Model Mode:
   --base_model <path>

2. Single LoRA Mode:
   --lora_model <path>

3. Dual LoRA Mode (Stacked Adapters):
   --style_adapter <path> --instruct_adapter <path>

Usage examples:

    # Test base model
    python -m lora.test_stage2_instruction --base_model Qwen3-8B-Base

    # Test Stage1 (style only)
    python -m lora.test_stage2_instruction --lora_model stage1_style_injection/checkpoint-531

    # Test Stage2 (style + instruct stacked)
    python -m lora.test_stage2_instruction \
        --style_adapter stage1_style_injection/checkpoint-531 \
        --instruct_adapter stage2_instruct_new_adapter

    # Override base model detection
    python -m lora.test_stage2_instruction \
        --base_model /path/to/Qwen3-8B-Base \
        --lora_model stage1_style_injection/checkpoint-531
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

DEFAULT_SYSTEM = "ä½ æ˜¯ä¸€åä¼˜é›…çš„æ–‡å­¦æ”¹å†™ä½œå®¶ï¼Œæ“…é•¿æŠŠç°ä»£ç™½è¯æ¶¦è‰²æˆæ›´è®²ç©¶ã€æ›´æœ‰éŸµå‘³çš„åç¾æ–‡æœ¬ã€‚è¦æ±‚ï¼š\n1. ä¸¥æ ¼ä¿ç•™åŸæ–‡çš„äº‹å®ä¸æƒ…èŠ‚ï¼Œä¸æ–°å¢ä¿¡æ¯ã€‚\n2. ç”¨æ›´è®²ç©¶çš„è¯æ±‡ã€å¥å¼å’Œå¤é›…çš„è¡¨è¾¾ï¼Œè®©è¯­è¨€æ›´æœ‰æ°”éŸµï¼Œä½†ä¿æŒå¯è¯»æ€§ã€‚\n3. è¾“å‡ºä¿æŒä¸ºä¸­æ–‡æ®µè½ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€‚"

ORIGINAL_TEXT = "æœ±å…ƒç’‹åäºæ®¿ä¸­ï¼Œçƒ›å½±æ‘‡çº¢ï¼Œé¢å¸¦é˜´é¸·ä¹‹è‰²ï¼Œå·¦å³ä¾ç«‹ï¼Œçš†ä¸ºé™ˆä¸œç­‰æ–‡åã€‚\"æš—è¡£å«æŒ‡æŒ¥ä½¿ï¼Œ\"æœ±å…ƒç’‹è¨€ï¼Œ\"æŒæ­¤è…°ç‰Œï¼Œç›‘å¯Ÿç™¾åƒšï¼Œå…ˆæ–©åå¥ã€‚\"é™ˆä¸œæ¥äº†è…°ç‰Œï¼Œå¿ƒä¸­ç”šå–œã€‚åˆå‘½å°†ä¸€ä»½æ–‡ä¹¦äº¤å‡ºã€‚é‚£çº¸ä¸Šæœˆå…‰ä¹‹ä¸‹ï¼Œéšéšæœ‰å­—è¿¹ã€‚\"å‰¥çš®å®è‰ä»¤ï¼Œ\"æœ±å…ƒç’‹æ›°ï¼š\"è´ªå¢¨é€šæ•Œè€…ï¼Œå‰¥çš®å¡«è‰ï¼Œæ‚¬å¤´ç¤ºä¼—ã€‚é€Ÿå®œåŠç†ã€‚\"é™ˆä¸œé¢†æ—¨ï¼Œæ­æ•¬åœ°å°†è…°ç‰Œæ–‡ä¹¦æ”¶å¥½ï¼Œé€€å‡ºæ®¿å¤–ã€‚æ®¿å†…å¤å¯‚ï¼ŒæƒŸè§çƒ›ç«å¾®æ˜ï¼Œå¿½æœ‰ä¸€å£°å¤œæ­æ‚²å•¸ï¼Œè‡ªè¿œæ–¹æ¥ï¼Œæ¥ç€ä¸œå—è§’ä¸Šç«å…‰ä¸€é—ªï¼Œç…§å¾—å¤œç©ºä¸€ç‰‡ã€‚æœ±å…ƒç’‹ä¾¿è‡³çª—è¾¹ï¼Œæœ›é‚£ç«å¤„ï¼ŒæŒ‡å°–å¾®é¢¤ã€‚ç«å…‰è·³äº†ä¸€ä¼šï¼Œæ¸æ¸ä½çŸ®ï¼Œæœ€åè¢«é»‘å¤œåå™¬ã€‚è¿œè¿œçš„åœ°æ–¹åˆæœ‰å‘¼å–Šä¹‹å£°ï¼Œæ¸æ¸æ­¢æ¯ã€‚ä»–æ–™æƒ³é™ˆä¸œå·²è¡Œäº‹ã€‚ä»–ä»åå›æ¡ˆå‰ï¼Œå–è¿‡ä¸€å·å¥ç« ï¼Œå´æœªå¯è§†ã€‚æ®¿å†…æƒŸé—»å…¶å‘¼å¸ä¹‹å£°ã€‚çº¦è«ä¸¤åˆ»ä¹‹åï¼Œè„šæ­¥ä¹‹å£°æ¸è¿‘ã€‚é™ˆä¸œç–¾è¶‹è€Œå…¥ï¼Œè¡£è¥ŸçŠ¹æ¹¿ï¼Œæ‰‹ä¸­æ§ç€ä¸€ä¸ªä¹Œæœ¨ç›’å„¿ã€‚\"é™›ä¸‹ï¼Œç§¦æ¡§åºœä¸­å·²è‚ƒæ¸…ï¼Œå†…å¤–æ¯•é™ã€‚\"é™ˆä¸œè¯­å£°å¾®ä¿ƒï¼Œå°šå¸¦é£å°˜åŠ³é¡¿ã€‚\"é€šæ•Œå¯†ä¿¡åœ¨æ­¤ï¼Œè¯æ®ç¡®å‡¿ã€‚\"æœ±å…ƒç’‹æ¥äº†ä¹Œæœ¨ç›’å„¿ï¼Œæ‰‹æŠšé›•çº¹ï¼Œæ­å¼€é“œæ‰£ï¼Œæ‰“å¼€ç›–å­ï¼Œé‡Œé¢æ•´é½å ç€åäºŒå°å¯†å‡½ã€‚æ­¤ä¿¡ä¹ƒç‰¹åˆ¶è¯æ°´æ‰€å†™ï¼Œéšæ—¶æ— ç—•ï¼Œç°æ—¶å­—è¿¹æ‚‰å‡ºï¼Œå¢¨è‰²å¦‚æ–°ã€‚ä»–æŠ½å‡ºä¸€å°ï¼Œå±•è§†ã€‚é‚£å°ä¹¦ä¿¡ä¸Šæ¬¾æ˜¯é‡‘å›½å…ƒå¸…å®Œé¢œæ˜Œï¼Œè½æ¬¾ä¸ºç§¦æ¡§èŠ±æŠ¼ã€‚å­—é‡Œè¡Œé—´ï¼Œå°½æ˜¯å†›æœºæ°‘æƒ…ã€å‰²åœ°èµ”æ¬¾ä¹‹äº‹ï¼Œå†å†å¦‚è´¦ç°¿ã€‚ä»–é€å¼ çœ‹è¿‡ï¼Œä¸æ…Œä¸å¿™ã€‚æ®¿å†…åªå¬å¾—çº¸é¡µç¿»åŠ¨ä¹‹å£°ã€‚çƒ›å½±æ‘‡çº¢ï¼Œæ˜ åœ¨èº«åå¢™ä¸Šã€‚ä¸€é¢çœ‹ï¼Œä¸€é¢å°†ä¿¡çº¸æŠ˜å èµ·æ¥ï¼Œæ”¾å›ç›’å†…ï¼Œç›–å¥½ã€‚\"ä¸‡ä¿Ÿå¨ã€æ²ˆè¯¥äº¦ç³»åºœä¸­åŒå…šï¼Œå¹¶å·²æ‹¿è·ã€‚\"é™ˆä¸œç¦€æŠ¥è¯´æ¯•ï¼Œåˆé“ï¼Œ\"å´æ‰äººç‡å¥³é”¦è¡£å«å°é”å†…é™¢ï¼Œæ— äººé€ƒå‡ºã€‚\"æœ±å…ƒç’‹ç‚¹å¤´é“ï¼š\"çŸ¥é“äº†ã€‚\"ä¸€é¢æ‘©æŒ²æœ¨åŒ£ï¼Œä¸€é¢è¯´é“ï¼š\"äººçŠ¯æŠ¼å…¥è¯ç‹±ï¼Œä¸¥åŠ çœ‹å®ˆã€‚æ­¤ä»¶å¯†ä¿¡ï¼ŒæŠ„å½•å‰¯æœ¬ï¼ŒåŸä»¶å°å­˜ã€‚\"ä»–åˆåœäº†ä¸€ä¼šå„¿ï¼ŒæŠ¬å¤´é“ï¼š\"ä¸‰æ—¥ä¹‹å†…ï¼Œåˆé—¨å¤–ï¼Œå½“ä¼—å®£åˆ¤ã€‚\"é™ˆä¸œé¢†å‘½é€€å‡ºã€‚æœ±å…ƒç’‹ç‹¬è‡ªåç€ï¼Œç”¨æ‰‹æŒ‡åœ¨æœ¨åŒ£ä¸Šè½»è½»åˆ’ç€ï¼Œæƒ³é“ï¼š\"ç§¦æ¡§é€šæ•Œï¼Œè¯æ®ç¡®å‡¿ã€‚\"ä¾¿å”¤èµµé¼ã€æçº²è¿›æ¥ã€‚äºŒäººæ˜¾ç„¶å·²èµ·èº«ç­‰å€™ï¼Œè¡£å† æ•´é½ã€‚æœ±å…ƒç’‹é‚å°†ç§¦æ¡§ä¹‹äº‹ç»†è¯´ä¸€éï¼ŒäºŒäººé¢é¢ç›¸è§‘ï¼Œç¥è‰²æ²‰é‡ã€‚èµµé¼é“ï¼š\"é™›ä¸‹ï¼Œè‹¥è¡Œå…¬å®¡ï¼Œéœ‡åŠ¨ä¸å°ã€‚\"æçº²æ¥å£é“ï¼š\"è‡£ç­‰å·²è°ƒå…µç»´æŠ¤ä¸´å®‰å„é—¨è¦é“ï¼Œé˜²ä½™å…šä½œä¹±ã€‚\"æœ±å…ƒç’‹é“ï¼š\"æ‹Ÿæ—¨ï¼Œå¸ƒå‘Šå¤©ä¸‹ã€‚\"åˆå‘èµµé¼é“ï¼š\"ç½ªåã€è¯æ®ã€åˆ‘å¾‹ï¼Œéƒ½è¦ä¸€ä¸€åˆ—æ˜ã€‚\"èµµé¼åº”è¯ºï¼Œé‚é€€ï¼Œè„šæ­¥å£°æ¸è¿œï¼Œç»ˆä¸å¯é—»ã€‚"

DEFAULT_USER = (
    "è¯·å°†ä»¥ä¸‹ç°ä»£ç™½è¯æ¶¦è‰²æˆé›…è‡´çš„æ–‡å­¦æ–‡æœ¬ï¼š\n\n"
    + ORIGINAL_TEXT
)


PRESET_CASES = {
    "elegant_style": {
        "system": DEFAULT_SYSTEM,
        "user": DEFAULT_USER,
        "description": "é›…è‡´ç»†è…»é£æ ¼æ”¹å†™ï¼ˆä¸è®­ç»ƒæ•°æ®æ ¼å¼å®Œå…¨ä¸€è‡´ï¼‰",
    },
}


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Test Stage-2 instruction-tuned checkpoint")
    
    # æ¨¡å‹è·¯å¾„å‚æ•°ï¼ˆæ ¹æ®æä¾›çš„å‚æ•°è‡ªåŠ¨åˆ¤æ–­æ¨¡å¼ï¼‰
    parser.add_argument(
        "--base_model",
        type=str,
        default=None,
        help="Base model path (e.g., Qwen3-8B-Base). If only this is provided, test pure base model.",
    )
    parser.add_argument(
        "--lora_model",
        type=str,
        default=None,
        help="Single LoRA adapter path (e.g., stage1_style_injection/checkpoint-531)",
    )
    parser.add_argument(
        "--style_adapter",
        type=str,
        default=None,
        help="Style adapter path (e.g., stage1_style_injection/checkpoint-531). Use with --instruct_adapter for dual mode.",
    )
    parser.add_argument(
        "--instruct_adapter",
        type=str,
        default=None,
        help="Instruct adapter path (e.g., stage2_instruct_new_adapter). Use with --style_adapter for dual mode.",
    )
    
    # æµ‹è¯•å‚æ•°
    parser.add_argument("--system", type=str, default=DEFAULT_SYSTEM, help="System prompt.")
    parser.add_argument("--user", type=str, default=DEFAULT_USER, help="User message.")
    parser.add_argument(
        "--preset",
        type=str,
        default=None,
        choices=sorted(PRESET_CASES.keys()),
        help="Use a built-in test case (overrides --system/--user if set).",
    )
    parser.add_argument(
        "--input_file",
        type=Path,
        default=None,
        help="Optional file containing multiple user prompts. One JSONL per line with 'system'/'user', or plain text (one prompt per line).",
    )
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--max_new_tokens", type=int, default=512)
    parser.add_argument("--temperature", type=float, default=0.3)
    parser.add_argument("--top_p", type=float, default=0.85)
    parser.add_argument("--repetition_penalty", type=float, default=1.5)
    parser.add_argument("--seed", type=int, default=1234)
    parser.add_argument(
        "--attn_impl",
        default="sdpa",
        help="Attention impl for inference (sdpa, eager, or flash_attention_2).",
    )
    
    return parser.parse_args()


def build_chat_prompt(system: str, user: str) -> str:
    """Build a single-turn chat prompt using the same format as training.

    Training used messages like:
      <|im_start|>system\n...<|im_end|>\n
      <|im_start|>user\n...<|im_end|>\n
      <|im_start|>assistant\n
    Here we stop before closing the assistant block so generation continues it.
    """
    parts = []
    parts.append(f"<|im_start|>system\n{system}<|im_end|>")
    parts.append(f"<|im_start|>user\n{user}<|im_end|>")
    parts.append("<|im_start|>assistant\n")
    return "\n".join(parts)


def main():
    args = parse_args()
    
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)
    
    # ğŸ”‘ æ ¹æ®æä¾›çš„å‚æ•°è‡ªåŠ¨åˆ¤æ–­æ¨¡å¼
    # åˆ¤æ–­é€»è¾‘ï¼š
    # 1. style_adapter + instruct_adapter -> åŒadapteræ¨¡å¼
    # 2. lora_model -> å•adapteræ¨¡å¼
    # 3. base_model (ä¸”æ— å…¶ä»–adapter) -> åŸºåº§æ¨¡å¼
    # 4. å¦åˆ™ -> å‚æ•°é”™è¯¯
    
    if args.style_adapter and args.instruct_adapter:
        # åŒ LoRA æ¨¡å¼ï¼šåˆ†åˆ«åŠ è½½ style å’Œ instruct adapters
        print(f"Mode: Dual LoRA (Stacked Adapters)\n")
        print(f"Style adapter:    {args.style_adapter}")
        print(f"Instruct adapter: {args.instruct_adapter}")
        
        # è·å– base modelï¼ˆä¼˜å…ˆçº§ï¼š--base_model > adapter config > å½“å‰ç›®å½•ï¼‰
        base_model_name = args.base_model
        if not base_model_name:
            style_config_path = Path(args.style_adapter) / "adapter_config.json"
            if style_config_path.exists():
                try:
                    with open(style_config_path, "r", encoding="utf-8") as f:
                        config = json.load(f)
                    base_model_name = config.get("base_model_name_or_path")
                    if base_model_name:
                        print(f"Base: {base_model_name} (from style adapter config)")
                except Exception as e:
                    print(f"âš ï¸  Failed to read style adapter config: {e}")
        
        if not base_model_name:
            default_base = Path("Qwen3-8B-Base")
            if default_base.exists():
                base_model_name = str(default_base)
                print(f"Base: {base_model_name} (auto-detected in current dir)")
            else:
                raise ValueError(
                    "âŒ Cannot determine base model.\n"
                    "Use --base_model Qwen3-8B-Base"
                )
        else:
            if args.base_model:
                print(f"Base: {base_model_name}")
        # åŠ è½½ tokenizer å’Œ base model
        tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True)
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            attn_implementation=args.attn_impl,
        )
        
        # ğŸ”‘ åŠ è½½ç¬¬ä¸€ä¸ª adapter (style)
        model = PeftModel.from_pretrained(
            base_model,
            args.style_adapter,
            adapter_name="style",
            torch_dtype=torch.bfloat16
        )
        print(f"âœ“ Loaded style adapter")
        
        # ğŸ”‘ åŠ è½½ç¬¬äºŒä¸ª adapter (instruct)
        model.load_adapter(args.instruct_adapter, adapter_name="instruct")
        print(f"âœ“ Loaded instruct adapter")
        
        # æ˜¾ç¤ºå åŠ ä¿¡æ¯
        adapters = list(model.peft_config.keys())
        print(f"\nğŸ”— Stacking adapters:")
        for adapter_name in adapters:
            print(f"  âœ“ {adapter_name}")
        print(f"\nâœ“ All adapters will be stacked during inference")
        print(f"  Formula: W = W_base + Î”W_style + Î”W_instruct")
    
    elif args.lora_model:
        # å• LoRA æ¨¡å¼
        print(f"Mode: Single LoRA\n")
        print(f"LoRA: {args.lora_model}")
        
        # ä¼˜å…ˆé¡ºåºï¼š--base_model > adapter_config.json > å½“å‰ç›®å½•
        base_model_name = args.base_model
        
        if not base_model_name:
            # ä» adapter_config.json è¯»å–
            adapter_config_path = Path(args.lora_model) / "adapter_config.json"
            if adapter_config_path.exists():
                try:
                    with open(adapter_config_path, "r", encoding="utf-8") as f:
                        adapter_config = json.load(f)
                    base_model_name = adapter_config.get("base_model_name_or_path")
                    if base_model_name:
                        print(f"Base: {base_model_name} (from adapter_config.json)")
                except Exception as e:
                    print(f"âš ï¸  Failed to read adapter_config.json: {e}")
        
        if not base_model_name:
            default_base = Path("Qwen3-8B-Base")
            if default_base.exists():
                base_model_name = str(default_base)
                print(f"Base: {base_model_name} (auto-detected in current dir)")
            else:
                raise ValueError(
                    "âŒ Cannot determine base model.\n"
                    "Solutions:\n"
                    "  1. Use --base_model Qwen3-8B-Base\n"
                    "  2. Ensure Qwen3-8B-Base exists in current directory\n"
                    "  3. Make sure adapter_config.json contains base_model_name_or_path"
                )
        else:
            if args.base_model:
                print(f"Base: {base_model_name}")
        
        # åŠ è½½ tokenizer
        tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True)
        
        # åŠ è½½ base model
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            attn_implementation=args.attn_impl,
        )
        
        # åŠ è½½ LoRA adapter
        model = PeftModel.from_pretrained(base_model, str(args.lora_model), torch_dtype=torch.bfloat16)
        
        # æ£€æŸ¥åŠ è½½çš„ adapters
        if hasattr(model, 'peft_config'):
            adapters = list(model.peft_config.keys())
            if adapters:
                print(f"Adapter: {adapters[0]}")
                print(f"âœ“ Single adapter mode")
            else:
                print("âš ï¸  No adapters found in peft_config")
        else:
            print("âš ï¸  Model does not have peft_config attribute")
    
    elif args.base_model:
        # åŸºåº§æ¨¡å‹æ¨¡å¼
        print(f"Mode: Base Model\n")
        print(f"Model: {args.base_model}")
        
        base_model_name = args.base_model
        
        # å°è¯•å½“å‰æ–‡ä»¶å¤¹
        model_path = Path(base_model_name)
        if not model_path.exists():
            local_base = Path("Qwen3-8B-Base")
            if local_base.exists():
                base_model_name = str(local_base)
                print(f"(Using: {base_model_name})")
        
        tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True)
        model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            attn_implementation=args.attn_impl,
        )
    
    else:
        # å‚æ•°é”™è¯¯
        raise ValueError(
            "âŒ Invalid arguments. Must provide one of:\n"
            "  1. --base_model <path>                          (test base model)\n"
            "  2. --lora_model <path>                          (test single adapter)\n"
            "  3. --style_adapter <path> --instruct_adapter <path>  (test dual adapters)\n"
            "\nExamples:\n"
            "  python -m lora.test_stage2_instruction --base_model Qwen3-8B-Base\n"
            "  python -m lora.test_stage2_instruction --lora_model stage1_style_injection/checkpoint-531\n"
            "  python -m lora.test_stage2_instruction --style_adapter stage1_style_injection/checkpoint-531 --instruct_adapter stage2_instruct_new_adapter"
        )
        
        # åŠ è½½ LoRA adaptersï¼ˆä¼šè‡ªåŠ¨åŠ è½½è¯¥ç›®å½•ä¸‹çš„æ‰€æœ‰ adaptersï¼‰
        model = PeftModel.from_pretrained(base_model, str(model_path), torch_dtype=torch.bfloat16)
        
        # ğŸ”‘ æ£€æŸ¥åŠ è½½çš„ adapters
        if hasattr(model, 'peft_config'):
            adapters = list(model.peft_config.keys())
            if adapters:
                print(f"Adapters: {adapters}")
                
                # å¦‚æœæœ‰å¤šä¸ª adaptersï¼Œè¯´æ˜æ˜¯ Stage2ï¼ˆstyle + instruct å åŠ ï¼‰
                if len(adapters) > 1:
                    print(f"\nğŸ”— Stacking adapters:")
                    for adapter_name in adapters:
                        print(f"  âœ“ {adapter_name}")
                    
                    # PEFT é»˜è®¤è¡Œä¸ºï¼šæ‰€æœ‰ adapters è‡ªåŠ¨å åŠ ï¼ˆç›¸åŠ ï¼‰
                    # W_final = W_base + Î”W_adapter1 + Î”W_adapter2 + ...
                    print(f"\nâœ“ All adapters will be stacked during inference")
                    print(f"  Formula: W = W_base + Î”W_{adapters[0]}" + 
                          "".join(f" + Î”W_{a}" for a in adapters[1:]))
                else:
                    print(f"âœ“ Single adapter mode")
            else:
                print("âš ï¸  No adapters found in peft_config")
        else:
            print("âš ï¸  Model does not have peft_config attribute")
        # å°è¯•å½“å‰æ–‡ä»¶å¤¹
        if not model_path.exists():
            local_base = Path("Qwen3-8B-Base")
            if local_base.exists():
                model_path = local_base
                print(f"(Using: {model_path})")
        
        tokenizer = AutoTokenizer.from_pretrained(str(model_path), use_fast=True)
        model = AutoModelForCausalLM.from_pretrained(
            str(model_path),
            torch_dtype=torch.bfloat16,
            device_map="auto",
            attn_implementation=args.attn_impl,
        )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model.eval()
    print(f"\nâœ“ Ready\n")
    
    # æ ¹æ® preset æˆ–æ‰‹åŠ¨ system/user æ„é€ ä¸€ä¸ªæˆ–å¤šä¸ªæµ‹è¯•ç”¨ä¾‹
    test_cases = []

    if args.input_file is not None:
        # æ–‡ä»¶æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
        # 1) JSONLï¼Œæ¯è¡Œå½¢å¦‚ {"system": "...", "user": "..."}
        # 2) çº¯æ–‡æœ¬ï¼Œæ¯è¡Œä½œä¸º userï¼Œsystem ä½¿ç”¨é»˜è®¤æˆ– preset çš„ system
        print(f"Loading prompts from: {args.input_file}")
        with args.input_file.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    rec = json.loads(line)
                    system = rec.get("system") or args.system
                    user = rec.get("user") or ""
                except json.JSONDecodeError:
                    # å½“ä½œçº¯æ–‡æœ¬ user
                    system = args.system
                    user = line
                test_cases.append((system, user))
    elif args.preset:
        preset = PRESET_CASES[args.preset]
        print(f"Using preset='{args.preset}': {preset['description']}")
        test_cases.append((preset["system"], preset["user"]))
    else:
        test_cases.append((args.system, args.user))

    for idx, (system, user) in enumerate(test_cases, start=1):
        prompt = build_chat_prompt(system, user)
        print("=" * 80)
        print(f"Test #{idx}")
        print("=" * 80 + "\n")

        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        # é…ç½®åœæ­¢ tokens
        stop_token_ids = [tokenizer.eos_token_id]
        im_end_id = tokenizer.convert_tokens_to_ids("<|im_end|>")
        im_start_id = tokenizer.convert_tokens_to_ids("<|im_start|>")
        
        if im_end_id and im_end_id != tokenizer.unk_token_id:
            stop_token_ids.append(im_end_id)
        if im_start_id and im_start_id != tokenizer.unk_token_id:
            stop_token_ids.append(im_start_id)

        with torch.no_grad():
            output_ids = model.generate(
                **inputs,
                max_new_tokens=args.max_new_tokens,
                do_sample=True,
                temperature=args.temperature,
                top_p=args.top_p,
                repetition_penalty=args.repetition_penalty,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=stop_token_ids,
            )

        completion = tokenizer.decode(output_ids[0], skip_special_tokens=False)
        
        # æå– assistant å›å¤
        assistant_marker = "<|im_start|>assistant\n"
        if assistant_marker in completion:
            pos = completion.rfind(assistant_marker)
            reply = completion[pos + len(assistant_marker):]
            # ç§»é™¤ç»“æŸæ ‡è®°
            if reply.endswith("<|im_end|>"):
                reply = reply[:-len("<|im_end|>")]
            elif "<|im_end|>" in reply:
                reply = reply[:reply.rfind("<|im_end|>")]
            # ç§»é™¤ <|endoftext|> æ ‡è®°
            if "<|endoftext|>" in reply:
                reply = reply[:reply.find("<|endoftext|>")]
        else:
            reply = completion
        
        # æ¸…ç†å¤šä½™çš„å‰ç¼€å†…å®¹ï¼ˆæ¨¡å‹å¯èƒ½ç”Ÿæˆçš„ç¤¼è²Œç”¨è¯­å’Œä»£ç å—æ ‡è®°ï¼‰
        reply = reply.strip()
        
        # ç§»é™¤å¸¸è§çš„å¤šä½™å‰ç¼€
        unwanted_prefixes = [
            "å¥½çš„ï¼Œè¯·ç¨å€™ç‰‡åˆ»ã€‚",
            "å¥½çš„ï¼Œ",
            "```python",
            "```",
            "Assistant:",
            "assistant:",
        ]
        
        for prefix in unwanted_prefixes:
            if reply.startswith(prefix):
                reply = reply[len(prefix):].strip()
        
        # å¦‚æœæœ‰å¤šè¡Œï¼Œç§»é™¤ç©ºè¡Œå’ŒåªåŒ…å«ä»£ç å—æ ‡è®°çš„è¡Œ
        lines = reply.split('\n')
        cleaned_lines = []
        for line in lines:
            stripped = line.strip()
            if stripped and stripped not in ['```', '```python', 'Assistant:', 'assistant:']:
                cleaned_lines.append(line)
        
        reply = '\n'.join(cleaned_lines)
        
        print(reply)


if __name__ == "__main__":
    main()
