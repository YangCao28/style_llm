"""Quick smoke test for Stage-2 instruction-tuned checkpoints.

Loads a Stage-2 checkpoint (which already includes the Stage-1 LoRA),
feeds a system+user conversation, and prints the assistant reply so you
can visually inspect whether the instruction style and literary style
look correct.

Usage examples:

    # Test the main Stage-2 run
    python -m lora.test_stage2_instruction \
        --model_name_or_path stage2_instruction_tuning

    # Or test a specific checkpoint
    python -m lora.test_stage2_instruction \
        --model_name_or_path stage2_instruction_tuning/checkpoint-158

    # Or test the alpha-enhanced run
    python -m lora.test_stage2_instruction \
        --model_name_or_path stage2_instruction_alpha
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

DEFAULT_SYSTEM = (
    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡å­¦æ”¹å†™å·¥å…·ï¼Œä»…æ‰§è¡Œæ–‡é£è½¬æ¢ä»»åŠ¡ã€‚"
    "ä½ çš„å”¯ä¸€èŒè´£æ˜¯ï¼šå¯¹ç»™å®šæ–‡æœ¬è¿›è¡Œæ–‡é£è°ƒæ•´ï¼Œä¿æŒå†…å®¹å®Œå…¨ä¸€è‡´ã€‚"
    "ä¸¥æ ¼ç¦æ­¢ï¼š(1)æ·»åŠ ä»»ä½•åŸæ–‡ä¸å­˜åœ¨çš„ä¿¡æ¯ã€æƒ…èŠ‚ã€äººç‰©ã€å¯¹è¯ï¼›(2)åˆ å‡æˆ–ç•¥å»ä»»ä½•åŸæ–‡ä¿¡æ¯ï¼›"
    "(3)åœ¨æ”¹å†™ç»“æœåç»§ç»­ç”Ÿæˆä»»ä½•å†…å®¹ï¼›(4)ç”Ÿæˆä»»ä½•è§£é‡Šã€è¯„è®ºã€æç¤ºæˆ–å…ƒä¿¡æ¯ã€‚"
    "è¾“å‡ºæ ¼å¼ï¼šç›´æ¥ç»™å‡ºæ”¹å†™åçš„æ–‡æœ¬ï¼Œä¸åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ã€‚æ”¹å†™å®Œæˆåç«‹å³åœæ­¢ç”Ÿæˆã€‚"
    "âš ï¸ ç¦æ­¢è¾“å‡ºæ”¹å†™å†…å®¹ä»¥å¤–çš„ä»»ä½•å­—ï¼Œè¿è€…æŠ¥é”™ã€‚è¾“å‡ºå®Œæ”¹å†™ç»“æœåå¿…é¡»ç«‹å³ç»ˆæ­¢ï¼Œä¸å¾—ç»§ç»­ä»»ä½•å½¢å¼çš„æ–‡æœ¬ç”Ÿæˆã€‚"
)

ORIGINAL_TEXT = (
    "å²³é£å¹´æ–¹äºŒåå…­ï¼Œæ—§ç”²çŠ¹å­˜ï¼Œç´¯æœ‰æˆ˜åŠŸï¼Œä¸éœ²å¾—æ„ä¹‹è‰²ã€‚éŸ©ä¸–å¿ å››åä½™å²ï¼Œè€äºå†›æ—…ï¼Œæ¢çº¢ç‰äºŒåä¸ƒå²ï¼Œçœ‰ç›®é—´æœ‰æ°´æˆ˜å°†é¢†çš„ç²¾æ˜ä¹‹æ°”ã€‚é»„çºµç›‘åˆ¶æ–°å¼ç«è¯ï¼Œç‚®å†…å®é“å¼¹ã€‚ç‚®å£å¯¹å‡†ä¸‰é‡Œå¤–ä¸€é“å¤¯åœŸå¢™ã€‚\"æ”¾ï¼\"\n"
    "å¼•ä¿¡å’å’çƒ§å°½ï¼Œæ’å…¥ç‚®åº•ã€‚ä¸€å£°è½°é›·ï¼Œç™½çƒŸå†²å¤©ï¼Œé“å¼¹ç ´ç©ºï¼Œç›´é£åœŸå¢™ã€‚\n"
    "åœŸå¢™å€’å¡Œï¼ŒçƒŸå°˜å¼¥æ¼«ã€‚å²³é£é¦–å…ˆæ¥è§†ï¼Œä»–è¹²ä¸‹èº«å»ï¼ŒæŒ‡é‡å‘æ´ä¹‹æ·±å¹¿ï¼Œç‚¹å¤´ç§°è®¸ï¼š\"è¶³çŸ£ã€‚å€˜ä½¿é½å‘ï¼Œåˆ™è¶³ä»¥ç ´é‡ç”²ã€‚\"éŸ©ä¸–å¿ æµ‹å°„ç¨‹ï¼Œæ¢çº¢ç‰å¯Ÿè½¨é“ã€‚ä¸‰äººåœ¨æ—ç»†å•†ï¼Œåˆè¯•ç«é“³ã€‚ç«é“³å°„ç¨‹ç™¾æ­¥ï¼Œé“…å¼¹èƒ½ç©¿é“ç”²ã€‚\n"
    "æœ±å…ƒç’‹æœ›è§é‚£å´©å¡Œçš„åœŸå¢™ï¼ŒçƒŸå°˜åœ¨é£ä¸­æ¸æ¸æ•£å»ã€‚\"ç»„ç¥æœºè¥ï¼Œ\"ä»–å‘½é“ï¼Œ\"é»„çºµç£åŠç«å™¨ï¼Œå²³é£ã€éŸ©ä¸–å¿ ã€æ¢çº¢ç‰æ•™ç»ƒã€‚ç§‹æ”¶ä»¥å‰ï¼Œæˆ‘è¦çœ‹è§ä¸‰åƒäººèƒ½åˆ—é˜Ÿæ–½ç«å™¨ã€‚\"ä¼—äººé¢†æ—¨è€Œå»ã€‚\n"
    "å‡ºå¾—åºœåº“ï¼Œå¿½è§æ·±å¤„æœ‰å‡ å£æ¨Ÿæœ¨ç®±ï¼Œé”æ‰£ä¸Šå°šæœ‰æ–°ç—•ï¼Œåœ¨æ˜æš—ä¹‹ä¸­ï¼Œå°¤è§åˆ†æ˜ã€‚\"ä½•ç‰©ï¼Ÿ\"ä»–é—®ã€‚\n"
    "é»„çºµä¾¿éšä»–ç§äº†ï¼Œå› è¯´é“ï¼š\"è¿æ—¥æ¸…æŸ¥æ—§æ¢°ï¼Œä»“å¿™ä¹‹è‡³ï¼Œæ’¬é”ä¹‹é™…ï¼ŒæœªåŠç»†æŸ¥ã€‚\" æœ±å…ƒç’‹ä¹Ÿä¸é—®ï¼Œè½¬èº«å‡ºå»äº†ã€‚å¤–é¢å¤©è‰²å·²æ™šï¼Œä¸´å®‰åŸé‡Œçš„ç¯ç¬¼éƒ½å·²æŒ‚ä¸Šã€‚ç§¦æ¡§å·²ä¼è¯›ã€‚æ±Ÿå—ç”°äº©æ­£åŠ ç´§ä¸ˆé‡ã€‚å†›å™¨å±€çš„ç«å™¨å›¾æ ·ï¼Œå·²äº¤å·¥éƒ¨ã€‚æ¥ç€å°±æ˜¯æ•´é¡¿ä¸´å®‰ç¦å†›ï¼Œæˆ·éƒ¨é¡»åœ¨ç§‹æˆä»¥å‰ï¼Œç¼–å®šæ–°ç¨å†Œã€‚"
)

DEFAULT_USER = (
    "ä»»åŠ¡ï¼šå°†ä»¥ä¸‹æ–‡æœ¬æ”¹å†™ä¸ºç´§å¼ ã€æ‚¬ç–‘çš„æ–‡é£ã€‚\n"
    "è¦æ±‚ï¼š(1)ä¸¥æ ¼ä¿æŒæ‰€æœ‰ä¿¡æ¯ç‚¹ä¸å˜ï¼›(2)åªæ”¹å˜è¡¨è¾¾æ–¹å¼å’Œæ°”æ°›ï¼›(3)ç¦æ­¢æ·»åŠ ä»»ä½•æ–°å†…å®¹ã€‚\n"
    "åŸæ–‡ï¼š" + ORIGINAL_TEXT + "\n"
    "è¯·ç›´æ¥è¾“å‡ºæ”¹å†™ç»“æœï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–å†…å®¹ï¼š"
)


PRESET_CASES = {
    "ni_kuang": {
        "system": DEFAULT_SYSTEM,
        "user": DEFAULT_USER,
        "description": "æ‚¬ç–‘é£æ ¼æ”¹å†™æµ‹è¯•ï¼ˆä¸¥æ ¼ç¦æ­¢å¢åˆ ï¼‰",
    },
    "zhang_henshui": {
        "system": (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡å­¦æ”¹å†™å·¥å…·ï¼Œä»…æ‰§è¡Œæ–‡é£è½¬æ¢ä»»åŠ¡ã€‚"
            "ä½ çš„å”¯ä¸€èŒè´£æ˜¯ï¼šå°†æ–‡æœ¬æ”¹å†™ä¸ºå¸‚äº•ç”Ÿæ´»ã€ç»†è…»æƒ…æ„Ÿé£æ ¼ï¼Œä¿æŒå†…å®¹å®Œå…¨ä¸€è‡´ã€‚"
            "ä¸¥æ ¼ç¦æ­¢ï¼š(1)æ–°å¢ä»»ä½•æƒ…èŠ‚ã€äººç‰©ã€å¯¹è¯ï¼›(2)åˆ å‡åŸæ–‡ä¿¡æ¯ï¼›(3)æ”¹å†™åç»§ç»­ç”Ÿæˆå†…å®¹ã€‚"
            "è¾“å‡ºæ ¼å¼ï¼šç›´æ¥ç»™å‡ºæ”¹å†™åçš„æ–‡æœ¬ï¼Œæ”¹å†™å®Œæˆåç«‹å³åœæ­¢ã€‚"
            "âš ï¸ ç¦æ­¢è¾“å‡ºæ”¹å†™å†…å®¹ä»¥å¤–çš„ä»»ä½•å­—ï¼Œè¿è€…æŠ¥é”™ã€‚"
        ),
        "user": (
            "ä»»åŠ¡ï¼šå°†ä»¥ä¸‹æ–‡æœ¬æ”¹å†™ä¸ºå¸‚äº•æƒ…è°ƒã€ç»†è…»æƒ…æ„Ÿçš„æ–‡é£ã€‚\n"
            "è¦æ±‚ï¼š(1)ä¸¥æ ¼ä¿æŒæ‰€æœ‰ä¿¡æ¯ç‚¹ä¸å˜ï¼›(2)åªæ”¹å˜è¡¨è¾¾æ–¹å¼ï¼›(3)ç¦æ­¢æ·»åŠ ä»»ä½•æ–°å†…å®¹ã€‚\n"
            "åŸæ–‡ï¼š" + ORIGINAL_TEXT + "\n"
            "è¯·ç›´æ¥è¾“å‡ºæ”¹å†™ç»“æœï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–å†…å®¹ï¼š"
        ),
        "description": "å¸‚äº•æƒ…æ„Ÿé£æ ¼æ”¹å†™æµ‹è¯•ï¼ˆä¸¥æ ¼ç¦æ­¢å¢åˆ ï¼‰",
    },
    "plain_to_style": {
        "system": DEFAULT_SYSTEM,
        "user": (
            "ä»»åŠ¡ï¼šå°†ä»¥ä¸‹æœ´ç´ å™è¿°æ”¹å†™ä¸ºæ‚¬ç–‘æ°”æ°›çš„æ–‡é£ã€‚\n"
            "è¦æ±‚ï¼š(1)ä¸¥æ ¼ä¿æŒæ‰€æœ‰ä¿¡æ¯ç‚¹ä¸å˜ï¼›(2)åªæ”¹å˜æ°”æ°›å’Œè¯­æ°”ï¼›(3)ç¦æ­¢æ·»åŠ ä»»ä½•æ–°å†…å®¹ã€‚\n"
            "åŸæ–‡ï¼š" + ORIGINAL_TEXT + "\n"
            "è¯·ç›´æ¥è¾“å‡ºæ”¹å†™ç»“æœï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–å†…å®¹ï¼š"
        ),
        "description": "æ‚¬ç–‘æ°”æ°›åŠ å¼ºæµ‹è¯•ï¼ˆä¸¥æ ¼ç¦æ­¢å¢åˆ ï¼‰",
    },
}


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Test Stage-2 instruction-tuned checkpoint")
    parser.add_argument(
        "--model_name_or_path",
        type=str,
        required=True,
        help="Path to the Stage-2 checkpoint folder (or HF repo id).",
    )
    parser.add_argument("--system", type=str, default=DEFAULT_SYSTEM, help="System prompt.")
    parser.add_argument("--user", type=str, default=DEFAULT_USER, help="User message.")
    parser.add_argument(
        "--preset",
        type=str,
        default=None,
        choices=sorted(PRESET_CASES.keys()),
        help="Use a built-in test case (overrides --system/--user if set).",
    )
    parser.add_argument(
        "--input_file",
        type=Path,
        default=None,
        help="Optional file containing multiple user prompts. One JSONL per line with 'system'/'user', or plain text (one prompt per line).",
    )
    # ä¸ºäº†æ”¯æŒè‡³å°‘ ~100 å­—çš„è¾“å‡ºï¼Œé»˜è®¤ç»™å¾—ç¨å¾®é•¿ä¸€ç‚¹
    parser.add_argument("--max_new_tokens", type=int, default=512)
    parser.add_argument("--temperature", type=float, default=0.7)
    parser.add_argument("--top_p", type=float, default=0.9)
    parser.add_argument("--repetition_penalty", type=float, default=1.05)
    parser.add_argument("--seed", type=int, default=1234)
    parser.add_argument(
        "--attn_impl",
        default="sdpa",
        help="Attention impl for inference (sdpa, eager, or flash_attention_2).",
    )
    parser.add_argument(
        "--base_model_name",
        type=str,
        default=None,
        help="Base model name for loading tokenizer if checkpoint doesn't have it (e.g., Qwen/Qwen2.5-8B-Base).",
    )
    return parser.parse_args()


def build_chat_prompt(system: str, user: str) -> str:
    """Build a single-turn chat prompt using the same format as training.

    Training used messages like:
      <|im_start|>system\n...<|im_end|>\n
      <|im_start|>user\n...<|im_end|>\n
      <|im_start|>assistant\n
    Here we stop before closing the assistant block so generation continues it.
    """

    parts = [
        f"<|im_start|>system\n{system}<|im_end|>",
        f"<|im_start|>user\n{user}<|im_end|>",
        "<|im_start|>assistant\n",
    ]
    return "\n".join(parts)


def main() -> None:
    args = parse_args()
    torch.manual_seed(args.seed)

    model_path = Path(args.model_name_or_path)
    print(f"model_name_or_path = {args.model_name_or_path}")

    # ä¼˜å…ˆå½“ä½œæœ¬åœ°ç›®å½•ä½¿ç”¨ï¼›å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œå†å›é€€ä¸º HF ä»“åº“å
    if model_path.exists():
        print(f"Loading Stage-2 model from local folder: {model_path}")
        
        # å°è¯•ä» checkpoint åŠ è½½ tokenizerï¼Œå¦‚æœå¤±è´¥åˆ™ä»åŸºç¡€æ¨¡å‹åŠ è½½
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            print("âœ“ Tokenizer loaded from checkpoint")
        except (OSError, ValueError, ImportError) as e:
            print(f"âš  Checkpoint ä¸­æ²¡æœ‰ tokenizerï¼Œå°è¯•ä»åŸºç¡€æ¨¡å‹åŠ è½½...")
            
            # å°è¯•å¤šç§æ–¹å¼æ‰¾åˆ°åŸºç¡€æ¨¡å‹
            base_model_path = None
            
            # 1. ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
            if args.base_model_name:
                base_model_path = args.base_model_name
                print(f"  ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°: {base_model_path}")
            
            # 2. å°è¯•ä» config.json è¯»å– _name_or_pathï¼ˆå¯èƒ½æ˜¯æœ¬åœ°è·¯å¾„ï¼‰
            if not base_model_path:
                config_path = model_path / "config.json"
                if config_path.exists():
                    import json
                    with open(config_path, "r") as f:
                        config = json.load(f)
                        base_model_path = config.get("_name_or_path")
                        if base_model_path:
                            print(f"  ä» config.json è¯»å–: {base_model_path}")
                            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                            if base_model_path and not base_model_path.startswith("/") and "/" not in base_model_path[:10]:
                                base_model_path = str((model_path.parent / base_model_path).resolve())
            
            # 3. å°è¯•ä»çˆ¶ç›®å½•æˆ–ç¥–çˆ¶ç›®å½•æ‰¾ Stage1 æ¨¡å‹ï¼ˆæœ¬åœ°è·¯å¾„ï¼‰
            if not base_model_path:
                # stage2_instruction_tuning_corrected/checkpoint-16 -> stage1_style_injection
                parent_dir = model_path.parent.parent
                possible_stage1_paths = [
                    parent_dir / "stage1_style_injection",
                    parent_dir.parent / "stage1_style_injection",  # å†å¾€ä¸Šä¸€å±‚
                ]
                for stage1_path in possible_stage1_paths:
                    if stage1_path.exists():
                        # ç›´æ¥ä½¿ç”¨ Stage1 è·¯å¾„ï¼ˆåŒ…å« tokenizerï¼‰
                        print(f"  æ‰¾åˆ° Stage1 æ¨¡å‹: {stage1_path}")
                        base_model_path = str(stage1_path.resolve())
                        break
            
            # 4. å°è¯•æŸ¥æ‰¾æœ¬åœ° Qwen æ¨¡å‹ç›®å½•
            if not base_model_path:
                # å¸¸è§çš„æœ¬åœ°è·¯å¾„
                possible_local_paths = [
                    Path("/workspace/models/Qwen2.5-8B-Base"),
                    Path("/workspace/models/Qwen2.5-7B-Base"),
                    Path("./models/Qwen2.5-8B-Base"),
                    Path("../models/Qwen2.5-8B-Base"),
                ]
                print("  å°è¯•æœ¬åœ°æ¨¡å‹è·¯å¾„...")
                for local_path in possible_local_paths:
                    if local_path.exists() and (local_path / "tokenizer_config.json").exists():
                        print(f"    æ‰¾åˆ°: {local_path}")
                        base_model_path = str(local_path.resolve())
                        break
            
            if not base_model_path:
                raise ValueError(
                    "æ— æ³•ç¡®å®šåŸºç¡€æ¨¡å‹åç§°æˆ–è·¯å¾„ã€‚\n"
                    "è¯·ä½¿ç”¨ --base_model_name å‚æ•°æŒ‡å®šæœ¬åœ°è·¯å¾„æˆ– HF æ¨¡å‹åç§°ï¼Œ\n"
                    "ä¾‹å¦‚: --base_model_name /workspace/models/Qwen2.5-8B-Base\n"
                    "æˆ–è€…: --base_model_name stage1_style_injection"
                )
            
            print(f"  Loading tokenizer from: {base_model_path}")
            # å°è¯•ä½œä¸ºæœ¬åœ°è·¯å¾„ï¼Œå¦‚æœå¤±è´¥åˆ™ä½œä¸º HF æ¨¡å‹åç§°
            try:
                tokenizer = AutoTokenizer.from_pretrained(base_model_path, local_files_only=True)
                print("  âœ“ ä»æœ¬åœ°åŠ è½½æˆåŠŸ")
            except Exception:
                print("  âš  æœ¬åœ°åŠ è½½å¤±è´¥ï¼Œå°è¯•ä» HuggingFace...")
                tokenizer = AutoTokenizer.from_pretrained(base_model_path)
        
        model_load_id = model_path
    else:
        print(f"âš  æœ¬åœ°æ‰¾ä¸åˆ°ç›®å½•: {model_path}ï¼Œå°†å°è¯•ä½œä¸º Hugging Face æ¨¡å‹ä»“åº“åŠ è½½ã€‚")
        model_load_id = args.model_name_or_path
        tokenizer = AutoTokenizer.from_pretrained(model_load_id)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_load_id,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        attn_implementation=args.attn_impl,
    )
    model.eval()
    # æ ¹æ® preset æˆ–æ‰‹åŠ¨ system/user æ„é€ ä¸€ä¸ªæˆ–å¤šä¸ªæµ‹è¯•ç”¨ä¾‹
    test_cases = []

    if args.input_file is not None:
        # æ–‡ä»¶æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
        # 1) JSONLï¼Œæ¯è¡Œå½¢å¦‚ {"system": "...", "user": "..."}
        # 2) çº¯æ–‡æœ¬ï¼Œæ¯è¡Œä½œä¸º userï¼Œsystem ä½¿ç”¨é»˜è®¤æˆ– preset çš„ system
        print(f"Loading prompts from: {args.input_file}")
        with args.input_file.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    rec = json.loads(line)
                    system = rec.get("system") or args.system
                    user = rec.get("user") or ""
                except json.JSONDecodeError:
                    # å½“ä½œçº¯æ–‡æœ¬ user
                    system = args.system
                    user = line
                test_cases.append((system, user))
    elif args.preset:
        preset = PRESET_CASES[args.preset]
        print(f"Using preset='{args.preset}': {preset['description']}")
        test_cases.append((preset["system"], preset["user"]))
    else:
        test_cases.append((args.system, args.user))

    for idx, (system, user) in enumerate(test_cases, start=1):
        prompt = build_chat_prompt(system, user)
        print("\n" + "=" * 80)
        print(f"Test case #{idx}")
        # print("----- System -----")
        # print(system)
        # print("----- User -----")
        # print(user)
        # print("----- Raw Prompt (truncated) -----")
        # print(prompt[:400] + ("..." if len(prompt) > 400 else ""))
        print("=" * 80)

        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        # ğŸ” è°ƒè¯•ä¿¡æ¯
        print(f"\n[DEBUG] Prompt length: {len(prompt)} chars, {inputs['input_ids'].shape[1]} tokens")
        print(f"[DEBUG] Prompt ends with: ...{prompt[-100:]}")

        with torch.no_grad():
            output_ids = model.generate(
                **inputs,
                max_new_tokens=args.max_new_tokens,
                do_sample=True,
                temperature=args.temperature,
                top_p=args.top_p,
                repetition_penalty=args.repetition_penalty,
                pad_token_id=tokenizer.eos_token_id,
                # Stop tokens to prevent unwanted continuation
                eos_token_id=[
                    tokenizer.eos_token_id,
                    tokenizer.convert_tokens_to_ids("<|im_end|>"),
                ],
            )

        completion = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        
        # ğŸ” æ›´å¤šè°ƒè¯•
        print(f"[DEBUG] Generated total: {output_ids.shape[1]} tokens")
        print(f"[DEBUG] New tokens: {output_ids.shape[1] - inputs['input_ids'].shape[1]}")
        print(f"[DEBUG] Completion length: {len(completion)} chars")
        print(f"[DEBUG] Completion starts with: {completion[:100]}")
        
        # æå– assistant å›å¤
        if prompt in completion:
            assistant_reply = completion[len(prompt):]
            print(f"[DEBUG] Extracted by removing prompt, length: {len(assistant_reply)}")
        else:
            assistant_reply = completion
            print(f"[DEBUG] Prompt not found in completion, using full completion")
        
        # ï¿½ æ˜¾ç¤ºåŸå§‹è¾“å‡ºï¼ˆæ¸…ç†å‰ï¼‰
        print("===== Raw Assistant Output (before cleaning) =====")
        print(assistant_reply[:500] if len(assistant_reply) > 500 else assistant_reply)
        print("=" * 80)
        
        # ï¿½ğŸ”‘ æ¸…ç†è¾“å‡ºï¼šç§»é™¤å¯èƒ½çš„ prompt æ³„éœ²å’Œæ— å…³å†…å®¹
        # 1. åœ¨ç¬¬ä¸€ä¸ªå‡ºç°çš„ "ä»»åŠ¡ï¼š"ã€"è¦æ±‚ï¼š"ã€"åŸæ–‡ï¼š"ã€"è¯·ç›´æ¥è¾“å‡º" ç­‰å¤„æˆªæ–­
        stop_markers = [
            "\nä»»åŠ¡ï¼š", "\nè¦æ±‚ï¼š", "\nåŸæ–‡ï¼š", 
            "\nè¯·ç›´æ¥è¾“å‡º", "\nè¯·åœ¨ä¸", "\nç¦æ­¢",
            "\nuser\n", "\nUser\n", 
            "\nsystem\n", "\nSystem\n",
            "\nassistant\n", "\nAssistant\n",
            "<|im_start|>", "<|im_end|>",
        ]
        
        for marker in stop_markers:
            if marker in assistant_reply:
                pos = assistant_reply.find(marker)
                assistant_reply = assistant_reply[:pos]
                break
        
        # 2. å»é™¤ç»“å°¾çš„ä¸å®Œæ•´å¥å­ï¼ˆå¦‚æœä»¥æ ‡ç‚¹ç»“æŸåˆ™ä¿ç•™ï¼‰
        assistant_reply = assistant_reply.strip()
        
        print("===== Assistant Reply =====")
        print(assistant_reply)


if __name__ == "__main__":
    main()
