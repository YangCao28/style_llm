"""Quick smoke test for Stage-2 instruction-tuned checkpoints.

Loads a Stage-2 checkpoint (which already includes the Stage-1 LoRA),
feeds a system+user conversation, and prints the assistant reply so you
can visually inspect whether the instruction style and literary style
look correct.

Usage examples:

    # Test the main Stage-2 run
    python -m lora.test_stage2_instruction \
        --model_name_or_path stage2_instruction_tuning

    # Or test a specific checkpoint
    python -m lora.test_stage2_instruction \
        --model_name_or_path stage2_instruction_tuning/checkpoint-158

    # Or test the alpha-enhanced run
    python -m lora.test_stage2_instruction \
        --model_name_or_path stage2_instruction_alpha
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

DEFAULT_SYSTEM = (
    "ä½ æ˜¯ä¸€åä¼˜é›…çš„æ–‡å­¦æ”¹å†™ä½œå®¶ï¼Œæ“…é•¿æŠŠç°ä»£ç™½è¯æ¶¦è‰²æˆæ›´è®²ç©¶ã€æ›´æœ‰éŸµå‘³çš„åç¾æ–‡æœ¬ã€‚è¦æ±‚ï¼š\n"
    "1. ä¸¥æ ¼ä¿ç•™åŸæ–‡çš„äº‹å®ä¸æƒ…èŠ‚ï¼Œä¸æ–°å¢ä¿¡æ¯ã€‚\n"
    "2. ç”¨æ›´è®²ç©¶çš„è¯æ±‡ã€å¥å¼å’Œå¤é›…çš„è¡¨è¾¾ï¼Œè®©è¯­è¨€æ›´æœ‰æ°”éŸµï¼Œä½†ä¿æŒå¯è¯»æ€§ã€‚\n"
    "3. è¾“å‡ºä¿æŒä¸ºä¸­æ–‡æ®µè½ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€‚"
)

ORIGINAL_TEXT = (
    "æœ±å…ƒç’‹åäºæ®¿ä¸­ï¼Œçƒ›å½±æ‘‡çº¢ï¼Œé¢å¸¦é˜´é¸·ä¹‹è‰²ï¼Œå·¦å³ä¾ç«‹ï¼Œçš†ä¸ºé™ˆä¸œç­‰æ–‡åã€‚\n"
    "\"æš—è¡£å«æŒ‡æŒ¥ä½¿ï¼Œ\"æœ±å…ƒç’‹è¨€ï¼Œ\"æŒæ­¤è…°ç‰Œï¼Œç›‘å¯Ÿç™¾åƒšï¼Œå…ˆæ–©åå¥ã€‚\"é™ˆä¸œæ¥äº†è…°ç‰Œï¼Œå¿ƒä¸­ç”šå–œã€‚\n"
    "åˆå‘½å°†ä¸€ä»½æ–‡ä¹¦äº¤å‡ºã€‚é‚£çº¸ä¸Šæœˆå…‰ä¹‹ä¸‹ï¼Œéšéšæœ‰å­—è¿¹ã€‚\"å‰¥çš®å®è‰ä»¤ï¼Œ\"æœ±å…ƒç’‹æ›°ï¼š\"è´ªå¢¨é€šæ•Œè€…ï¼Œå‰¥çš®å¡«è‰ï¼Œæ‚¬å¤´ç¤ºä¼—ã€‚é€Ÿå®œåŠç†ã€‚\"é™ˆä¸œé¢†æ—¨ï¼Œæ­æ•¬åœ°å°†è…°ç‰Œæ–‡ä¹¦æ”¶å¥½ï¼Œé€€å‡ºæ®¿å¤–ã€‚\n"
    "æ®¿å†…å¤å¯‚ï¼ŒæƒŸè§çƒ›ç«å¾®æ˜ï¼Œå¿½æœ‰ä¸€å£°å¤œæ­æ‚²å•¸ï¼Œè‡ªè¿œæ–¹æ¥ï¼Œæ¥ç€ä¸œå—è§’ä¸Šç«å…‰ä¸€é—ªï¼Œç…§å¾—å¤œç©ºä¸€ç‰‡ã€‚æœ±å…ƒç’‹ä¾¿è‡³çª—è¾¹ï¼Œæœ›é‚£ç«å¤„ï¼ŒæŒ‡å°–å¾®é¢¤ã€‚ç«å…‰è·³äº†ä¸€ä¼šï¼Œæ¸æ¸ä½çŸ®ï¼Œæœ€åè¢«é»‘å¤œåå™¬ã€‚è¿œè¿œçš„åœ°æ–¹åˆæœ‰å‘¼å–Šä¹‹å£°ï¼Œæ¸æ¸æ­¢æ¯ã€‚ä»–æ–™æƒ³é™ˆä¸œå·²è¡Œäº‹ã€‚\n"
    "ä»–ä»åå›æ¡ˆå‰ï¼Œå–è¿‡ä¸€å·å¥ç« ï¼Œå´æœªå¯è§†ã€‚æ®¿å†…æƒŸé—»å…¶å‘¼å¸ä¹‹å£°ã€‚çº¦è«ä¸¤åˆ»ä¹‹åï¼Œè„šæ­¥ä¹‹å£°æ¸è¿‘ã€‚é™ˆä¸œç–¾è¶‹è€Œå…¥ï¼Œè¡£è¥ŸçŠ¹æ¹¿ï¼Œæ‰‹ä¸­æ§ç€ä¸€ä¸ªä¹Œæœ¨ç›’å„¿ã€‚\"é™›ä¸‹ï¼Œç§¦æ¡§åºœä¸­å·²è‚ƒæ¸…ï¼Œå†…å¤–æ¯•é™ã€‚\"é™ˆä¸œè¯­å£°å¾®ä¿ƒï¼Œå°šå¸¦é£å°˜åŠ³é¡¿ã€‚\"é€šæ•Œå¯†ä¿¡åœ¨æ­¤ï¼Œè¯æ®ç¡®å‡¿ã€‚\"æœ±å…ƒç’‹æ¥äº†ä¹Œæœ¨ç›’å„¿ï¼Œæ‰‹æŠšé›•çº¹ï¼Œæ­å¼€é“œæ‰£ï¼Œæ‰“å¼€ç›–å­ï¼Œé‡Œé¢æ•´é½å ç€åäºŒå°å¯†å‡½ã€‚æ­¤ä¿¡ä¹ƒç‰¹åˆ¶è¯æ°´æ‰€å†™ï¼Œéšæ—¶æ— ç—•ï¼Œç°æ—¶å­—è¿¹æ‚‰å‡ºï¼Œå¢¨è‰²å¦‚æ–°ã€‚ä»–æŠ½å‡ºä¸€å°ï¼Œå±•è§†ã€‚é‚£å°ä¹¦ä¿¡ä¸Šæ¬¾æ˜¯é‡‘å›½å…ƒå¸…å®Œé¢œæ˜Œï¼Œè½æ¬¾ä¸ºç§¦æ¡§èŠ±æŠ¼ã€‚å­—é‡Œè¡Œé—´ï¼Œå°½æ˜¯å†›æœºæ°‘æƒ…ã€å‰²åœ°èµ”æ¬¾ä¹‹äº‹ï¼Œå†å†å¦‚è´¦ç°¿ã€‚\n"
    "ä»–é€å¼ çœ‹è¿‡ï¼Œä¸æ…Œä¸å¿™ã€‚æ®¿å†…åªå¬å¾—çº¸é¡µç¿»åŠ¨ä¹‹å£°ã€‚çƒ›å½±æ‘‡çº¢ï¼Œæ˜ åœ¨èº«åå¢™ä¸Šã€‚ä¸€é¢çœ‹ï¼Œä¸€é¢å°†ä¿¡çº¸æŠ˜å èµ·æ¥ï¼Œæ”¾å›ç›’å†…ï¼Œç›–å¥½ã€‚\"ä¸‡ä¿Ÿå¨ã€æ²ˆè¯¥äº¦ç³»åºœä¸­åŒå…šï¼Œå¹¶å·²æ‹¿è·ã€‚\"é™ˆä¸œç¦€æŠ¥è¯´æ¯•ï¼Œåˆé“ï¼Œ\"å´æ‰äººç‡å¥³é”¦è¡£å«å°é”å†…é™¢ï¼Œæ— äººé€ƒå‡ºã€‚\"æœ±å…ƒç’‹ç‚¹å¤´é“ï¼š\"çŸ¥é“äº†ã€‚\"ä¸€é¢æ‘©æŒ²æœ¨åŒ£ï¼Œä¸€é¢è¯´é“ï¼š\"äººçŠ¯æŠ¼å…¥è¯ç‹±ï¼Œä¸¥åŠ çœ‹å®ˆã€‚æ­¤ä»¶å¯†ä¿¡ï¼ŒæŠ„å½•å‰¯æœ¬ï¼ŒåŸä»¶å°å­˜ã€‚\"ä»–åˆåœäº†ä¸€ä¼šå„¿ï¼ŒæŠ¬å¤´é“ï¼š\"ä¸‰æ—¥ä¹‹å†…ï¼Œåˆé—¨å¤–ï¼Œå½“ä¼—å®£åˆ¤ã€‚\"é™ˆä¸œé¢†å‘½é€€å‡ºã€‚æœ±å…ƒç’‹ç‹¬è‡ªåç€ï¼Œç”¨æ‰‹æŒ‡åœ¨æœ¨åŒ£ä¸Šè½»è½»åˆ’ç€ï¼Œæƒ³é“ï¼š\"ç§¦æ¡§é€šæ•Œï¼Œè¯æ®ç¡®å‡¿ã€‚\"ä¾¿å”¤èµµé¼ã€æçº²è¿›æ¥ã€‚äºŒäººæ˜¾ç„¶å·²èµ·èº«ç­‰å€™ï¼Œè¡£å† æ•´é½ã€‚æœ±å…ƒç’‹é‚å°†ç§¦æ¡§ä¹‹äº‹ç»†è¯´ä¸€éï¼ŒäºŒäººé¢é¢ç›¸è§‘ï¼Œç¥è‰²æ²‰é‡ã€‚èµµé¼é“ï¼š\"é™›ä¸‹ï¼Œè‹¥è¡Œå…¬å®¡ï¼Œéœ‡åŠ¨ä¸å°ã€‚\"æçº²æ¥å£é“ï¼š\"è‡£ç­‰å·²è°ƒå…µç»´æŠ¤ä¸´å®‰å„é—¨è¦é“ï¼Œé˜²ä½™å…šä½œä¹±ã€‚\"æœ±å…ƒç’‹é“ï¼š\"æ‹Ÿæ—¨ï¼Œå¸ƒå‘Šå¤©ä¸‹ã€‚\"åˆå‘èµµé¼é“ï¼š\"ç½ªåã€è¯æ®ã€åˆ‘å¾‹ï¼Œéƒ½è¦ä¸€ä¸€åˆ—æ˜ã€‚\"èµµé¼åº”è¯ºï¼Œé‚é€€ï¼Œè„šæ­¥å£°æ¸è¿œï¼Œç»ˆä¸å¯é—»ã€‚"
)

DEFAULT_USER = (
    "è¯·å°†ä»¥ä¸‹ç°ä»£ç™½è¯æ¶¦è‰²æˆé›…è‡´çš„æ–‡å­¦æ–‡æœ¬ï¼š\n\n"
    + ORIGINAL_TEXT
)


PRESET_CASES = {
    "elegant_style": {
        "system": DEFAULT_SYSTEM,
        "user": DEFAULT_USER,
        "description": "é›…è‡´ç»†è…»é£æ ¼æ”¹å†™ï¼ˆä¸è®­ç»ƒæ•°æ®æ ¼å¼å®Œå…¨ä¸€è‡´ï¼‰",
    },
}


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Test Stage-2 instruction-tuned checkpoint")
    parser.add_argument(
        "--model_name_or_path",
        type=str,
        required=True,
        help="Path to the Stage-2 checkpoint folder (or HF repo id).",
    )
    parser.add_argument("--system", type=str, default=DEFAULT_SYSTEM, help="System prompt.")
    parser.add_argument("--user", type=str, default=DEFAULT_USER, help="User message.")
    parser.add_argument(
        "--preset",
        type=str,
        default=None,
        choices=sorted(PRESET_CASES.keys()),
        help="Use a built-in test case (overrides --system/--user if set).",
    )
    parser.add_argument(
        "--input_file",
        type=Path,
        default=None,
        help="Optional file containing multiple user prompts. One JSONL per line with 'system'/'user', or plain text (one prompt per line).",
    )
    # ä¸ºäº†æ”¯æŒè‡³å°‘ ~100 å­—çš„è¾“å‡ºï¼Œé»˜è®¤ç»™å¾—ç¨å¾®é•¿ä¸€ç‚¹
    parser.add_argument("--max_new_tokens", type=int, default=2048)
    parser.add_argument("--temperature", type=float, default=0.3)
    parser.add_argument("--top_p", type=float, default=0.85)
    parser.add_argument("--repetition_penalty", type=float, default=1.3)
    parser.add_argument("--seed", type=int, default=1234)
    parser.add_argument(
        "--attn_impl",
        default="sdpa",
        help="Attention impl for inference (sdpa, eager, or flash_attention_2).",
    )
    parser.add_argument(
        "--base_model_name",
        type=str,
        default=None,
        help="Base model name for loading tokenizer if checkpoint doesn't have it (e.g., Qwen/Qwen2.5-8B-Base).",
    )
    return parser.parse_args()


def build_chat_prompt(system: str, user: str) -> str:
    """Build a single-turn chat prompt using the same format as training.

    Training used messages like:
      <|im_start|>system\n...<|im_end|>\n
      <|im_start|>user\n...<|im_end|>\n
      <|im_start|>assistant\n
    Here we stop before closing the assistant block so generation continues it.
    """

    parts = [
        f"<|im_start|>system\n{system}<|im_end|>",
        f"<|im_start|>user\n{user}<|im_end|>",
        "<|im_start|>assistant\n",
    ]
    return "\n".join(parts)


def main() -> None:
    args = parse_args()
    torch.manual_seed(args.seed)

    model_path = Path(args.model_name_or_path)
    print(f"model_name_or_path = {args.model_name_or_path}")

    # ä¼˜å…ˆå½“ä½œæœ¬åœ°ç›®å½•ä½¿ç”¨ï¼›å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œå†å›é€€ä¸º HF ä»“åº“å
    if model_path.exists():
        print(f"Loading Stage-2 model from local folder: {model_path}")
        
        # å°è¯•ä» checkpoint åŠ è½½ tokenizerï¼Œå¦‚æœå¤±è´¥åˆ™ä»åŸºç¡€æ¨¡å‹åŠ è½½
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            print("âœ“ Tokenizer loaded from checkpoint")
        except (OSError, ValueError, ImportError) as e:
            print(f"âš  Checkpoint ä¸­æ²¡æœ‰ tokenizerï¼Œå°è¯•ä»åŸºç¡€æ¨¡å‹åŠ è½½...")
            
            # å°è¯•å¤šç§æ–¹å¼æ‰¾åˆ°åŸºç¡€æ¨¡å‹
            base_model_path = None
            
            # 1. ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
            if args.base_model_name:
                base_model_path = args.base_model_name
                print(f"  ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°: {base_model_path}")
            
            # 2. å°è¯•ä» config.json è¯»å– _name_or_pathï¼ˆå¯èƒ½æ˜¯æœ¬åœ°è·¯å¾„ï¼‰
            if not base_model_path:
                config_path = model_path / "config.json"
                if config_path.exists():
                    import json
                    with open(config_path, "r") as f:
                        config = json.load(f)
                        base_model_path = config.get("_name_or_path")
                        if base_model_path:
                            print(f"  ä» config.json è¯»å–: {base_model_path}")
                            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                            if base_model_path and not base_model_path.startswith("/") and "/" not in base_model_path[:10]:
                                base_model_path = str((model_path.parent / base_model_path).resolve())
            
            # 3. å°è¯•ä»çˆ¶ç›®å½•æˆ–ç¥–çˆ¶ç›®å½•æ‰¾ Stage1 æ¨¡å‹ï¼ˆæœ¬åœ°è·¯å¾„ï¼‰
            if not base_model_path:
                # stage2_instruction_tuning_corrected/checkpoint-16 -> stage1_style_injection
                parent_dir = model_path.parent.parent
                possible_stage1_paths = [
                    parent_dir / "stage1_style_injection",
                    parent_dir.parent / "stage1_style_injection",  # å†å¾€ä¸Šä¸€å±‚
                ]
                for stage1_path in possible_stage1_paths:
                    if stage1_path.exists():
                        # ç›´æ¥ä½¿ç”¨ Stage1 è·¯å¾„ï¼ˆåŒ…å« tokenizerï¼‰
                        print(f"  æ‰¾åˆ° Stage1 æ¨¡å‹: {stage1_path}")
                        base_model_path = str(stage1_path.resolve())
                        break
            
            # 4. å°è¯•æŸ¥æ‰¾æœ¬åœ° Qwen æ¨¡å‹ç›®å½•
            if not base_model_path:
                # å¸¸è§çš„æœ¬åœ°è·¯å¾„
                possible_local_paths = [
                    Path("/workspace/models/Qwen2.5-8B-Base"),
                    Path("/workspace/models/Qwen2.5-7B-Base"),
                    Path("./models/Qwen2.5-8B-Base"),
                    Path("../models/Qwen2.5-8B-Base"),
                ]
                print("  å°è¯•æœ¬åœ°æ¨¡å‹è·¯å¾„...")
                for local_path in possible_local_paths:
                    if local_path.exists() and (local_path / "tokenizer_config.json").exists():
                        print(f"    æ‰¾åˆ°: {local_path}")
                        base_model_path = str(local_path.resolve())
                        break
            
            if not base_model_path:
                raise ValueError(
                    "æ— æ³•ç¡®å®šåŸºç¡€æ¨¡å‹åç§°æˆ–è·¯å¾„ã€‚\n"
                    "è¯·ä½¿ç”¨ --base_model_name å‚æ•°æŒ‡å®šæœ¬åœ°è·¯å¾„æˆ– HF æ¨¡å‹åç§°ï¼Œ\n"
                    "ä¾‹å¦‚: --base_model_name /workspace/models/Qwen2.5-8B-Base\n"
                    "æˆ–è€…: --base_model_name stage1_style_injection"
                )
            
            print(f"  Loading tokenizer from: {base_model_path}")
            # å°è¯•ä½œä¸ºæœ¬åœ°è·¯å¾„ï¼Œå¦‚æœå¤±è´¥åˆ™ä½œä¸º HF æ¨¡å‹åç§°
            try:
                tokenizer = AutoTokenizer.from_pretrained(base_model_path, local_files_only=True)
                print("  âœ“ ä»æœ¬åœ°åŠ è½½æˆåŠŸ")
            except Exception:
                print("  âš  æœ¬åœ°åŠ è½½å¤±è´¥ï¼Œå°è¯•ä» HuggingFace...")
                tokenizer = AutoTokenizer.from_pretrained(base_model_path)
        
        model_load_id = model_path
    else:
        print(f"âš  æœ¬åœ°æ‰¾ä¸åˆ°ç›®å½•: {model_path}ï¼Œå°†å°è¯•ä½œä¸º Hugging Face æ¨¡å‹ä»“åº“åŠ è½½ã€‚")
        model_load_id = args.model_name_or_path
        tokenizer = AutoTokenizer.from_pretrained(model_load_id)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_load_id,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        attn_implementation=args.attn_impl,
    )
    model.eval()
    # æ ¹æ® preset æˆ–æ‰‹åŠ¨ system/user æ„é€ ä¸€ä¸ªæˆ–å¤šä¸ªæµ‹è¯•ç”¨ä¾‹
    test_cases = []

    if args.input_file is not None:
        # æ–‡ä»¶æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
        # 1) JSONLï¼Œæ¯è¡Œå½¢å¦‚ {"system": "...", "user": "..."}
        # 2) çº¯æ–‡æœ¬ï¼Œæ¯è¡Œä½œä¸º userï¼Œsystem ä½¿ç”¨é»˜è®¤æˆ– preset çš„ system
        print(f"Loading prompts from: {args.input_file}")
        with args.input_file.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    rec = json.loads(line)
                    system = rec.get("system") or args.system
                    user = rec.get("user") or ""
                except json.JSONDecodeError:
                    # å½“ä½œçº¯æ–‡æœ¬ user
                    system = args.system
                    user = line
                test_cases.append((system, user))
    elif args.preset:
        preset = PRESET_CASES[args.preset]
        print(f"Using preset='{args.preset}': {preset['description']}")
        test_cases.append((preset["system"], preset["user"]))
    else:
        test_cases.append((args.system, args.user))

    for idx, (system, user) in enumerate(test_cases, start=1):
        prompt = build_chat_prompt(system, user)
        print("\n" + "=" * 80)
        print(f"Test case #{idx}")
        # print("----- System -----")
        # print(system)
        # print("----- User -----")
        # print(user)
        # print("----- Raw Prompt (truncated) -----")
        # print(prompt[:400] + ("..." if len(prompt) > 400 else ""))
        print("=" * 80)

        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        # ğŸ” è°ƒè¯•ä¿¡æ¯
        print(f"\n[DEBUG] Prompt length: {len(prompt)} chars, {inputs['input_ids'].shape[1]} tokens")
        print(f"[DEBUG] Prompt ends with: ...{prompt[-100:]}")

        with torch.no_grad():
            output_ids = model.generate(
                **inputs,
                max_new_tokens=args.max_new_tokens,
                do_sample=True,
                temperature=args.temperature,
                top_p=args.top_p,
                repetition_penalty=args.repetition_penalty,
                pad_token_id=tokenizer.pad_token_id,  # ğŸ”‘ ä½¿ç”¨æ­£ç¡®çš„ pad_token
                # Stop tokens to prevent unwanted continuation
                eos_token_id=[
                    tokenizer.eos_token_id,
                    tokenizer.convert_tokens_to_ids("<|im_end|>"),
                ],
            )

        completion = tokenizer.decode(output_ids[0], skip_special_tokens=False)
        
        # ğŸ” æ›´å¤šè°ƒè¯•
        print(f"[DEBUG] Generated total: {output_ids.shape[1]} tokens")
        print(f"[DEBUG] New tokens: {output_ids.shape[1] - inputs['input_ids'].shape[1]}")
        print(f"[DEBUG] Completion length: {len(completion)} chars")
        print(f"[DEBUG] Completion starts with: {completion[:100]}")
        print(f"[DEBUG] Has <|im_start|> in completion: {'<|im_start|>' in completion}")
        
        # æå– assistant å›å¤ - å¯»æ‰¾æœ€åä¸€ä¸ª <|im_start|>assistant
        assistant_marker = "<|im_start|>assistant\n"
        if assistant_marker in completion:
            # æ‰¾åˆ°æœ€åä¸€ä¸ªassistantæ ‡è®°
            pos = completion.rfind(assistant_marker)
            assistant_reply = completion[pos + len(assistant_marker):]
            print(f"[DEBUG] Found assistant marker at position {pos}")
            # ç§»é™¤ç»“å°¾çš„ <|im_end|> å¦‚æœæœ‰
            if assistant_reply.endswith("<|im_end|>"):
                assistant_reply = assistant_reply[:-len("<|im_end|>")]
        else:
            assistant_reply = completion
            print(f"[DEBUG] Assistant marker not found, using full completion")
        
        # ï¿½ æ˜¾ç¤ºåŸå§‹è¾“å‡ºï¼ˆæ¸…ç†å‰ï¼‰
        print("===== Raw Assistant Output (before cleaning) =====")
        print(assistant_reply[:500] if len(assistant_reply) > 500 else assistant_reply)
        print("=" * 80)
        
        # ï¿½ğŸ”‘ æ¸…ç†è¾“å‡ºï¼šç§»é™¤å¯èƒ½çš„ prompt æ³„éœ²å’Œæ— å…³å†…å®¹
        # 1. æˆªæ–­äºç« èŠ‚æ ‡é¢˜ã€æç¤ºè¯­ç­‰
        stop_markers = [
            "\nä»»åŠ¡ï¼š", "\nè¦æ±‚ï¼š", "\nåŸæ–‡ï¼š", 
            "\nè¯·ç›´æ¥è¾“å‡º", "\nè¯·åœ¨ä¸", "\nç¦æ­¢",
            "\nè¯·ç»§ç»­é˜…è¯»", "\nç¬¬", "ç« ",  # ç« èŠ‚æ ‡é¢˜
            "aalborg",  # è®­ç»ƒæ•°æ®æ±¡æŸ“
            "\nuser\n", "\nUser\n", 
            "\nsystem\n", "\nSystem\n",
            "\nassistant\n", "\nAssistant\n",
            "<|im_start|>",
        ]
        
        for marker in stop_markers:
            if marker in assistant_reply:
                pos = assistant_reply.find(marker)
                assistant_reply = assistant_reply[:pos]
                break
        
        # 2. å»é™¤ç»“å°¾çš„ä¸å®Œæ•´å¥å­ï¼ˆå¦‚æœä»¥æ ‡ç‚¹ç»“æŸåˆ™ä¿ç•™ï¼‰
        assistant_reply = assistant_reply.strip()
        
        print("===== Assistant Reply =====")
        print(assistant_reply)


if __name__ == "__main__":
    main()
