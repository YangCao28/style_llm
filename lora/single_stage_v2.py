"""å•é˜¶æ®µæŒ‡ä»¤å¾®è°ƒ V2 - æ”¯æŒSoft Masking

ğŸ”‘ å…³é”®æ”¹è¿›ï¼š
  1. æ”¯æŒæ··åˆå±è”½æ³•ï¼ˆ20% Soft Maskingï¼‰
  2. å¯é…ç½®soft_mask_ratio
  3. é™ä½learning rateä»¥é…åˆsoft masking

Usage:
    python -m lora.single_stage_v2 --config lora/single_stage_v2_config.json
"""

from __future__ import annotations

import argparse
import json
import random
from pathlib import Path

import torch
from datasets import load_dataset
from peft import LoraConfig, get_peft_model
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    TrainerCallback,
)


def formatting_func_with_soft_mask(example, tokenizer, max_seq_length=2048, soft_mask_ratio=0.2, debug=False):
    """æ ¼å¼åŒ–å¯¹è¯æ•°æ® - æ”¯æŒSoft Masking
    
    Args:
        soft_mask_ratio: Soft Maskingæ¯”ä¾‹ï¼Œè¿™éƒ¨åˆ†æ ·æœ¬ä¸å±è”½userç«¯ï¼ˆé»˜è®¤20%ï¼‰
                        è®¾ä¸º0.0åˆ™å®Œå…¨Hard Maskï¼Œè®¾ä¸º1.0åˆ™å…¨é‡å­¦ä¹ 
    """
    conversations = example.get("conversations", [])
    if not conversations:
        return {"input_ids": [], "attention_mask": [], "labels": []}
    
    # æ„å»ºå¯¹è¯
    messages = []
    assistant_response = None
    
    for msg in conversations:
        role = msg.get("role") or msg.get("from") or "user"
        content = msg.get("content") or msg.get("value") or ""
        
        if role in ("system", "sys"):
            norm_role = "system"
        elif role in ("assistant", "gpt", "bot"):
            norm_role = "assistant"
            assistant_response = content.strip()
        else:
            norm_role = "user"
        
        messages.append({"role": norm_role, "content": content})
    
    if assistant_response is None:
        return {"input_ids": [], "attention_mask": [], "labels": []}
    
    # æ„å»º promptï¼ˆä¸åŒ…å« assistant å›å¤å†…å®¹ï¼Œä½†åŒ…å« assistant å¼€å§‹æ ‡ç­¾ï¼‰
    prompt_parts = []
    for msg in messages:
        if msg["role"] != "assistant":
            prompt_parts.append(f"<|im_start|>{msg['role']}\n{msg['content']}<|im_end|>")
    prompt_parts.append("<|im_start|>assistant\n")
    prompt_text = "\n".join(prompt_parts)
    
    # åˆ†åˆ«tokenize
    prompt_ids = tokenizer(prompt_text, add_special_tokens=False)["input_ids"]
    assistant_text = assistant_response + "<|im_end|>"
    assistant_ids = tokenizer(assistant_text, add_special_tokens=False)["input_ids"]
    
    # æ‹¼æ¥å®Œæ•´åºåˆ—
    input_ids = prompt_ids + assistant_ids
    
    # æˆªæ–­
    if len(input_ids) > max_seq_length:
        input_ids = input_ids[:max_seq_length]
        if len(prompt_ids) > max_seq_length:
            return {"input_ids": [], "attention_mask": [], "labels": []}
    
    # ğŸ”¥ æ··åˆå±è”½æ³•ï¼šå†³å®šæ˜¯å¦ä½¿ç”¨Soft Masking
    use_soft_mask = random.random() < soft_mask_ratio
    
    if use_soft_mask:
        # Soft Masking: å…¨é‡å­¦ä¹ ï¼Œä¸å±è”½user
        # æ¨¡å‹éœ€è¦é¢„æµ‹æ•´æ®µå¯¹è¯ï¼Œæœ‰åŠ©äºç†è§£æŒ‡ä»¤
        labels = input_ids.copy()
    else:
        # Hard Masking: åªè®¡ç®—assistantçš„loss
        labels = [-100] * len(prompt_ids) + assistant_ids
    
    labels = labels[:max_seq_length]
    
    # Padding
    padding_length = max_seq_length - len(input_ids)
    input_ids = input_ids + [tokenizer.pad_token_id] * padding_length
    attention_mask = [1] * len(input_ids[:max_seq_length - padding_length]) + [0] * padding_length
    labels = labels + [-100] * (max_seq_length - len(labels))
    
    if debug:
        mask_type = "Soft (å…¨é‡å­¦ä¹ )" if use_soft_mask else "Hard (åªå­¦Assistant)"
        print(f"\nğŸ” æ ·æœ¬å±è”½ç±»å‹: {mask_type}")
        print(f"  Prompt tokens: {len(prompt_ids)}")
        print(f"  Assistant tokens: {len(assistant_ids)}")
        print(f"  Labelsä¸­-100æ•°é‡: {sum(1 for l in labels if l == -100)}")
        print(f"  Labelsä¸­æœ‰æ•ˆæ•°é‡: {sum(1 for l in labels if l != -100)}")
    
    return {
        "input_ids": input_ids[:max_seq_length],
        "attention_mask": attention_mask[:max_seq_length],
        "labels": labels[:max_seq_length],
    }


class LossRecorderCallback(TrainerCallback):
    def __init__(self):
        self.training_losses = []
        self.eval_losses = []
        self.steps = []
        self.eval_steps = []
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            if "loss" in logs:
                self.training_losses.append(logs["loss"])
                self.steps.append(state.global_step)
                if getattr(state, "is_world_process_zero", True):
                    print(f"[step {state.global_step}] train_loss = {logs['loss']:.4f}")
            
            if "eval_loss" in logs:
                self.eval_losses.append(logs["eval_loss"])
                self.eval_steps.append(state.global_step)
                if getattr(state, "is_world_process_zero", True):
                    print(f"[step {state.global_step}] eval_loss = {logs['eval_loss']:.4f}")


class TestGenerationCallback(TrainerCallback):
    """æ¯Næ­¥ç”Ÿæˆæµ‹è¯•æ ·æœ¬ï¼Œä¾›äººå·¥è¯„ä¼°æ”¹å†™æ•ˆæœ"""
    
    def __init__(self, model, tokenizer, test_prompts, test_interval=100, output_dir="."):
        self.model = model
        self.tokenizer = tokenizer
        self.test_prompts = test_prompts
        self.test_interval = test_interval
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
    
    def on_step_end(self, args, state, control, **kwargs):
        # æ¯test_intervalæ­¥è¿è¡Œä¸€æ¬¡æµ‹è¯•
        if state.global_step % self.test_interval == 0 and state.global_step > 0:
            self._run_test_generation(state.global_step)
    
    def _run_test_generation(self, step):
        """è¿è¡Œæµ‹è¯•ç”Ÿæˆ"""
        print(f"\n{'='*80}")
        print(f"ğŸ§ª [Step {step}] è¿è¡Œæµ‹è¯•ç”Ÿæˆ - äººå·¥è¯„ä¼°æ”¹å†™æ•ˆæœ")
        print(f"{'='*80}")
        
        self.model.eval()
        test_results = []
        
        with torch.no_grad():
            for i, prompt in enumerate(self.test_prompts):
                print(f"\n--- æµ‹è¯•æ ·æœ¬ {i+1}/{len(self.test_prompts)} ---")
                print(f"åŸæ–‡: {prompt[:100]}...")
                
                # æ„å»ºè¾“å…¥
                messages = [
                    {"role": "user", "content": f"è¯·å°†ä¸‹é¢çš„æ–‡è¨€æ–‡æ”¹å†™ä¸ºç°ä»£ç™½è¯æ–‡ï¼š\n\n{prompt}"}
                ]
                input_text = "\n".join([
                    f"<|im_start|>{msg['role']}\n{msg['content']}<|im_end|>"
                    for msg in messages
                ]) + "\n<|im_start|>assistant\n"
                
                input_ids = self.tokenizer(input_text, return_tensors="pt").input_ids.to(self.model.device)
                
                # ç”Ÿæˆ
                output_ids = self.model.generate(
                    input_ids,
                    max_new_tokens=512,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
                
                # è§£ç è¾“å‡º
                generated_text = self.tokenizer.decode(
                    output_ids[0][input_ids.shape[1]:],
                    skip_special_tokens=True
                ).strip()
                
                print(f"æ”¹å†™: {generated_text[:200]}...")
                
                test_results.append({
                    "step": step,
                    "sample_id": i + 1,
                    "original": prompt,
                    "rewritten": generated_text
                })
        
        # ä¿å­˜æµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶
        test_file = self.output_dir / f"test_generation_step_{step}.json"
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(test_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ“ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {test_file}")
        print(f"{'='*80}\n")
        
        self.model.train()


def main():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print(f"âœ“ CUDA: {torch.cuda.get_device_name(0)}")
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=Path, required=True, help="Path to JSON config file")
    parser.add_argument("--soft_mask_ratio", type=float, default=None, help="Override soft masking ratio")
    parser.add_argument("--resume_from_checkpoint", type=str, default=None, help="Resume from checkpoint path")
    args = parser.parse_args()

    # åŠ è½½é…ç½®
    with open(args.config, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # å‚æ•°è¦†ç›–
    if args.soft_mask_ratio is not None:
        config["soft_mask_ratio"] = args.soft_mask_ratio
    
    base_model_name = config["base_model_name"]
    dataset_path = config["dataset_path"]
    validation_dataset_path = config.get("validation_dataset_path")  # å¯é€‰éªŒè¯é›†
    output_dir = config["output_dir"]
    soft_mask_ratio = config.get("soft_mask_ratio", 0.2)  # é»˜è®¤20%
    
    print(f"\nğŸ”§ é…ç½®:")
    print(f"  Base Model: {base_model_name}")
    print(f"  Dataset: {dataset_path}")
    print(f"  Validation: {validation_dataset_path or 'None'}")
    print(f"  Output: {output_dir}")
    print(f"  Soft Mask Ratio: {soft_mask_ratio:.1%} ({'æ··åˆå±è”½' if 0 < soft_mask_ratio < 1 else 'å…¨é‡å­¦ä¹ ' if soft_mask_ratio == 1 else 'Hard Mask'})")
    print(f"  Learning Rate: {config.get('learning_rate', 4e-5)}")
    print(f"  LoRA Rank: {config.get('lora_r', 64)}")
    
    # åŠ è½½tokenizer
    print(f"\nğŸ“¦ åŠ è½½ tokenizer...")
    tokenizer_path = config.get("tokenizer_path", base_model_name)
    tokenizer = AutoTokenizer.from_pretrained(
        tokenizer_path,
        trust_remote_code=True,
        use_fast=False,
        local_files_only=True
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # åŠ è½½base model
    print(f"ğŸ“¦ åŠ è½½ base model...")
    model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        attn_implementation="sdpa",
        local_files_only=True
    )
    
    # é…ç½®LoRA
    lora_config = LoraConfig(
        r=config.get("lora_r", 64),
        lora_alpha=config.get("lora_alpha", 128),
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=config.get("lora_dropout", 0.05),
        bias="none",
        task_type="CAUSAL_LM",
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # åŠ è½½æ•°æ®é›†
    print(f"\nğŸ“Š åŠ è½½æ•°æ®é›†: {dataset_path}")
    dataset = load_dataset("json", data_files=dataset_path, split="train")
    print(f"âœ“ åŠ è½½ {len(dataset)} æ¡æ ·æœ¬")
    
    # æ ¼å¼åŒ–æ•°æ®é›† - ä¼ å…¥soft_mask_ratio
    print(f"\nğŸ”„ æ ¼å¼åŒ–æ•°æ®é›† (Soft Mask Ratio={soft_mask_ratio:.1%})...")
    
    def format_fn(example):
        return formatting_func_with_soft_mask(
            example,
            tokenizer,
            max_seq_length=config.get("max_seq_length", 2048),
            soft_mask_ratio=soft_mask_ratio,
            debug=False
        )
    
    formatted_dataset = dataset.map(
        format_fn,
        remove_columns=dataset.column_names,
        num_proc=1,
        desc="Formatting with Soft Masking"
    )
    
    # è¿‡æ»¤ç©ºæ ·æœ¬
    formatted_dataset = formatted_dataset.filter(lambda x: len(x["input_ids"]) > 0)
    print(f"âœ“ æ ¼å¼åŒ–å®Œæˆ: {len(formatted_dataset)} æ¡æœ‰æ•ˆæ ·æœ¬")
    
    # åŠ è½½éªŒè¯é›†ï¼ˆå¦‚æœæä¾›ï¼‰
    formatted_eval_dataset = None
    if validation_dataset_path:
        print(f"\nğŸ“Š åŠ è½½éªŒè¯é›†: {validation_dataset_path}")
        eval_dataset = load_dataset("json", data_files=validation_dataset_path, split="train")
        print(f"âœ“ åŠ è½½ {len(eval_dataset)} æ¡éªŒè¯æ ·æœ¬")
        
        formatted_eval_dataset = eval_dataset.map(
            format_fn,
            remove_columns=eval_dataset.column_names,
            num_proc=1,
            desc="Formatting validation set"
        )
        formatted_eval_dataset = formatted_eval_dataset.filter(lambda x: len(x["input_ids"]) > 0)
        print(f"âœ“ éªŒè¯é›†æ ¼å¼åŒ–å®Œæˆ: {len(formatted_eval_dataset)} æ¡æœ‰æ•ˆæ ·æœ¬")
    
    # è®­ç»ƒå‚æ•°
    eval_steps = config.get("eval_steps", 100)  # é»˜è®¤æ¯100æ­¥è¯„ä¼°
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=config.get("num_train_epochs", 2.0),
        per_device_train_batch_size=config.get("per_device_train_batch_size", 4),
        per_device_eval_batch_size=config.get("per_device_eval_batch_size", 4),
        gradient_accumulation_steps=config.get("gradient_accumulation_steps", 4),
        learning_rate=config.get("learning_rate", 4e-5),
        lr_scheduler_type=config.get("lr_scheduler_type", "cosine"),
        warmup_ratio=config.get("warmup_ratio", 0.1),
        logging_steps=config.get("logging_steps", 10),
        eval_strategy="steps" if formatted_eval_dataset else "no",
        eval_steps=eval_steps if formatted_eval_dataset else None,
        save_strategy="steps",
        save_steps=config.get("save_steps", 100),  # æ¯100æ­¥ä¿å­˜checkpoint
        save_total_limit=config.get("save_total_limit", 5),  # ä¿ç•™æœ€è¿‘5ä¸ªcheckpoint
        load_best_model_at_end=True if formatted_eval_dataset else False,
        metric_for_best_model="eval_loss" if formatted_eval_dataset else None,
        bf16=True,
        gradient_checkpointing=True,
        dataloader_num_workers=4,
        report_to="none",
    )
    
    # å‡†å¤‡æµ‹è¯•æ ·æœ¬ï¼ˆç”¨äºäººå·¥è¯„ä¼°ï¼‰
    test_prompts = config.get("test_prompts", [
        "è¯è¯´å¤©ä¸‹å¤§åŠ¿ï¼Œåˆ†ä¹…å¿…åˆï¼Œåˆä¹…å¿…åˆ†ã€‚",
        "å´è¯´ç„å¾·å¼•å†›å‰è¿›ï¼Œå¿½æŠ¥å‰é¢æœ‰ä¸€å†›é˜»è·¯ã€‚",
        "ä¸”è¯´æ›¹æ“å¼•å…µè‡³èµ¤å£ï¼Œä¸å‘¨ç‘œç›¸æ‹’ã€‚",
    ])
    
    # å›è°ƒ
    loss_recorder = LossRecorderCallback()
    test_callback = TestGenerationCallback(
        model=model,
        tokenizer=tokenizer,
        test_prompts=test_prompts,
        test_interval=config.get("test_interval", 100),  # æ¯100æ­¥æµ‹è¯•
        output_dir=output_dir
    )
    
    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=formatted_dataset,
        eval_dataset=formatted_eval_dataset,
        callbacks=[loss_recorder, test_callback],
    )
    
    # è®­ç»ƒ
    resume_checkpoint = args.resume_from_checkpoint or config.get("resume_from_checkpoint")
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    if resume_checkpoint:
        print(f"  ğŸ“‚ ä»checkpointæ¢å¤: {resume_checkpoint}")
    print(f"  {'='*80}")
    print(f"  ğŸ¯ å…³é”®é…ç½®:")
    print(f"     - Soft Masking: {soft_mask_ratio:.1%} æ ·æœ¬å…¨é‡å­¦ä¹ ")
    print(f"     - Hard Masking: {(1-soft_mask_ratio):.1%} æ ·æœ¬åªå­¦Assistant")
    print(f"     - Learning Rate: {config.get('learning_rate', 4e-5)} (æ¸©å’Œä»¥é…åˆSoft Masking)")
    print(f"  {'='*80}\n")
    
    trainer.train(resume_from_checkpoint=resume_checkpoint)
    
    # ä¿å­˜
    print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
    trainer.save_model(output_dir)
    
    # æ˜¾ç¤ºè®­ç»ƒæ›²çº¿
    if loss_recorder.training_losses:
        print(f"\nğŸ“Š è®­ç»ƒç»Ÿè®¡:")
        print(f"  åˆå§‹ train_loss: {loss_recorder.training_losses[0]:.4f}")
        print(f"  æœ€ç»ˆ train_loss: {loss_recorder.training_losses[-1]:.4f}")
        print(f"  Train Loss ä¸‹é™: {loss_recorder.training_losses[0] - loss_recorder.training_losses[-1]:.4f}")
        
        if loss_recorder.eval_losses:
            print(f"  åˆå§‹ eval_loss: {loss_recorder.eval_losses[0]:.4f}")
            print(f"  æœ€ç»ˆ eval_loss: {loss_recorder.eval_losses[-1]:.4f}")
            print(f"  Eval Loss ä¸‹é™: {loss_recorder.eval_losses[0] - loss_recorder.eval_losses[-1]:.4f}")
    
    # ä¿å­˜lossæ›²çº¿åˆ°JSONæ–‡ä»¶
    loss_history = {
        "train": {
            "steps": loss_recorder.steps,
            "losses": loss_recorder.training_losses
        },
        "eval": {
            "steps": loss_recorder.eval_steps,
            "losses": loss_recorder.eval_losses
        }
    }
    
    loss_file = Path(output_dir) / "loss_history.json"
    with open(loss_file, 'w', encoding='utf-8') as f:
        json.dump(loss_history, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ“ è®­ç»ƒå®Œæˆï¼")
    print(f"  ğŸ“ æ¨¡å‹ä¿å­˜åˆ°: {output_dir}")
    print(f"  ğŸ“Š Lossæ›²çº¿ä¿å­˜åˆ°: {loss_file}")


if __name__ == "__main__":
    main()
