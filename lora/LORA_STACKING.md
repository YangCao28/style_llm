# LoRA å åŠ æœºåˆ¶è¯¦è§£

## ä»€ä¹ˆæ˜¯ LoRA å åŠ ï¼Ÿ

å½“æ¨¡åž‹åŒ…å«å¤šä¸ª LoRA adapters æ—¶ï¼ˆå¦‚ Stage2 çš„ `style` + `instruct`ï¼‰ï¼ŒæŽ¨ç†æ—¶è¿™äº› adapters ä¼š**è‡ªåŠ¨å åŠ **ï¼ˆç›¸åŠ ï¼‰åº”ç”¨åˆ°åŸºåº§æ¨¡åž‹ä¸Šã€‚

## ðŸ”‘ å…³é”®æ¦‚å¿µï¼šä¸¤ä¸ª Adapter åœ¨ä¸åŒæ–‡ä»¶å¤¹

**é‡è¦**ï¼šStage1 å’Œ Stage2 çš„ adapters æ˜¯åˆ†å¼€å­˜å‚¨çš„ï¼š
- **Style adapter**ï¼ˆStage1è®­ç»ƒå¾—åˆ°ï¼‰ï¼š`stage1_style_injection/checkpoint-531/`
- **Instruct adapter**ï¼ˆStage2è®­ç»ƒå¾—åˆ°ï¼‰ï¼š`stage2_instruct_new_adapter/`

**ä¸ºä»€ä¹ˆåˆ†å¼€ï¼Ÿ**
- Stage1 è®­ç»ƒåŽä¿å­˜äº† style adapter
- Stage2 è®­ç»ƒæ—¶åŠ è½½ Stage1 çš„ style adapterï¼ˆå†»ç»“ï¼‰ï¼Œæ·»åŠ æ–°çš„ instruct adapterï¼Œè®­ç»ƒåŽ**åªä¿å­˜ instruct adapter**
- å› æ­¤æµ‹è¯• Stage2 æ—¶éœ€è¦**åˆ†åˆ«åŠ è½½ä¸¤ä¸ª adapters**ï¼Œè®© PEFT åº“è‡ªåŠ¨å åŠ å®ƒä»¬

**ç›®å½•ç»“æž„**ï¼š
```
é¡¹ç›®ç›®å½•/
â”œâ”€â”€ stage1_style_injection/
â”‚   â””â”€â”€ checkpoint-531/
â”‚       â”œâ”€â”€ adapter_config.json
â”‚       â””â”€â”€ adapter_model.safetensors  # Style adapter æƒé‡
â””â”€â”€ stage2_instruct_new_adapter/
    â”œâ”€â”€ adapter_config.json
    â””â”€â”€ adapter_model.safetensors      # Instruct adapter æƒé‡
```

## æ•°å­¦åŽŸç†

### å•ä¸ª LoRA
```
W_modified = W_base + Î”W
Î”W = B @ A  (rank-r çŸ©é˜µ)
```

### å¤šä¸ª LoRA å åŠ 
```
W_modified = W_base + Î”W_style + Î”W_instruct
           = W_base + (B_style @ A_style) + (B_instruct @ A_instruct)
```

**å…³é”®**ï¼šå¤šä¸ª LoRA çš„æ•ˆæžœæ˜¯**ç›¸åŠ çš„**ï¼Œä¸æ˜¯æ›¿æ¢æˆ–è¦†ç›–ã€‚

## åœ¨æˆ‘ä»¬çš„é¡¹ç›®ä¸­

### Stage1ï¼šå•ä¸ª Adapter
```
Base Model (Qwen3-8B-Base)
    â””â”€â”€ LoRA-style (é£Žæ ¼æ³¨å…¥)
```

è¾“å‡º = Base + style

### Stage2ï¼šåŒ Adapter å åŠ 
```
Base Model (Qwen3-8B-Base)
    â”œâ”€â”€ LoRA-style (é£Žæ ¼æ³¨å…¥ï¼Œå†»ç»“)
    â””â”€â”€ LoRA-instruct (æŒ‡ä»¤éµå¾ªï¼Œè®­ç»ƒ)
```

è¾“å‡º = Base + style + instruct

## PEFT åº“çš„é»˜è®¤è¡Œä¸º

### è‡ªåŠ¨å åŠ 
```python
# åŠ è½½åŒ…å«å¤šä¸ª adapters çš„æ¨¡åž‹
model = PeftModel.from_pretrained(base_model, "stage2_instruct_new_adapter")

# æŽ¨ç†æ—¶ï¼Œæ‰€æœ‰ adapters è‡ªåŠ¨å åŠ 
output = model.generate(...)  # è‡ªåŠ¨åº”ç”¨ style + instruct
```

### æ‰‹åŠ¨æŽ§åˆ¶ï¼ˆé«˜çº§ç”¨æ³•ï¼‰

**å¯ç”¨ç‰¹å®š adapterï¼š**
```python
# åªä½¿ç”¨ style adapter
model.set_adapter("style")
output = model.generate(...)  # åªæœ‰é£Žæ ¼ï¼Œæ²¡æœ‰æŒ‡ä»¤èƒ½åŠ›

# åªä½¿ç”¨ instruct adapter
model.set_adapter("instruct")
output = model.generate(...)  # åªæœ‰æŒ‡ä»¤ï¼Œæ²¡æœ‰é£Žæ ¼

# å¯ç”¨æ‰€æœ‰ adaptersï¼ˆé»˜è®¤ï¼‰
model.enable_adapters()
output = model.generate(...)  # style + instruct å åŠ 
```

**ç¦ç”¨æ‰€æœ‰ adaptersï¼š**
```python
model.disable_adapters()
output = model.generate(...)  # çº¯åŸºåº§æ¨¡åž‹ï¼Œæ— ä»»ä½• adapter
```

## æµ‹è¯•è„šæœ¬çš„å¤„ç†

### å‚æ•°è®¾è®¡

æµ‹è¯•è„šæœ¬ä½¿ç”¨ç®€æ´ç»Ÿä¸€çš„å‚æ•°è®¾è®¡ï¼Œæ ¹æ®æä¾›çš„å‚æ•°**è‡ªåŠ¨åˆ¤æ–­æ¨¡å¼**ï¼š

| å‚æ•°ç»„åˆ | æ¨¡å¼ | è¯´æ˜Ž |
|---------|------|------|
| `--base_model` | åŸºåº§æ¨¡å¼ | æµ‹è¯•çº¯åŸºåº§æ¨¡åž‹ |
| `--lora_model` | å•adapter | æµ‹è¯•ä¸€ä¸ªadapterï¼ˆå¦‚Stage1ï¼‰ |
| `--style_adapter` + `--instruct_adapter` | åŒadapterå åŠ  | æµ‹è¯•ä¸¤ä¸ªadapterå åŠ ï¼ˆStage2ï¼‰ |

**è‡ªåŠ¨base modelæ£€æµ‹ä¼˜å…ˆçº§**ï¼š
1. å‘½ä»¤è¡Œå‚æ•° `--base_model`ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
2. adapterçš„ `adapter_config.json` ä¸­çš„ `base_model_name_or_path`
3. å½“å‰ç›®å½•ä¸‹çš„ `Qwen3-8B-Base/`

### å½“å‰å®žçŽ°
```python
# test_stage2_instruction.py
model = PeftModel.from_pretrained(base_model, lora_path)

# æ£€æŸ¥ adapters
adapters = list(model.peft_config.keys())
print(f"Adapters: {adapters}")

# å¦‚æžœæœ‰å¤šä¸ªï¼Œæç¤ºç”¨æˆ·
if len(adapters) > 1:
    print("All adapters will be stacked during inference")
```

### è¾“å‡ºç¤ºä¾‹
```
Mode: Dual LoRA (Stacked Adapters)

Style adapter:    stage1_style_injection/checkpoint-531
Instruct adapter: stage2_instruct_new_adapter
Base: Qwen3-8B-Base (from style adapter config)

âœ“ Loaded style adapter
âœ“ Loaded instruct adapter

ðŸ”— Stacking adapters:
  âœ“ style
  âœ“ instruct

âœ“ All adapters will be stacked during inference
  Formula: W = W_base + Î”W_style + Î”W_instruct
```

**è§£é‡Š**ï¼š
- åŒæ—¶æŒ‡å®š `--style_adapter` å’Œ `--instruct_adapter` è‡ªåŠ¨å¯ç”¨åŒadapteræ¨¡å¼
- PEFT åº“ä¼šåˆ†åˆ«åŠ è½½ä¸¤ä¸ªadapteræ–‡ä»¶å¤¹çš„æƒé‡å¹¶è‡ªåŠ¨å åŠ 
- æŽ¨ç†æ—¶æ•ˆæžœï¼š`W = W_base + Î”W_style + Î”W_instruct`
- `--base_model` å‚æ•°å¯é€‰ï¼ˆä¼˜å…ˆçº§ï¼šå‘½ä»¤è¡Œ > adapter_config.json > å½“å‰ç›®å½•ï¼‰

## è®­ç»ƒæ—¶çš„å åŠ 

### Stage2 è®­ç»ƒé…ç½®
```python
# stage2_with_new_adapter.py

# 1. åŠ è½½ base model
base_model = AutoModelForCausalLM.from_pretrained("Qwen3-8B-Base")

# 2. åŠ è½½ Stage1 çš„ style adapter
model = PeftModel.from_pretrained(base_model, "stage1_checkpoint", adapter_name="style")

# 3. æ·»åŠ æ–°çš„ instruct adapter
model.add_adapter("instruct", lora_config)

# 4. å†»ç»“ styleï¼Œåªè®­ç»ƒ instruct
model.set_adapter("instruct")
for name, param in model.named_parameters():
    if "style" in name:
        param.requires_grad = False
```

**è®­ç»ƒæ—¶**ï¼š
- Forward: Base + style + instructï¼ˆstyle å†»ç»“ï¼Œä½†å‚ä¸Žå‰å‘ä¼ æ’­ï¼‰
- Backward: åªæ›´æ–° instruct çš„å‚æ•°

**æŽ¨ç†æ—¶**ï¼š
- Forward: Base + style + instructï¼ˆä¸¤ä¸ª adapter éƒ½ç”Ÿæ•ˆï¼‰

## éªŒè¯å åŠ æ•ˆæžœ

### æ–¹æ³•1ï¼šå¯¹æ¯”æµ‹è¯•
```bash
# æµ‹è¯•åŸºåº§æ¨¡åž‹
python -m lora.test_stage2_instruction \
    --base_model Qwen3-8B-Base \
    > base_output.txt

# æµ‹è¯• Stage1ï¼ˆåªæœ‰ styleï¼‰
python -m lora.test_stage2_instruction \
    --lora_model stage1_style_injection/checkpoint-531 \
    > stage1_output.txt

# æµ‹è¯• Stage2ï¼ˆstyle + instruct å åŠ ï¼‰
python -m lora.test_stage2_instruction \
    --style_adapter stage1_style_injection/checkpoint-531 \
    --instruct_adapter stage2_instruct_new_adapter \
    > stage2_output.txt

# å¯¹æ¯”ç»“æžœ
diff base_output.txt stage1_output.txt
diff stage1_output.txt stage2_output.txt
```

**é¢„æœŸç»“æžœ**ï¼š
- `base_output.txt`: åŸºç¡€è¾“å‡ºï¼Œæ— é£Žæ ¼ï¼Œä¸å¬æŒ‡ä»¤ï¼ˆåŽŸå§‹ Qwen3-8B-Baseï¼‰
- `stage1_output.txt`: æœ‰é›…è‡´é£Žæ ¼ï¼Œä½†ä¸å¬æŒ‡ä»¤ï¼ˆå¯èƒ½ä¼šç»­å†™è€Œä¸æ˜¯å›žç­”ï¼‰
- `stage2_output.txt`: æœ‰é£Žæ ¼ + å¬æŒ‡ä»¤ + æ­£ç¡®åœæ­¢ï¼ˆstyle å’Œ instruct å®Œç¾Žå åŠ ï¼‰

### æ–¹æ³•2ï¼šæ£€æŸ¥å‚æ•°
```python
# æ£€æŸ¥æ¨¡åž‹åŒ…å«çš„ adapters
for name, module in model.named_modules():
    if "lora" in name.lower():
        print(name)

# è¾“å‡ºï¼š
# base_model.model.model.layers.0.self_attn.q_proj.lora_A.style
# base_model.model.model.layers.0.self_attn.q_proj.lora_B.style
# base_model.model.model.layers.0.self_attn.q_proj.lora_A.instruct
# base_model.model.model.layers.0.self_attn.q_proj.lora_B.instruct
```

## å¸¸è§é—®é¢˜

### Q1: Adapters å åŠ ä¼šå†²çªå—ï¼Ÿ

**A**: ä¸ä¼šï¼å› ä¸ºï¼š
1. å®ƒä»¬æ˜¯**ç›¸åŠ **å…³ç³»ï¼Œä¸æ˜¯è¦†ç›–
2. Style å’Œ Instruct å­¦ä¹ çš„æ˜¯**ä¸åŒæ–¹é¢**çš„èƒ½åŠ›
3. è®­ç»ƒæ—¶ style è¢«å†»ç»“ï¼Œä¿è¯äº†èƒ½åŠ›åˆ†ç¦»

### Q2: èƒ½å¦é€‰æ‹©æ€§ä½¿ç”¨ adapterï¼Ÿ

**A**: å¯ä»¥ï¼ä½¿ç”¨ `model.set_adapter("style")` æˆ– `model.set_adapter("instruct")`

### Q3: å åŠ é¡ºåºé‡è¦å—ï¼Ÿ

**A**: æ•°å­¦ä¸Šä¸é‡è¦ï¼ˆåŠ æ³•äº¤æ¢å¾‹ï¼‰ï¼Œä½†ï¼š
- è®­ç»ƒé¡ºåºé‡è¦ï¼šå…ˆ style åŽ instruct
- åŠ è½½é¡ºåºæœ€å¥½ä¿æŒä¸€è‡´

### Q4: å¦‚ä½•ç¡®è®¤å åŠ ç”Ÿæ•ˆï¼Ÿ

**A**: çœ‹è¾“å‡ºæ•ˆæžœï¼š
- âœ… æ—¢æœ‰é›…è‡´æ–‡å­¦é£Žæ ¼ï¼ˆæ¥è‡ª styleï¼‰
- âœ… åˆèƒ½éµå¾ªæŒ‡ä»¤æ ¼å¼ï¼ˆæ¥è‡ª instructï¼‰
- âœ… ç”ŸæˆåŽæ­£ç¡®åœæ­¢ï¼ˆæ¥è‡ª instructï¼‰

## æŠ€æœ¯ç»†èŠ‚

### Adapter çš„å­˜å‚¨ç»“æž„

**å®žé™…æƒ…å†µ**ï¼ˆä¸¤ä¸ªadapteråˆ†å¼€å­˜å‚¨ï¼‰ï¼š
```
é¡¹ç›®ç›®å½•/
â”œâ”€â”€ stage1_style_injection/checkpoint-531/
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â””â”€â”€ adapter_model.safetensors    # Style adapter æƒé‡
â””â”€â”€ stage2_instruct_new_adapter/
    â”œâ”€â”€ adapter_config.json
    â””â”€â”€ adapter_model.safetensors    # Instruct adapter æƒé‡
```

**åŠ è½½æ—¶è¡Œä¸º**ï¼š
- Stage2 æµ‹è¯•æ—¶ï¼Œåˆ†åˆ«ä»Žä¸¤ä¸ªç›®å½•åŠ è½½adapter
- PEFT åº“åœ¨å‰å‘ä¼ æ’­æ—¶è‡ªåŠ¨å åŠ ä¸¤ä¸ªadapterçš„æ•ˆæžœ

### Forward è¿‡ç¨‹
```python
# ä¼ªä»£ç 
def forward(x):
    # åŸºåº§æ¨¡åž‹
    h = base_linear(x)
    
    # å åŠ æ‰€æœ‰å¯ç”¨çš„ adapters
    for adapter in active_adapters:
        lora_A = adapter.lora_A
        lora_B = adapter.lora_B
        h = h + lora_B(lora_A(x))
    
    return h
```

## å®Œæ•´æµ‹è¯•æµç¨‹

### 1. æµ‹è¯•åŸºåº§æ¨¡åž‹ï¼ˆbaselineï¼‰
```bash
python -m lora.test_stage2_instruction \
    --base_model Qwen3-8B-Base \
    --preset elegant_style
```

### 2. æµ‹è¯• Stage1ï¼ˆä»…é£Žæ ¼ adapterï¼‰
```bash
python -m lora.test_stage2_instruction \
    --lora_model stage1_style_injection/checkpoint-531 \
    --preset elegant_style
```

### 3. æµ‹è¯• Stage2ï¼ˆé£Žæ ¼ + æŒ‡ä»¤åŒ adapters å åŠ ï¼‰
```bash
# âš ï¸ æ­£ç¡®æ–¹å¼ï¼šåˆ†åˆ«æŒ‡å®šä¸¤ä¸ªadapterçš„è·¯å¾„
python -m lora.test_stage2_instruction \
    --style_adapter stage1_style_injection/checkpoint-531 \
    --instruct_adapter stage2_instruct_new_adapter \
    --preset elegant_style

# å¯é€‰ï¼šæ‰‹åŠ¨æŒ‡å®šbase modelï¼ˆè¦†ç›–è‡ªåŠ¨æ£€æµ‹ï¼‰
python -m lora.test_stage2_instruction \
    --base_model /path/to/Qwen3-8B-Base \
    --style_adapter stage1_style_injection/checkpoint-531 \
    --instruct_adapter stage2_instruct_new_adapter \
    --preset elegant_style
```

### 4. å¯¹æ¯”åˆ†æž
è§‚å¯Ÿä¸‰ä¸ªè¾“å‡ºçš„åŒºåˆ«ï¼š
- **Base**: çŽ°ä»£ç™½è¯ï¼Œä¸éµå¾ªæ”¹å†™æŒ‡ä»¤
- **Stage1**: é›…è‡´é£Žæ ¼ï¼Œä½†å¯èƒ½ç»§ç»­ç»­å†™ï¼ˆä¸å¬æŒ‡ä»¤ï¼‰
- **Stage2**: é›…è‡´é£Žæ ¼ + éµå¾ªæŒ‡ä»¤ + ç”ŸæˆåŽåœæ­¢ âœ…ï¼ˆä¸¤ä¸ªadapterå®Œç¾Žå åŠ ï¼‰

---

## æœ€ä½³å®žè·µ

1. **è®­ç»ƒæ—¶**ï¼šæ˜Žç¡®å†»ç»“ä¸éœ€è¦æ›´æ–°çš„ adapter
2. **æŽ¨ç†æ—¶**ï¼šé»˜è®¤å¯ç”¨æ‰€æœ‰ adaptersï¼ˆè‡ªåŠ¨å åŠ ï¼‰
3. **è°ƒè¯•æ—¶**ï¼šé€ä¸ªæµ‹è¯•æ¯ä¸ª adapter çš„æ•ˆæžœ
4. **éƒ¨ç½²æ—¶**ï¼šç¡®ä¿æ‰€æœ‰ adapters éƒ½è¢«æ­£ç¡®åŠ è½½

## å¿«é€ŸéªŒè¯å‘½ä»¤

### ä¸€é”®æµ‹è¯•æ‰€æœ‰æ¨¡å¼
```bash
# åˆ›å»ºæµ‹è¯•è„šæœ¬
cat > test_all.sh << 'EOF'
#!/bin/bash

echo "=== æµ‹è¯•åŸºåº§æ¨¡åž‹ ==="
python -m lora.test_stage2_instruction --base_model Qwen3-8B-Base --preset elegant_style

echo -e "\n\n=== æµ‹è¯• Stage1 (style only) ==="
python -m lora.test_stage2_instruction --lora_model stage1_style_injection/checkpoint-531 --preset elegant_style

echo -e "\n\n=== æµ‹è¯• Stage2 (style + instruct stacked) ==="
python -m lora.test_stage2_instruction \
    --style_adapter stage1_style_injection/checkpoint-531 \
    --instruct_adapter stage2_instruct_new_adapter \
    --preset elegant_style
EOF

chmod +x test_all.sh
./test_all.sh
```

### Windows PowerShell ç‰ˆæœ¬
```powershell
# æµ‹è¯•åŸºåº§æ¨¡åž‹
Write-Host "=== æµ‹è¯•åŸºåº§æ¨¡åž‹ ===" -ForegroundColor Cyan
python -m lora.test_stage2_instruction --base_model Qwen3-8B-Base --preset elegant_style

# æµ‹è¯• Stage1
Write-Host "`n`n=== æµ‹è¯• Stage1 (style only) ===" -ForegroundColor Cyan
python -m lora.test_stage2_instruction --lora_model stage1_style_injection/checkpoint-531 --preset elegant_style

# æµ‹è¯• Stage2ï¼ˆåŒadapterå åŠ ï¼‰
Write-Host "`n`n=== æµ‹è¯• Stage2 (style + instruct stacked) ===" -ForegroundColor Cyan
python -m lora.test_stage2_instruction `
    --style_adapter stage1_style_injection/checkpoint-531 `
    --instruct_adapter stage2_instruct_new_adapter `
    --preset elegant_style
```

---

## å‚è€ƒèµ„æ–™

- PEFT æ–‡æ¡£: https://huggingface.co/docs/peft
- LoRA è®ºæ–‡: https://arxiv.org/abs/2106.09685
- Multi-Adapter å®žè·µ: https://github.com/huggingface/peft/tree/main/examples
- æœ¬é¡¹ç›®æµ‹è¯•è„šæœ¬: [test_stage2_instruction.py](test_stage2_instruction.py)
- ä½¿ç”¨è¯´æ˜Ž: [TEST_USAGE.md](TEST_USAGE.md)
