"""è¯Šæ–­è„šæœ¬ï¼šæ£€æµ‹æ¨¡å‹æ˜¯å¦è¿˜åœ¨"ç»­å†™å¯¹è¯"

Usage:
    python -m lora.diagnose_continuation \
        --model_name_or_path stage2_instruction_tuning/checkpoint-158
"""

import argparse
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer


def build_test_prompt():
    """æ„å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯• prompt"""
    return """<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªæ–‡å­¦æ”¹å†™åŠ©æ‰‹ï¼Œåªè´Ÿè´£è°ƒæ•´æ–‡é£å’Œæªè¾ã€‚æ”¹å†™æ—¶å¿…é¡»å®Œæ•´ä¿ç•™åŸæ–‡æä¾›çš„å…¨éƒ¨ä¿¡æ¯ï¼Œä¸å¾—æ‰©å†™æƒ…èŠ‚ï¼Œä¹Ÿä¸å¾—åˆ å‡å†…å®¹ã€‚ä½ çš„å›å¤å¿…é¡»åœ¨å®Œæˆæ”¹å†™åç«‹å³ç»“æŸï¼Œä¸å¾—ç»§ç»­ç”Ÿæˆä»»ä½•å¯¹è¯ã€æç¤ºæˆ–æ–°ä»»åŠ¡ã€‚<|im_end|>
<|im_start|>user
è¯·åœ¨ä¸å¢åˆ ä»»ä½•ä¿¡æ¯çš„å‰æä¸‹ï¼Œç”¨æ›´ç´§å¼ ã€æ‚¬ç–‘çš„æ–‡é£æ”¹å†™ä¸‹é¢è¿™æ®µï¼š
ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œå°æ˜å‡ºé—¨å»å…¬å›­æ•£æ­¥ã€‚<|im_end|>
<|im_start|>assistant
"""


def diagnose(model_path: str, attn_impl: str = "sdpa"):
    """è¿è¡Œè¯Šæ–­æµ‹è¯•"""
    print("=" * 80)
    print("ğŸ” Stage2 ç»­å†™é—®é¢˜è¯Šæ–­")
    print("=" * 80)
    print(f"\næ¨¡å‹è·¯å¾„: {model_path}")
    
    # åŠ è½½æ¨¡å‹å’Œ tokenizer
    print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        attn_implementation=attn_impl,
    )
    model.eval()
    
    # æ„å»ºæµ‹è¯• prompt
    prompt = build_test_prompt()
    print("\nğŸ“ æµ‹è¯• Prompt:")
    print(prompt)
    print("\n" + "-" * 80)
    
    # æµ‹è¯• 1: ä¸ä½¿ç”¨ stop tokensï¼ˆæ—§è¡Œä¸ºï¼‰
    print("\nğŸ§ª æµ‹è¯• 1: ä¸ä½¿ç”¨ stop tokensï¼ˆæ—§è¡Œä¸ºï¼‰")
    print("-" * 80)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=200,
            do_sample=False,  # ä½¿ç”¨è´ªå©ªè§£ç ï¼Œç»“æœæ›´ç¡®å®š
            pad_token_id=tokenizer.eos_token_id,
        )
    
    completion = tokenizer.decode(output_ids[0], skip_special_tokens=False)
    assistant_response = completion[len(prompt):]
    
    print("ğŸ“¤ æ¨¡å‹è¾“å‡º:")
    print(assistant_response[:500])
    
    # æ£€æµ‹é—®é¢˜æ ‡è®°
    issues = []
    if "\n<|im_start|>user" in assistant_response.lower() or "\nuser\n" in assistant_response.lower():
        issues.append("âŒ æ£€æµ‹åˆ° 'user' æ ‡è®° - æ¨¡å‹è¯•å›¾å¼€å¯æ–°å¯¹è¯")
    if "\n<|im_start|>system" in assistant_response.lower() or "\nsystem\n" in assistant_response.lower():
        issues.append("âŒ æ£€æµ‹åˆ° 'system' æ ‡è®° - æ¨¡å‹è¯•å›¾å¼€å¯æ–°å¯¹è¯")
    if "\n<|im_start|>assistant" in assistant_response.lower() or "\nassistant\n" in assistant_response.lower():
        issues.append("âŒ æ£€æµ‹åˆ°å¤šä¸ª 'assistant' æ ‡è®° - æ¨¡å‹è¯•å›¾ç»§ç»­å¯¹è¯")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ <|im_end|> åä»ç»§ç»­ç”Ÿæˆ
    if "<|im_end|>" in assistant_response:
        end_pos = assistant_response.find("<|im_end|>")
        after_end = assistant_response[end_pos + len("<|im_end|>"):].strip()
        if len(after_end) > 10:  # è¶…è¿‡ 10 ä¸ªå­—ç¬¦è®¤ä¸ºæ˜¯ç»§ç»­ç”Ÿæˆ
            issues.append(f"âŒ <|im_end|> åä»ç”Ÿæˆäº† {len(after_end)} ä¸ªå­—ç¬¦")
    
    if not issues:
        print("\nâœ… æµ‹è¯• 1 é€šè¿‡ï¼šæ¨¡å‹æ²¡æœ‰å°è¯•ç»§ç»­å¯¹è¯")
    else:
        print("\nâš ï¸  æµ‹è¯• 1 å‘ç°é—®é¢˜:")
        for issue in issues:
            print(f"  {issue}")
    
    # æµ‹è¯• 2: ä½¿ç”¨ stop tokensï¼ˆæ–°è¡Œä¸ºï¼‰
    print("\n" + "=" * 80)
    print("ğŸ§ª æµ‹è¯• 2: ä½¿ç”¨ stop tokensï¼ˆä¿®å¤åï¼‰")
    print("-" * 80)
    
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=200,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=[
                tokenizer.eos_token_id,
                tokenizer.convert_tokens_to_ids("<|im_end|>"),
            ],
        )
    
    completion = tokenizer.decode(output_ids[0], skip_special_tokens=False)
    assistant_response = completion[len(prompt):]
    
    print("ğŸ“¤ æ¨¡å‹è¾“å‡º:")
    print(assistant_response[:500])
    
    # æ£€æŸ¥æ˜¯å¦æ­£ç¡®åœæ­¢
    if "<|im_end|>" in assistant_response:
        end_pos = assistant_response.find("<|im_end|>")
        after_end = assistant_response[end_pos + len("<|im_end|>"):].strip()
        if len(after_end) == 0:
            print("\nâœ… æµ‹è¯• 2 é€šè¿‡ï¼šæ¨¡å‹åœ¨ <|im_end|> åæ­£ç¡®åœæ­¢")
        else:
            print(f"\nâš ï¸  <|im_end|> åä»æœ‰ {len(after_end)} ä¸ªå­—ç¬¦ï¼ˆä½†å·²å¤§å¹…æ”¹å–„ï¼‰")
    else:
        print("\nâš ï¸  æœªæ£€æµ‹åˆ° <|im_end|> - å¯èƒ½ç”Ÿæˆè¢«æˆªæ–­")
    
    # æ€»ç»“
    print("\n" + "=" * 80)
    print("ğŸ“Š è¯Šæ–­æ€»ç»“")
    print("=" * 80)
    
    if not issues:
        print("\nğŸ‰ æ­å–œï¼æ¨¡å‹è¡Œä¸ºæ­£å¸¸ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚")
        print("   - å»ºè®®ï¼šä½¿ç”¨ä¿®å¤åçš„æµ‹è¯•è„šæœ¬ (test_stage2_instruction.py)")
    else:
        print("\nâš ï¸  æ£€æµ‹åˆ°è¡Œä¸ºè¾¹ç•Œé—®é¢˜ï¼Œå»ºè®®é‡‡å–ä»¥ä¸‹æªæ–½ï¼š")
        print("\n   ç«‹å³æ”¹å–„ï¼ˆæ— éœ€é‡è®­ï¼‰:")
        print("   1. ä½¿ç”¨ test_stage2_instruction.pyï¼ˆå·²åŒ…å« stop tokensï¼‰")
        print("   2. åœ¨ system prompt ä¸­æ·»åŠ ç¡¬çº¦æŸ")
        print("\n   å½»åº•è§£å†³ï¼ˆæ¨èï¼‰:")
        print("   1. å‡†å¤‡ 20% æ•°æ®å­é›†: python prepare_correction_subset.py")
        print("   2. è¿è¡Œä¿®æ­£è®­ç»ƒ: python -m lora.stage2_instruction_tuning_fixed \\")
        print("                       --config lora/stage2_correction_config.json")
        print("\n   è¯¦è§: lora/STAGE2_FIX_README.md")


def main():
    parser = argparse.ArgumentParser(description="è¯Šæ–­ Stage2 æ¨¡å‹ç»­å†™é—®é¢˜")
    parser.add_argument(
        "--model_name_or_path",
        type=str,
        required=True,
        help="Stage2 æ¨¡å‹è·¯å¾„",
    )
    parser.add_argument(
        "--attn_impl",
        type=str,
        default="sdpa",
        help="Attention implementation",
    )
    
    args = parser.parse_args()
    diagnose(args.model_name_or_path, args.attn_impl)


if __name__ == "__main__":
    main()
