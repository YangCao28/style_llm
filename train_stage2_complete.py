"""Stage2 æŒ‡ä»¤å¾®è°ƒä¸€é”®è®­ç»ƒè„šæœ¬

æ•´åˆäº†æ•°æ®å‡†å¤‡ã€æ¸…ç†å’Œè®­ç»ƒçš„å®Œæ•´æµç¨‹ï¼š
1. ä»å®Œæ•´æ•°æ®é›†ä¸­é‡‡æ ·å­é›†ï¼ˆå¯é€‰ï¼‰
2. æ¸…ç† assistant å›å¤ä¸­çš„"å°¾å·´"
3. æ‰§è¡Œ Stage2 è®­ç»ƒ

Usage:
    # ä½¿ç”¨å®Œæ•´æ•°æ®é›†è®­ç»ƒ
    python train_stage2_complete.py --config lora/stage2_instruction_config.json
    
    # ä½¿ç”¨å­é›†è®­ç»ƒï¼ˆè½»é‡æ ¡æ­£ï¼‰
    python train_stage2_complete.py \
        --config lora/stage2_correction_config.json \
        --use_subset \
        --subset_ratio 0.2
"""

from __future__ import annotations

import argparse
import json
import random
import re
from pathlib import Path
from typing import Dict, List

import torch
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, PeftModel
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    TrainerCallback,
)
import torch.nn.functional as F


class WeightedLossTrainer(Trainer):
    """æ”¯æŒ per-token loss æƒé‡çš„è‡ªå®šä¹‰ Trainer"""
    
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """è®¡ç®—å¸¦æƒé‡çš„ loss"""
        # æå–æƒé‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        loss_weight = inputs.pop("loss_weight", None)
        
        # å¦‚æœæ²¡æœ‰æƒé‡ï¼Œä½¿ç”¨çˆ¶ç±»çš„æ ‡å‡†å®ç°
        if loss_weight is None or loss_weight[loss_weight != 1.0].numel() == 0:
            return super().compute_loss(model, inputs, return_outputs, num_items_in_batch)
        
        # å‰å‘ä¼ æ’­
        outputs = model(**inputs)
        logits = outputs.logits
        labels = inputs["labels"]
        
        # Shift for causal LM: é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        shift_weight = loss_weight[..., 1:].contiguous()
        
        # Flatten
        shift_logits = shift_logits.view(-1, shift_logits.size(-1))
        shift_labels = shift_labels.view(-1)
        shift_weight = shift_weight.view(-1)
        
        # è®¡ç®—æ¯ä¸ª token çš„ loss
        loss_fct = torch.nn.CrossEntropyLoss(reduction='none', ignore_index=-100)
        loss_per_token = loss_fct(shift_logits, shift_labels)
        
        # åº”ç”¨æƒé‡å¹¶è®¡ç®—å¹³å‡
        weighted_loss = loss_per_token * shift_weight
        # åªåœ¨é -100 çš„ä½ç½®æ±‚å¹³å‡
        valid_mask = (shift_labels != -100).float()
        loss = (weighted_loss * valid_mask).sum() / valid_mask.sum()
        
        return (loss, outputs) if return_outputs else loss

DEFAULT_SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä¸ªæ–‡å­¦åˆ›ä½œåŠ©æ‰‹ï¼Œæ“…é•¿ç”¨å„ç§é£æ ¼æ”¹å†™æ–‡æœ¬ã€‚"

# éœ€è¦æ¸…ç†çš„å¸¸è§åç¼€æ¨¡å¼
TAIL_PATTERNS = [
    r"æ”¹å†™å®Œæˆ[ã€‚ï¼\.!]*$",
    r"è¯·å‚è€ƒ[ã€‚ï¼\.!]*$",
    r"ä»¥ä¸Šæ˜¯æ”¹å†™ç»“æœ[ã€‚ï¼\.!]*$",
    r"æ”¹å†™å¦‚ä¸‹[ï¼š:ã€‚ï¼\.!]*$",
    r"ä¾›æ‚¨å‚è€ƒ[ã€‚ï¼\.!]*$",
    r"å¸Œæœ›å¯¹æ‚¨æœ‰å¸®åŠ©[ã€‚ï¼\.!]*$",
    r"è°¢è°¢[ã€‚ï¼\.!]*$",
    r"è¿˜æœ‰å…¶ä»–é—®é¢˜å—[ï¼Ÿ?ã€‚ï¼\.!]*$",
    r"[\n\s]+$",  # ç»“å°¾çš„å¤šä½™ç©ºç™½
]


def clean_assistant_response(response: str) -> str:
    """æ¸…ç† assistant å›å¤ä¸­çš„å°¾å·´"""
    cleaned = response.strip()
    
    # åº”ç”¨æ‰€æœ‰æ¸…ç†æ¨¡å¼
    for pattern in TAIL_PATTERNS:
        cleaned = re.sub(pattern, "", cleaned, flags=re.IGNORECASE)
    
    return cleaned.strip()


def prepare_dataset(
    input_path: Path,
    use_subset: bool = False,
    subset_ratio: float = 0.2,
    seed: int = 42,
) -> List[Dict]:
    """å‡†å¤‡è®­ç»ƒæ•°æ®é›†ï¼šåŠ è½½ã€é‡‡æ ·ã€æ¸…ç†"""
    print(f"\n{'='*80}")
    print("ğŸ“– å‡†å¤‡è®­ç»ƒæ•°æ®")
    print(f"{'='*80}")
    print(f"è¾“å…¥è·¯å¾„: {input_path}")
    
    # 1. åŠ è½½æ•°æ®
    samples = []
    with input_path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                samples.append(json.loads(line))
    
    print(f"âœ“ åŠ è½½äº† {len(samples):,} ä¸ªæ ·æœ¬")
    
    # 2. é‡‡æ ·ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if use_subset and subset_ratio < 1.0:
        random.seed(seed)
        num_samples = int(len(samples) * subset_ratio)
        samples = random.sample(samples, num_samples)
        print(f"âœ“ é‡‡æ ·äº† {num_samples:,} ä¸ªæ ·æœ¬ ({subset_ratio:.1%})")
    
    # 3. æ¸…ç† assistant å›å¤
    cleaned_count = 0
    for sample in samples:
        conversations = sample.get("conversations", [])
        for msg in conversations:
            role = msg.get("role") or msg.get("from")
            if role in ("assistant", "gpt", "bot"):
                content = msg.get("content") or msg.get("value") or ""
                cleaned_content = clean_assistant_response(content)
                
                if cleaned_content != content:
                    cleaned_count += 1
                    if "content" in msg:
                        msg["content"] = cleaned_content
                    if "value" in msg:
                        msg["value"] = cleaned_content
    
    print(f"âœ“ æ¸…ç†äº† {cleaned_count} ä¸ª assistant å›å¤ä¸­çš„'å°¾å·´'")
    
    return samples


def formatting_func_stage2_fixed(example, tokenizer, max_seq_length=2048, eos_loss_weight=2.0):
    """æ ¼å¼åŒ– stage2 å¯¹è¯æ•°æ® - labels åªåŒ…å« assistant å›å¤ + EOS
    
    Args:
        eos_loss_weight: EOS token çš„ loss æƒé‡å€æ•°ã€‚>1.0 ä¼šå¼ºåŒ–åœæ­¢è¡Œä¸ºçš„å­¦ä¹ ã€‚
    """
    conversations = example.get("conversations", [])
    if not conversations:
        return {"input_ids": [], "attention_mask": [], "labels": []}
    
    # æ„å»ºå®Œæ•´çš„ prompt å’Œæ‰¾åˆ° assistant çš„å›å¤
    messages = []
    assistant_response = None
    
    for msg in conversations:
        role = msg.get("role") or msg.get("from") or "user"
        content = msg.get("content") or msg.get("value") or ""
        
        if role in ("system", "sys"):
            norm_role = "system"
        elif role in ("assistant", "gpt", "bot"):
            norm_role = "assistant"
            assistant_response = content
        else:
            norm_role = "user"
        
        messages.append({"role": norm_role, "content": content})
    
    if assistant_response is None:
        return {"input_ids": [], "attention_mask": [], "labels": []}
    
    # ç¡®ä¿ assistant å›å¤å·²æ¸…ç†
    assistant_response = assistant_response.strip()
    
    # æ„å»º promptï¼ˆä¸åŒ…å« assistant å›å¤ï¼‰
    prompt_messages = [m for m in messages if m["role"] != "assistant"]
    prompt_parts = []
    for msg in prompt_messages:
        prompt_parts.append(f"<|im_start|>{msg['role']}\n{msg['content']}<|im_end|>")
    prompt_parts.append("<|im_start|>assistant\n")
    prompt_text = "\n".join(prompt_parts)
    
    # å®Œæ•´æ–‡æœ¬ï¼ˆåŒ…å« assistant å›å¤ + å¼ºåˆ¶çš„ç»“æŸæ ‡è®°ï¼‰
    full_text = prompt_text + assistant_response + "<|im_end|>"
    
    # Tokenize
    prompt_ids = tokenizer(prompt_text, add_special_tokens=False)["input_ids"]
    full_ids = tokenizer(full_text, truncation=True, max_length=max_seq_length, add_special_tokens=False)["input_ids"]
    
    # è·å– EOS token id
    eos_token_id = tokenizer.convert_tokens_to_ids("<|im_end|>")
    
    # æ„å»º labelsï¼šåªæœ‰ assistant å›å¤éƒ¨åˆ†ï¼ˆåŒ…æ‹¬ <|im_end|>ï¼‰æ˜¯æœ‰æ•ˆçš„
    labels = [-100] * len(prompt_ids) + full_ids[len(prompt_ids):]
    
    # ğŸ”‘ ä¸º EOS token åˆ›å»ºæƒé‡ï¼ˆå¦‚æœéœ€è¦å¼ºåŒ–å­¦ä¹ ï¼‰
    loss_weight = [1.0] * len(labels)  # é»˜è®¤æƒé‡éƒ½æ˜¯ 1.0
    if eos_loss_weight > 1.0:
        for i, label_id in enumerate(labels):
            if label_id == eos_token_id:
                loss_weight[i] = eos_loss_weight
    
    # Padding
    input_ids = full_ids + [tokenizer.pad_token_id] * (max_seq_length - len(full_ids))
    attention_mask = [1] * len(full_ids) + [0] * (max_seq_length - len(full_ids))
    labels = labels + [-100] * (max_seq_length - len(labels))
    loss_weight = loss_weight + [0.0] * (max_seq_length - len(loss_weight))
    
    return {
        "input_ids": input_ids[:max_seq_length],
        "attention_mask": attention_mask[:max_seq_length],
        "labels": labels[:max_seq_length],
        "loss_weight": loss_weight[:max_seq_length],  # æ–°å¢ï¼šæ¯ä¸ª token çš„æƒé‡
    }


class LossRecorderCallback(TrainerCallback):
    """è®°å½•è®­ç»ƒ loss"""
    def __init__(self):
        self.training_losses = []
        self.steps = []
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs and "loss" in logs:
            self.training_losses.append(logs["loss"])
            self.steps.append(state.global_step)
            if getattr(state, "is_world_process_zero", True):
                print(f"[step {state.global_step}] loss = {logs['loss']:.4f}")


def main():
    # å¼ºåˆ¶æ¸…ç† CUDA ç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        try:
            _ = torch.zeros(1).cuda()
            torch.cuda.synchronize()
            torch.cuda.empty_cache()
            print(f"âœ“ CUDA initialized: {torch.cuda.get_device_name(0)}")
            print(f"  Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        except RuntimeError as e:
            print(f"âš ï¸  CUDA initialization failed: {e}")
            raise
    
    # è§£æå‚æ•°
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=Path, required=True, help="Path to JSON config file")
    parser.add_argument("--use_subset", action="store_true", help="Use subset of data for training")
    parser.add_argument("--subset_ratio", type=float, default=0.2, help="Ratio of data to use (0.0-1.0)")
    parser.add_argument("--seed", type=int, default=42, help="Random seed for subset sampling")
    
    # é…ç½®å‚æ•°ï¼ˆå¯ä»¥è¢«é…ç½®æ–‡ä»¶è¦†ç›–ï¼‰
    parser.add_argument("--model_name_or_path", type=str)
    parser.add_argument("--dataset_path", type=str)
    parser.add_argument("--output_dir", type=str)
    parser.add_argument("--per_device_train_batch_size", type=int, default=8)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=8)
    parser.add_argument("--learning_rate", type=float, default=5e-5)
    parser.add_argument("--warmup_steps", type=int, default=50)
    parser.add_argument("--num_train_epochs", type=float, default=2.0)
    parser.add_argument("--max_seq_length", type=int, default=2048)
    parser.add_argument("--logging_steps", type=int, default=10)
    parser.add_argument("--save_steps", type=int, default=50)
    parser.add_argument("--attn_impl", type=str, default="sdpa")
    parser.add_argument("--lora_r", type=int, default=128, help="LoRA rank (åªåœ¨ä» base model è®­ç»ƒæ—¶ç”Ÿæ•ˆ)")
    parser.add_argument("--lora_alpha", type=int, default=256, help="LoRA alpha (åªåœ¨ä» base model è®­ç»ƒæ—¶ç”Ÿæ•ˆ)")
    parser.add_argument("--eos_loss_weight", type=float, default=2.0, help="EOS token çš„ loss æƒé‡å€æ•°ï¼Œ>1.0 ä¼šå¼ºåŒ–åœæ­¢è¡Œä¸ºå­¦ä¹ ")
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®æ–‡ä»¶
    if not args.config.exists():
        raise FileNotFoundError(f"Config file not found: {args.config}")
    
    with args.config.open("r", encoding="utf-8") as f:
        config_data = json.load(f)
    
    # ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼è¦†ç›–é»˜è®¤å€¼
    for key, value in config_data.items():
        if not hasattr(args, key) or getattr(args, key) is None or getattr(args, key) == parser.get_default(key):
            setattr(args, key, value)
    
    # æ£€æŸ¥å¿…éœ€å‚æ•°
    if not args.model_name_or_path or not args.dataset_path or not args.output_dir:
        parser.error("Required arguments: --model_name_or_path, --dataset_path, --output_dir (or provide via --config)")
    
    args.dataset_path = Path(args.dataset_path)
    args.output_dir = Path(args.output_dir)
    args.output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ¸…ç† GPU ç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        print("âœ“ GPU ç¼“å­˜å·²æ¸…ç†")
    
    print("=" * 80)
    print("Stage 2: ä¸€é”®å¼æŒ‡ä»¤å¾®è°ƒè®­ç»ƒ")
    print("=" * 80)
    print(f"Model checkpoint: {args.model_name_or_path}")
    print(f"Dataset: {args.dataset_path}")
    print(f"Output: {args.output_dir}")
    print(f"Batch size: {args.per_device_train_batch_size} Ã— {args.gradient_accumulation_steps}")
    print(f"Learning rate: {args.learning_rate}")
    print(f"Epochs: {args.num_train_epochs}")
    print(f"Max seq length: {args.max_seq_length}")
    if args.use_subset:
        print(f"Using subset: {args.subset_ratio:.1%} of data")
    print(f"LoRA config: r={args.lora_r}, alpha={args.lora_alpha}")
    
    # 1. å‡†å¤‡æ•°æ®é›†ï¼ˆåŠ è½½ã€é‡‡æ ·ã€æ¸…ç†ï¼‰
    samples = prepare_dataset(
        args.dataset_path,
        use_subset=args.use_subset,
        subset_ratio=args.subset_ratio,
        seed=args.seed,
    )
    
    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
    temp_data_path = args.output_dir / "training_data_cleaned.jsonl"
    with temp_data_path.open("w", encoding="utf-8") as f:
        for sample in samples:
            f.write(json.dumps(sample, ensure_ascii=False) + "\n")
    print(f"âœ“ ä¿å­˜æ¸…ç†åçš„æ•°æ®åˆ°: {temp_data_path}")
    
    # 2. åŠ è½½ tokenizer
    print(f"\n{'='*80}")
    print("ğŸ”§ åŠ è½½æ¨¡å‹å’Œ Tokenizer")
    print(f"{'='*80}")
    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)
    tokenizer.padding_side = "right"
    tokenizer.model_max_length = args.max_seq_length
    
    # ğŸ”‘ ç¡®ä¿ pad_token å’Œ eos_token ä¸åŒï¼ˆé‡è¦ï¼ï¼‰
    # å³ä½¿å·²æœ‰ pad_tokenï¼Œå¦‚æœä¸ eos_token ç›¸åŒä¹Ÿå¿…é¡»ä¿®å¤
    if tokenizer.pad_token_id == tokenizer.eos_token_id:
        # ä¼˜å…ˆä½¿ç”¨ unk_token
        if tokenizer.unk_token is not None and tokenizer.unk_token_id != tokenizer.eos_token_id:
            tokenizer.pad_token = tokenizer.unk_token
            print(f"âœ“ ä½¿ç”¨ unk_token ä½œä¸º pad_token")
        else:
            # æ·»åŠ æ–°çš„ pad_token
            tokenizer.add_special_tokens({'pad_token': '<|pad|>'})
            print(f"âœ“ æ·»åŠ æ–°çš„ pad_token: <|pad|>")
    
    print(f"âœ“ Tokenizer loaded")
    print(f"  - EOS token: {tokenizer.eos_token} (id={tokenizer.eos_token_id})")
    print(f"  - PAD token: {tokenizer.pad_token} (id={tokenizer.pad_token_id})")
    if tokenizer.pad_token_id == tokenizer.eos_token_id:
        print("  âŒ ERROR: pad_token_id == eos_token_id ä»ç„¶ç›¸åŒï¼")
        raise ValueError("Failed to separate pad_token from eos_token")
    else:
        print("  âœ… pad_token å’Œ eos_token å·²åˆ†ç¦»")
    
    # 3. åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name_or_path,
        torch_dtype=torch.bfloat16,
        attn_implementation=args.attn_impl,
    )
    
    # å¦‚æœæ·»åŠ äº†æ–°çš„ special tokenï¼Œéœ€è¦ resize embeddings
    if len(tokenizer) > model.get_input_embeddings().weight.shape[0]:
        model.resize_token_embeddings(len(tokenizer))
        print(f"âœ“ Resized model embeddings to {len(tokenizer)}")
    
    print(f"âœ“ Model loaded")
    
    # 4. åŠ è½½å¹¶å¤„ç†æ•°æ®é›†
    print(f"\n{'='*80}")
    print("ğŸ“Š Tokenizing dataset")
    print(f"{'='*80}")
    dataset = load_dataset("json", data_files=str(temp_data_path), split="train")
    
    def tokenize_function(examples):
        results = {
            "input_ids": [],
            "attention_mask": [],
            "labels": [],
            "loss_weight": [],
        }
        
        num_samples = len(examples["conversations"])
        for i in range(num_samples):
            example = {key: examples[key][i] for key in examples}
            formatted = formatting_func_stage2_fixed(example, tokenizer, args.max_seq_length, args.eos_loss_weight)
            
            if formatted["input_ids"]:
                results["input_ids"].append(formatted["input_ids"])
                results["attention_mask"].append(formatted["attention_mask"])
                results["labels"].append(formatted["labels"])
                results["loss_weight"].append(formatted["loss_weight"])
        
        return results
    
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        batch_size=100,
        remove_columns=dataset.column_names,
    )
    print(f"âœ“ Tokenization complete: {len(tokenized_dataset):,} samples")
    
    # éªŒè¯ç¬¬ä¸€ä¸ªæ ·æœ¬
    print("\nğŸ” éªŒè¯ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ tokenization:")
    first_sample = tokenized_dataset[0]
    first_input_ids = first_sample["input_ids"]
    first_labels = first_sample["labels"]
    
    # è§£ç å®Œæ•´è¾“å…¥
    full_text = tokenizer.decode([tid for tid in first_input_ids if tid != tokenizer.pad_token_id], skip_special_tokens=False)
    print(f"\nå®Œæ•´è¾“å…¥æ–‡æœ¬ï¼ˆå‰500å­—ç¬¦ï¼‰:")
    print(full_text[:500])
    
    # è§£ç  labelsï¼ˆåªæœ‰è¿™éƒ¨åˆ†ä¼šè¢«è®­ç»ƒï¼‰
    valid_label_ids = [lid for lid in first_labels if lid != -100]
    if valid_label_ids:
        decoded_labels = tokenizer.decode(valid_label_ids, skip_special_tokens=False)
        print(f"\nâœ… è®­ç»ƒçš„ labelsï¼ˆå‰300å­—ç¬¦ï¼‰:")
        print(decoded_labels[:300])
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸è¯¥è®­ç»ƒçš„å†…å®¹
        if any(marker in decoded_labels for marker in ["<|im_start|>system", "<|im_start|>user", "ä»»åŠ¡ï¼š", "è¦æ±‚ï¼š"]):
            print("\nâŒ é”™è¯¯ï¼šLabels åŒ…å«äº† system/user promptï¼")
            print("   è¿™ä¼šå¯¼è‡´æ¨¡å‹å­¦ä¼šå›æ˜¾è¾“å…¥")
        elif decoded_labels.strip().startswith("<|im_start|>"):
            print("\nâŒ é”™è¯¯ï¼šLabels ä»¥ <|im_start|> å¼€å¤´ï¼Œåº”è¯¥ç›´æ¥æ˜¯å†…å®¹")
        else:
            print("\nâœ… Labels æ­£ç¡®ï¼šåªåŒ…å« assistant å›å¤å†…å®¹")
    else:
        print("\nâŒ é”™è¯¯ï¼šæ²¡æœ‰æœ‰æ•ˆçš„ labelsï¼")
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # 5. è®­ç»ƒå‚æ•°
    print(f"\n{'='*80}")
    print("âš™ï¸  é…ç½®è®­ç»ƒå‚æ•°")
    print(f"{'='*80}")
    training_args = TrainingArguments(
        output_dir=str(args.output_dir),
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        lr_scheduler_type="cosine",
        warmup_steps=args.warmup_steps,
        num_train_epochs=args.num_train_epochs,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        save_total_limit=3,
        bf16=True,
        tf32=True,
        optim="adamw_torch_fused",
        max_grad_norm=1.0,
        gradient_checkpointing=True,
        report_to=[],
        dataloader_drop_last=False,
    )
    
    # 6. åˆ›å»º Trainerï¼ˆä½¿ç”¨è‡ªå®šä¹‰çš„ WeightedLossTrainerï¼‰
    loss_recorder = LossRecorderCallback()
    trainer = WeightedLossTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        callbacks=[loss_recorder],
    )
    
    # 7. å¼€å§‹è®­ç»ƒ
    print("\n" + "=" * 80)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    print("=" * 80 + "\n")
    
    trainer.train()
    
    # 8. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    trainer.save_model()
    tokenizer.save_pretrained(args.output_dir)
    
    print(f"\nâœ“ è®­ç»ƒå®Œæˆ!")
    print(f"  Model saved to: {args.output_dir}")
    if loss_recorder.training_losses:
        print(f"  Initial loss: {loss_recorder.training_losses[0]:.4f}")
        print(f"  Final loss: {loss_recorder.training_losses[-1]:.4f}")
        print(f"  Total steps: {len(loss_recorder.steps)}")
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


if __name__ == "__main__":
    main()
